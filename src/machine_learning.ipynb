{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import collections\n",
    "import re\n",
    "from itertools import groupby\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "# pre-processing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, BorderlineSMOTE, SMOTENC\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# classifiers\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "\n",
    "# Feature selection\n",
    "\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel, SelectPercentile, SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "# learning and cross-validation\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedShuffleSplit, StratifiedKFold, RepeatedStratifiedKFold, \\\n",
    "                                    cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.metrics import make_scorer, get_scorer, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report   \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "try:\n",
    "    del sys.modules['feutils']\n",
    "    del sys.modules['config']\n",
    "except:\n",
    "    pass\n",
    "import config\n",
    "from feutils import DataHandler\n",
    "from feutils import JNCC2Wrapper\n",
    "from feutils import FEUtils\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "logger = logging.getLogger()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "feu = FEUtils()\n",
    "\n",
    "print('pandas ' + str(pd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn customizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class PassthroughTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    sklearn' default transformer does not give access to feature names, and Pipeline may change\n",
    "    order of features. This implementation overloads default passthrough transformer so it returns features names.\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.X = X\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.extmath import _incremental_mean_and_var\n",
    "from sklearn.utils.validation import (check_is_fitted, check_random_state,\n",
    "                                FLOAT_DTYPES)\n",
    "from sklearn.preprocessing.data import _handle_zeros_in_scale\n",
    "from scipy import sparse\n",
    "\n",
    "class CustomStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Customized to add 'get_feature_names'\n",
    "    \"\"\"  # noqa\n",
    "\n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        self.with_mean = with_mean\n",
    "        self.with_std = with_std\n",
    "        self.copy = copy\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset internal data-dependent state of the scaler, if necessary.\n",
    "        __init__ parameters are not touched.\n",
    "        \"\"\"\n",
    "\n",
    "        # Checking one attribute is enough, becase they are all set together\n",
    "        # in partial_fit\n",
    "        if hasattr(self, 'scale_'):\n",
    "            del self.scale_\n",
    "            del self.n_samples_seen_\n",
    "            del self.mean_\n",
    "            del self.var_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the mean and std to be used for later scaling.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
    "            The data used to compute the mean and standard deviation\n",
    "            used for later scaling along the features axis.\n",
    "        y\n",
    "            Ignored\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset internal state before fitting\n",
    "        self._reset()\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = X.columns\n",
    "        return self.partial_fit(X, y)\n",
    "\n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Online computation of mean and std on X for later scaling.\n",
    "        All of X is processed as a single batch. This is intended for cases\n",
    "        when `fit` is not feasible due to very large number of `n_samples`\n",
    "        or because X is read from a continuous stream.\n",
    "        The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
    "        in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
    "        for computing the sample variance: Analysis and recommendations.\"\n",
    "        The American Statistician 37.3 (1983): 242-247:\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
    "            The data used to compute the mean and standard deviation\n",
    "            used for later scaling along the features axis.\n",
    "        y\n",
    "            Ignored\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = X.columns        \n",
    "        X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n",
    "                        estimator=self, dtype=FLOAT_DTYPES,\n",
    "                        force_all_finite='allow-nan')\n",
    "\n",
    "        # Even in the case of `with_mean=False`, we update the mean anyway\n",
    "        # This is needed for the incremental computation of the var\n",
    "        # See incr_mean_variance_axis and _incremental_mean_variance_axis\n",
    "\n",
    "        # if n_samples_seen_ is an integer (i.e. no missing values), we need to\n",
    "        # transform it to a NumPy array of shape (n_features,) required by\n",
    "        # incr_mean_variance_axis and _incremental_variance_axis\n",
    "        if (hasattr(self, 'n_samples_seen_') and\n",
    "                isinstance(self.n_samples_seen_, (int, np.integer))):\n",
    "            self.n_samples_seen_ = np.repeat(\n",
    "                self.n_samples_seen_, X.shape[1]).astype(np.int64, copy=False)\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if self.with_mean:\n",
    "                raise ValueError(\n",
    "                    \"Cannot center sparse matrices: pass `with_mean=False` \"\n",
    "                    \"instead. See docstring for motivation and alternatives.\")\n",
    "\n",
    "            sparse_constructor = (sparse.csr_matrix\n",
    "                                  if X.format == 'csr' else sparse.csc_matrix)\n",
    "            counts_nan = sparse_constructor(\n",
    "                        (np.isnan(X.data), X.indices, X.indptr),\n",
    "                        shape=X.shape).sum(axis=0).A.ravel()\n",
    "\n",
    "            if not hasattr(self, 'n_samples_seen_'):\n",
    "                self.n_samples_seen_ = (\n",
    "                        X.shape[0] - counts_nan).astype(np.int64, copy=False)\n",
    "\n",
    "            if self.with_std:\n",
    "                # First pass\n",
    "                if not hasattr(self, 'scale_'):\n",
    "                    self.mean_, self.var_ = mean_variance_axis(X, axis=0)\n",
    "                # Next passes\n",
    "                else:\n",
    "                    self.mean_, self.var_, self.n_samples_seen_ = \\\n",
    "                        incr_mean_variance_axis(X, axis=0,\n",
    "                                                last_mean=self.mean_,\n",
    "                                                last_var=self.var_,\n",
    "                                                last_n=self.n_samples_seen_)\n",
    "            else:\n",
    "                self.mean_ = None\n",
    "                self.var_ = None\n",
    "                if hasattr(self, 'scale_'):\n",
    "                    self.n_samples_seen_ += X.shape[0] - counts_nan\n",
    "        else:\n",
    "            if not hasattr(self, 'n_samples_seen_'):\n",
    "                self.n_samples_seen_ = np.zeros(X.shape[1], dtype=np.int64)\n",
    "\n",
    "            # First pass\n",
    "            if not hasattr(self, 'scale_'):\n",
    "                self.mean_ = .0\n",
    "                if self.with_std:\n",
    "                    self.var_ = .0\n",
    "                else:\n",
    "                    self.var_ = None\n",
    "\n",
    "            if not self.with_mean and not self.with_std:\n",
    "                self.mean_ = None\n",
    "                self.var_ = None\n",
    "                self.n_samples_seen_ += X.shape[0] - np.isnan(X).sum(axis=0)\n",
    "            else:\n",
    "                self.mean_, self.var_, self.n_samples_seen_ = \\\n",
    "                    _incremental_mean_and_var(X, self.mean_, self.var_,\n",
    "                                              self.n_samples_seen_)\n",
    "\n",
    "        # for backward-compatibility, reduce n_samples_seen_ to an integer\n",
    "        # if the number of samples is the same for each feature (i.e. no\n",
    "        # missing values)\n",
    "        if np.ptp(self.n_samples_seen_) == 0:\n",
    "            self.n_samples_seen_ = self.n_samples_seen_[0]\n",
    "\n",
    "        if self.with_std:\n",
    "            self.scale_ = _handle_zeros_in_scale(np.sqrt(self.var_))\n",
    "        else:\n",
    "            self.scale_ = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"Perform standardization by centering and scaling\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = X.columns        \n",
    "        check_is_fitted(self, 'scale_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, accept_sparse='csr', copy=copy,\n",
    "                        estimator=self, dtype=FLOAT_DTYPES,\n",
    "                        force_all_finite='allow-nan')\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if self.with_mean:\n",
    "                raise ValueError(\n",
    "                    \"Cannot center sparse matrices: pass `with_mean=False` \"\n",
    "                    \"instead. See docstring for motivation and alternatives.\")\n",
    "            if self.scale_ is not None:\n",
    "                inplace_column_scale(X, 1 / self.scale_)\n",
    "        else:\n",
    "            if self.with_mean:\n",
    "                X -= self.mean_\n",
    "            if self.with_std:\n",
    "                X /= self.scale_\n",
    "        self.X = X\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        \"\"\"Scale back the data to the original representation\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        Returns\n",
    "        -------\n",
    "        X_tr : array-like, shape [n_samples, n_features]\n",
    "            Transformed array.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'scale_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        if sparse.issparse(X):\n",
    "            if self.with_mean:\n",
    "                raise ValueError(\n",
    "                    \"Cannot uncenter sparse matrices: pass `with_mean=False` \"\n",
    "                    \"instead See docstring for motivation and alternatives.\")\n",
    "            if not sparse.isspmatrix_csr(X):\n",
    "                X = X.tocsr()\n",
    "                copy = False\n",
    "            if copy:\n",
    "                X = X.copy()\n",
    "            if self.scale_ is not None:\n",
    "                inplace_column_scale(X, self.scale_)\n",
    "        else:\n",
    "            X = np.asarray(X)\n",
    "            if copy:\n",
    "                X = X.copy()\n",
    "            if self.with_std:\n",
    "                X *= self.scale_\n",
    "            if self.with_mean:\n",
    "                X += self.mean_\n",
    "        return X\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'allow_nan': True}\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "def parse_feature_names(feature_names):\n",
    "    reg = r'.*__(.*)'\n",
    "    return [re.findall(reg, feat)[0] for feat in feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPUs for agent and new way of computing IPUs for doctor/phases\n",
    "matrix_subject_nophase = os.path.join('..', 'features', 'new_ipus', 'matrix-2019-0607_00-25-49Features-nophase.xlsx')\n",
    "matrix_agent_phases = os.path.join('..', 'features', 'new_ipus', 'matrix-2019-0624_11-38-41Features-agent-157015.xlsx')\n",
    "matrix_subject_phases = os.path.join('..', 'features', 'new_ipus', 'matrix-2019-0624_12-13-32Features-157015.xlsx')\n",
    "matrix_agent_nophase = os.path.join('..', 'features', 'new_ipus', 'matrix-2019-0624_11-41-11Features-agent-nophase.xlsx')\n",
    "\n",
    "# new features angles speeds means/stds\n",
    "#matrix_subject_nophase = os.path.join('..', 'features', 'angles', 'matrix-2019-0607_00-25-49Features-nophase.xlsx')\n",
    "#matrix_agent_phases = os.path.join('..', 'features', 'angles', 'matrix-2019-0608_12-31-38Features-agent-157015.xlsx')\n",
    "#matrix_subject_phases = os.path.join('..', 'features', 'angles', 'matrix-2019-0608_08-46-42Features-157015.xlsx')\n",
    "#matrix_agent_nophase = os.path.join('..', 'features', 'angles', 'matrix-2019-0608_15-03-58Features-agent-nophase.xlsx')\n",
    "\n",
    "\n",
    "# new feature 'answerDelay'\n",
    "\"\"\"matrix_agent_phases = os.path.join('..', 'features', 'answersDelays', matrix-2019-0529_13-58-51Features-agent-157015.xlsx')\n",
    "matrix_subject_phases = os.path.join('..', 'features', 'answersDelays', 'matrix-2019-0529_11-20-12Features-157015.xlsx')\n",
    "trix_subject_nophase = os.path.join('..', 'features', 'answersDelays', 'matrix-2019-0602_22-16-02Features-nophase.xlsx')\n",
    "matrix_agent_nophase = os.path.join('..', 'features', 'answersDelays', 'matrix-2019-0602_21-45-19Features-agent-nophase.xlsx')\"\"\"\n",
    "\n",
    "# 'cleaned' dataset\n",
    "\"\"\"matrix_subject_phases = os.path.join('..', 'features', 'cleaned', 'matrix-2019-0409_14-52-24Features-157015.xlsx')\n",
    "matrix_agent_phases = os.path.join('..', 'features', 'cleaned', 'matrix-2019-0410_17-50-48Features-agent-157015.xlsx')\n",
    "matrix_subject_nophase = os.path.join('..', 'features', 'cleaned', 'matrix-2019-0411_14-05-24Features-nophase.xlsx')\n",
    "matrix_agent_nophase = os.path.join('..', 'features', 'cleaned','matrix-2019-0411_15-30-16Features-agent-nophase.xlsx') \"\"\"\n",
    "\n",
    "# old dataset\n",
    "\"\"\"matrix_subject_phases = os.path.join('..', 'features', 'matrix-2019-0226_15-16-44Features-157015.xlsx')\n",
    "matrix_agent_phases = os.path.join('..', 'features', 'matrix-2019-0226_17-18-28Features-agent-157015.xlsx')\n",
    "matrix_subject_nophase = os.path.join('..', 'features', 'matrix-2019-0227_20-11-53Features-nophase.xlsx')\n",
    "matrix_agent_nophase = os.path.join('..', 'features', 'matrix-2019-0227_22-43-15Features-agent-nophase.xlsx') \"\"\"\n",
    "\n",
    "def prepare_df(df):\n",
    "    # Replace 3 classes (low, medium, high) by 5 classes (ie, score rounded)\n",
    "    #df['Presence Class'] = df['Presence Score'].round()\n",
    "    #df['Co-presence Class'] = df['Co-presence Score'].round()\n",
    "    #df = df.drop(['Presence Score', 'Co-presence Score'], axis=1)\n",
    "    df = df.rename(index=str,\n",
    "                   columns={\"Presence Class\": \"PresenceClass\", \"Co-presence Class\": \"CopresenceClass\"})\n",
    "    return df\n",
    "\n",
    "def prepare_agent_df(df):\n",
    "    df = prepare_df(df)\n",
    "    df = df.drop(['PresenceClass', 'CopresenceClass', 'Presence Score', 'Co-presence Score', 'Duration', 'Expert'], axis=1)\n",
    "    agent_cols_rename = {}\n",
    "    for col in df.columns:\n",
    "        if col not in ['Candidate', 'Environment']:\n",
    "            agent_cols_rename[col] = col + '_agent'\n",
    "    df = df.rename(index=str, columns=agent_cols_rename)\n",
    "    return df\n",
    "    \n",
    "def dfval(df, candidate, environment, column):\n",
    "    return df.query(\"Candidate == '%s' & Environment == '%s'\" % (candidate, environment))[column].values\n",
    "\n",
    "print(\"Loading data - phases split ...\")\n",
    "\n",
    "subject_p_df = pd.read_excel(matrix_subject_phases).set_index('Candidate').drop('E6F', axis=0).reset_index()\n",
    "subject_p_df = prepare_df(subject_p_df)\n",
    "print(\"   Subject(phases): %d samples\" % len(subject_p_df))\n",
    "agent_p_df = pd.read_excel(matrix_agent_phases).set_index('Candidate').drop('E6F', axis=0).reset_index()\n",
    "agent_p_df = prepare_agent_df(agent_p_df)\n",
    "print(\"   Agent(phases): %d samples\" % len(agent_p_df))\n",
    "# add Duration feature from subject to agent - requires to first merge both datasets\n",
    "agent_p_df = pd.merge(agent_p_df, subject_p_df[['Candidate', 'Environment', 'Duration', 'PresenceClass', 'CopresenceClass', 'Presence Score', 'Co-presence Score']], on=['Candidate', 'Environment'], \n",
    "                      how='inner', suffixes=(False, False))\n",
    "print(\"   Agent(phases-merged): %d samples\" % len(agent_p_df))\n",
    "\n",
    "# no phase\n",
    "\n",
    "subject_np_df = pd.read_excel(matrix_subject_nophase).set_index('Candidate').drop('E6F', axis=0).reset_index()\n",
    "subject_np_df = prepare_df(subject_np_df)\n",
    "print(\"   Subject(no phase): %d samples\" % len(subject_np_df))\n",
    "agent_np_df = pd.read_excel(matrix_agent_nophase).set_index('Candidate').drop('E6F', axis=0).reset_index()\n",
    "agent_np_df = prepare_agent_df(agent_np_df)\n",
    "print(\"   Agent(no phase): %d samples\" % len(agent_p_df))\n",
    "# add Duration feature from subject to agent - requires to first merge both datasets\n",
    "agent_np_df = pd.merge(agent_np_df, subject_np_df[['Candidate', 'Environment', 'Duration', 'PresenceClass', 'CopresenceClass', 'Presence Score', 'Co-presence Score']], on=['Candidate', 'Environment'], \n",
    "                      how='inner', suffixes=(False, False))\n",
    "print(\"   Agent(no phase-merged): %d samples\" % len(agent_np_df))\n",
    "\n",
    "subject_p_df.index = [subject_p_df['Candidate'], subject_p_df['Environment']]\n",
    "agent_p_df.index = [agent_p_df['Candidate'], agent_p_df['Environment']]\n",
    "subject_np_df.index = [subject_np_df['Candidate'], subject_np_df['Environment']]\n",
    "agent_np_df.index = [agent_np_df['Candidate'], agent_np_df['Environment']]\n",
    "\n",
    "intersect = set(subject_p_df.index) & set(agent_p_df.index) & set(subject_np_df.index) & set(agent_np_df.index)\n",
    "print(\"  samples intersection %d\" % len(intersect))\n",
    "\n",
    "rejected_from_subject_p_df = set(subject_p_df.index) - intersect\n",
    "rejected_from_subject_np_df = set(subject_np_df.index) - intersect\n",
    "rejected_from_agent_p_df = set(agent_p_df.index) - intersect\n",
    "rejected_from_agent_np_df = set(agent_np_df.index) - intersect\n",
    "\n",
    "# restrict each dataset to the common samples\n",
    "subject_p_df = subject_p_df.loc[list(intersect)].reset_index(drop=True)\n",
    "subject_np_df = subject_np_df.loc[list(intersect)].reset_index(drop=True)\n",
    "agent_p_df = agent_p_df.loc[list(intersect)].reset_index(drop=True)\n",
    "agent_np_df = agent_np_df.loc[list(intersect)].reset_index(drop=True)\n",
    "\n",
    "all_p_df = pd.merge(subject_p_df, agent_p_df.drop(['Duration', 'PresenceClass', 'CopresenceClass', 'Presence Score', 'Co-presence Score'], axis=1), on=['Candidate', 'Environment'], \n",
    "                    how='inner', suffixes=(False, False))\n",
    "#all_p_df = all_p_df.drop(['PresenceClass_agent', 'CopresenceClass_agent', 'Duration_agent', 'Expert_agent'], axis=1)\n",
    "print(\"   Merge(phases): %d samples \" % len(all_p_df))\n",
    "\n",
    "all_np_df = pd.merge(subject_np_df, agent_np_df.drop(['Duration', 'PresenceClass', 'CopresenceClass', 'Presence Score', 'Co-presence Score'], axis=1), on=['Candidate', 'Environment'], \n",
    "                     how='inner', suffixes=(False, False))\n",
    "#all_np_df = all_np_df.drop(['PresenceClass_agent', 'CopresenceClass_agent', 'Duration_agent', 'Expert_agent'], axis=1)\n",
    "print(\"   Merge(no phase): %d samples \" % len(all_np_df))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Additional info:\")\n",
    "print(\"   samples rejected from subject / phases   %d\" % len(rejected_from_subject_p_df))\n",
    "for item in sorted(list(rejected_from_subject_p_df)) : print(\"     %s / %s\" % (item[0], item[1]))\n",
    "print(\"   samples rejected from subject / no phase %d\" % len(rejected_from_subject_np_df))\n",
    "for item in sorted(list(rejected_from_subject_np_df)) : print(\"     %s / %s\" % (item[0], item[1]))\n",
    "print(\"   samples rejected from agent / phases     %d\" % len(rejected_from_agent_p_df))\n",
    "for item in sorted(list(rejected_from_agent_p_df)) : print(\"     %s / %s\" % (item[0], item[1]))\n",
    "print(\"   samples rejected from agent / no phase   %d\" % len(rejected_from_agent_np_df))\n",
    "for item in sorted(list(rejected_from_agent_np_df)) : print(\"     %s / %s\" % (item[0], item[1]))\n",
    "    \n",
    "# compute new features being mean of 'yaw', 'pitch' and 'roll'\n",
    "\n",
    "for feature in ['%s%sAngularSpeed_Head_%s', 'Avg_%s%sAngularSpeed_Hand_%s']:\n",
    "    for stat in ['Mean', 'Std']:\n",
    "        for phase in ['Start', 'Mid', 'End']:\n",
    "            if phase == 'Mid':\n",
    "                all_np_df[feature % (stat, 'Mean', phase)] = all_np_df[[feature % (stat, 'Yaw', phase), \n",
    "                                                                        feature % (stat, 'Pitch', phase), \n",
    "                                                                        feature % (stat, 'Roll', phase)]].mean(axis=1)\n",
    "                all_np_df[feature % (stat, 'Mean', phase)+'_agent'] = all_np_df[[feature % (stat, 'Yaw', phase)+'_agent', \n",
    "                                                                        feature % (stat, 'Pitch', phase)+'_agent', \n",
    "                                                                        feature % (stat, 'Roll', phase)+'_agent']].mean(axis=1)        \n",
    "                subject_np_df[feature % (stat, 'Mean', phase)] = subject_np_df[[feature % (stat, 'Yaw', phase), \n",
    "                                                                        feature % (stat, 'Pitch', phase), \n",
    "                                                                        feature % (stat, 'Roll', phase)]].mean(axis=1)\n",
    "                agent_np_df[feature % (stat, 'Mean', phase)+'_agent'] = agent_np_df[[feature % (stat, 'Yaw', phase)+'_agent', \n",
    "                                                                        feature % (stat, 'Pitch', phase)+'_agent', \n",
    "                                                                        feature % (stat, 'Roll', phase)+'_agent']].mean(axis=1)\n",
    "            all_p_df[feature % (stat, 'Mean', phase)] = all_p_df[[feature % (stat, 'Yaw', phase), \n",
    "                                                                    feature % (stat, 'Pitch', phase), \n",
    "                                                                    feature % (stat, 'Roll', phase)]].mean(axis=1)\n",
    "            all_p_df[feature % (stat, 'Mean', phase)+'_agent'] = all_p_df[[feature % (stat, 'Yaw', phase)+'_agent', \n",
    "                                                                    feature % (stat, 'Pitch', phase)+'_agent', \n",
    "                                                                    feature % (stat, 'Roll', phase)+'_agent']].mean(axis=1)        \n",
    "            subject_p_df[feature % (stat, 'Mean', phase)] = subject_p_df[[feature % (stat, 'Yaw', phase), \n",
    "                                                                    feature % (stat, 'Pitch', phase), \n",
    "                                                                    feature % (stat, 'Roll', phase)]].mean(axis=1)\n",
    "            agent_p_df[feature % (stat, 'Mean', phase)+'_agent'] = agent_p_df[[feature % (stat, 'Yaw', phase)+'_agent', \n",
    "                                                                    feature % (stat, 'Pitch', phase)+'_agent', \n",
    "                                                                    feature % (stat, 'Roll', phase)+'_agent']].mean(axis=1)\n",
    "\n",
    "# drop useless columns\n",
    "all_np_df.drop([col for col in all_np_df.columns if 'Yaw' in col or 'Roll' in col or 'Pitch' in col or 'Freq_' in col], axis=1, inplace=True)\n",
    "all_p_df.drop([col for col in all_p_df.columns if 'Yaw' in col or 'Roll' in col or 'Pitch' in col or 'Freq_' in col], axis=1, inplace=True)\n",
    "subject_np_df.drop([col for col in subject_np_df.columns if 'Yaw' in col or 'Roll' in col or 'Pitch' in col or 'Freq_' in col], axis=1, inplace=True)\n",
    "subject_p_df.drop([col for col in subject_p_df.columns if 'Yaw' in col or 'Roll' in col or 'Pitch' in col or 'Freq_' in col], axis=1, inplace=True)\n",
    "agent_np_df.drop([col for col in agent_np_df.columns if 'Yaw' in col or 'Roll' in col or 'Pitch' in col or 'Freq_' in col], axis=1, inplace=True)\n",
    "agent_p_df.drop([col for col in agent_p_df.columns if 'Yaw' in col or 'Roll' in col or 'Pitch' in col or 'Freq_' in col], axis=1, inplace=True)\n",
    "\n",
    "# handle categorical data type\n",
    "all_np_df['Expert'] = all_np_df['Expert'].astype('category')\n",
    "all_p_df['Expert'] = all_p_df['Expert'].astype('category')\n",
    "subject_np_df['Expert'] = subject_np_df['Expert'].astype('category')\n",
    "subject_p_df['Expert'] = subject_p_df['Expert'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence and co-presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_p_df['Presence Score'], bins=20, range=(1,5), edgecolor='black')\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xlabel(\"Presence score\")\n",
    "plt.axvline(x=all_p_df['Presence Score'].mean(), c='red', label='mean (%.02f)'%all_p_df['Presence Score'].mean())\n",
    "plt.axvline(x=all_p_df['Presence Score'].median(), c='green', label='median (%.02f)'%all_p_df['Presence Score'].median())\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.hist(all_p_df['Co-presence Score'], bins=20, range=(1,5), edgecolor='black')\n",
    "plt.axvline(x=all_p_df['Co-presence Score'].mean(), c='red', label='mean (%.02f)'%all_p_df['Co-presence Score'].mean())\n",
    "plt.axvline(x=all_p_df['Co-presence Score'].median(), c='green', label='median (%.02f)'%all_p_df['Co-presence Score'].median())\n",
    "plt.legend()\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xlabel(\"Co-Presence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "discretization_params = {\n",
    "    'n_bins' : [2,3,4,5,6,7,8,9,10],\n",
    "    'strategy' : ['uniform', 'quantile', 'kmeans']\n",
    "}\n",
    "\n",
    "def discretize(data, n_bins, strategy):\n",
    "    enc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "    pres_arr = np.array(data).reshape(-1,1)\n",
    "    pres_kmeans = enc.fit_transform(pres_arr).astype(int)\n",
    "    return pres_kmeans, enc\n",
    "\n",
    "def compute_discretization_scores(data, labels):\n",
    "    calinski = metrics.calinski_harabaz_score(data, labels)\n",
    "    silhouette = metrics.silhouette_score(data, labels, metric='euclidean')\n",
    "    db = metrics.davies_bouldin_score(data, labels)\n",
    "    min_cluster = np.unique(labels, return_counts=True)[1].min()\n",
    "    max_cluster = np.unique(labels, return_counts=True)[1].max()  \n",
    "    return [calinski, silhouette, db, min_cluster, max_cluster]\n",
    "\n",
    "def discretization_gridsearch(data, discretization_params, orig_discretization=None, title_prefix='presence, '):\n",
    "    data = np.array(data).reshape(-1, 1)\n",
    "    discretization_params_grid = ParameterGrid(discretization_params)\n",
    "    results = np.zeros(shape=(len(discretization_params_grid)+1, 5))\n",
    "    for idx, param_set in enumerate(discretization_params_grid):\n",
    "        print(idx, param_set)\n",
    "        pres_kmeans, enc = discretize(data, n_bins=param_set['n_bins'], strategy=param_set['strategy'])\n",
    "        bins = np.bincount(pres_kmeans.astype(int).flatten())\n",
    "        plt.bar(range(len(bins)), bins, color='grey')\n",
    "        #plt.hist(pres_kmeans.astype(int))\n",
    "        plt.title('%s%d clusters, strategy %s, bins edges %s' % (title_prefix, param_set['n_bins'], param_set['strategy'], enc.bin_edges_))\n",
    "        plt.show()\n",
    "\n",
    "        results[idx] = compute_discretization_scores(data, pres_kmeans)\n",
    "    if orig_discretization is not None:\n",
    "        results[-1] = compute_discretization_scores(data, orig_discretization)\n",
    "    myindex = [\"%s (%s)\" % (p['n_bins'], p['strategy']) for p in discretization_params_grid]\n",
    "    myindex.append(\"3 (orig)\")\n",
    "    cluster_results = pd.DataFrame(data=results, index=myindex, columns=['Calinski-Harabaz', 'Silhouette', 'Davies-Bouldin',\n",
    "                                                                        'min cluster nb', 'max cluster nb'])\n",
    "    cluster_results['C-H'] = (cluster_results['Calinski-Harabaz'] - cluster_results['Calinski-Harabaz'].mean()) / cluster_results['Calinski-Harabaz'].std()\n",
    "    cluster_results['S'] = (cluster_results['Silhouette'] - cluster_results['Silhouette'].mean()) / cluster_results['Silhouette'].std()\n",
    "    cluster_results['D-B'] = (cluster_results['Davies-Bouldin'] - cluster_results['Davies-Bouldin'].mean()) / cluster_results['Davies-Bouldin'].std()\n",
    "    cluster_results['Weighted'] = cluster_results['D-B'] - cluster_results['C-H'] - cluster_results['S']\n",
    "    return cluster_results\n",
    "\n",
    "def highlight_max(x):\n",
    "    return ['color: red' if v == x.max() else 'color: green' if v == x.min() else '' for v in x]\n",
    "\n",
    "presence = all_p_df.sort_values(by=['Presence Score'])['Presence Score']\n",
    "copresence = all_p_df.sort_values(by=['Co-presence Score'])['Co-presence Score']\n",
    "orig_pres = all_p_df.sort_values(by=['Presence Score'])['PresenceClass']\n",
    "orig_copres = all_p_df.sort_values(by=['Co-presence Score'])['CopresenceClass']\n",
    "print(len(presence))\n",
    "print(len(orig_pres))\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "pres_clusters_results = discretization_gridsearch(presence, discretization_params, orig_pres, 'presence, ')\n",
    "copres_clusters_results = discretization_gridsearch(copresence, discretization_params, orig_copres, 'co-presence, ')\n",
    "np.set_printoptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pres_clusters_results.style.apply(highlight_max).to_excel(os.path.join(config.OUT_PATH, 'presence_discretization.xlsx'))\n",
    "#copres_clusters_results.style.apply(highlight_max).to_excel(os.path.join(config.OUT_PATH, 'copresence_discretization.xlsx'))\n",
    "\n",
    "def highlight_low_clusters(s):\n",
    "    if s['min cluster nb'] < 7.0: # because upsampling algorithms consider 6 neighbors\n",
    "        return ['color: red']*9\n",
    "    else:\n",
    "        return ['background-color: white']*9\n",
    "\n",
    "pres_clusters_results.sort_values(by=['Weighted']).style.apply(highlight_low_clusters, axis=1).to_excel(os.path.join(config.OUT_PATH, 'presence_discretization.xlsx'))\n",
    "copres_clusters_results.sort_values(by=['Weighted']).style.apply(highlight_low_clusters, axis=1).to_excel(os.path.join(config.OUT_PATH, 'copresence_discretization.xlsx'))\n",
    "pres_clusters_results.sort_values(by=['Weighted']).style.apply(highlight_low_clusters, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copres_clusters_results.sort_values(by=['Weighted']).style.apply(highlight_low_clusters, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best discretization corresponds to highest values for Calinsky-Harabaz and Silouhette, and lowest values for Davies-Bouldin indices.\n",
    "Columns C-H, S and D-B are centered / reduced values of each index, weighted is D-B minus S and C-H. By looking at lowest values in 'Weighted' we hope to maximize C-H and S while minimizing D-B.\n",
    "(lines in red are suppressed because they have too small cluster(s)).\n",
    "\n",
    "But effect on learning tasks has to be experimented. Results can be confirmed as interesting based on table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features definition\n",
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features including phases\n",
    "\n",
    "feats_df_index = ['type']\n",
    "feats_df_columns = all_p_df.columns\n",
    "\n",
    "feats_df = pd.DataFrame(index=feats_df_index, columns=feats_df_columns)\n",
    "feats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features including phases\n",
    "\n",
    "features_df_index = [\n",
    "             ['Doctor', 'Agent', 'Doctor+Agent'], \n",
    "             ['No Phase', '157015', 'Verbal', 'Non-Verbal', 'Multimodal', 'Non-Verbal-A', 'Multimodal-A', \n",
    "              'No Phase-A', '157015-A', 'No Phase-FULL', '157015-FULL']   #, 'Multimodal+Duration']\n",
    "            ]\n",
    "features_df_columns = ['precision', 'f1', 'recall', 'params']\n",
    "features_df_multiindex = pd.MultiIndex.from_product(features_df_index, \n",
    "                                                  names=['Subject', 'Feature Set'])\n",
    "\n",
    "features_df = pd.DataFrame(index=features_df_multiindex)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_set(subject_name, set_name):\n",
    "    row = features_df.loc[(subject_name, set_name)]\n",
    "    return features_df.columns[row == True]\n",
    "\n",
    "# Load features sets definition\n",
    "features_df = pd.read_excel(os.path.join(config.OUT_PATH, 'features_sets.xlsx'))\n",
    "features_df.index = features_df_multiindex\n",
    "features_df = features_df.drop(['Subject', 'Feature Set'], axis=1)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optional - (re)initialize features sets if necessary\n",
    "\n",
    "mean_angular_features = [\n",
    "    \"MeanMeanAngularSpeed_Head_Start\", \"MeanMeanAngularSpeed_Head_Mid\", \"MeanMeanAngularSpeed_Head_End\",\n",
    "    \"Avg_MeanMeanAngularSpeed_Hand_Start\", \"Avg_MeanMeanAngularSpeed_Hand_Mid\", \"Avg_MeanMeanAngularSpeed_Hand_End\"\n",
    "]\n",
    "std_angular_features = [\n",
    "    \"StdMeanAngularSpeed_Head_Start\", \"StdMeanAngularSpeed_Head_Mid\", \"StdMeanAngularSpeed_Head_End\",                    \n",
    "    \"Avg_StdMeanAngularSpeed_Hand_Start\", \"Avg_StdMeanAngularSpeed_Hand_Mid\", \"Avg_StdMeanAngularSpeed_Hand_End\"\n",
    "]\n",
    "subject_features = [\n",
    "    'Expert', \"Head_Entropy_Start\", \"Head_Entropy_Mid\", \"Head_Entropy_End\", \"Avg_HandEntropy_Begin\",\n",
    "    \"Avg_HandEntropy_Mid\", \"Avg_HandEntropy_End\", \"Avg_SentenceLength_Begin\", \"Avg_SentenceLength_Mid\",\n",
    "    \"Avg_SentenceLength_End\",  \"Avg_AnswersDelay_Begin\", \"Avg_AnswersDelay_Mid\", \"Avg_AnswersDelay_End\",\n",
    "    \"Avg_IPUlen_Begin\", \"Avg_IPUlen_Mid\", \"Avg_IPUlen_End\",\n",
    "    \"Ratio1_Begin\", \"Ratio1_Mid\", \"Ratio1_End\", \"Ratio2_Begin\", \"Ratio2_Mid\", \n",
    "    \"Ratio2_End\", \"Duration\"] \n",
    "\n",
    "agent_features = [feat+'_agent' for feat in subject_features if feat not in ['Expert', 'Duration'] and 'Angular' not in feat] + ['Duration']\n",
    "# not using \"Expert\" feature for agent features\n",
    "#agent_features = subject_features - {\"Expert\"}\n",
    "all_features = subject_features + agent_features[:-1]\n",
    "\n",
    "subject_features_a = [feat for feat in subject_features if not 'Entropy' in feat] + mean_angular_features + std_angular_features\n",
    "agent_features_a = [feat+'_agent' for feat in subject_features_a if feat not in ['Expert', 'Duration']] + ['Duration']\n",
    "# not using \"Expert\" feature for agent features\n",
    "#agent_features = subject_features - {\"Expert\"}\n",
    "all_features_a = subject_features_a + agent_features_a[:-1]\n",
    "\n",
    "subject_features_full = subject_features + mean_angular_features + std_angular_features\n",
    "all_features_full = subject_features_full + agent_features_a[:-1]\n",
    "\n",
    "features_df = pd.DataFrame(index=features_df_multiindex, columns=sorted(list(set(all_features+all_features_a))))\n",
    "\n",
    "\n",
    "# features - no phase\n",
    "\n",
    "subject_features_nophase = [feat for feat in subject_features if not (feat.endswith('_Begin') \n",
    "                            or feat.endswith('_End') or feat.endswith('_Start') or 'Angular' in feat)]\n",
    "#[\"Expert\", \"Head_Entropy_Mid\", \"Avg_HandEntropy_Mid\", \"Avg_SentenceLength_Mid\", \"Ratio1_Mid\", \n",
    "#                            \"Ratio2_Mid\", \"Duration\"]\n",
    "agent_features_nophase = [feat for feat in agent_features if not (feat.endswith('_Begin_agent') \n",
    "                            or feat.endswith('_End_agent') or feat.endswith('_Start_agent') or 'Angular' in feat)]\n",
    "print(agent_features)\n",
    "print(agent_features_nophase)\n",
    "all_features_nophase = subject_features_nophase + agent_features_nophase[:-1]\n",
    "\n",
    "\n",
    "subject_features_nophase_a = [feat for feat in subject_features_a if not (feat.endswith('_Begin') \n",
    "                            or feat.endswith('_End') or feat.endswith('_Start'))]\n",
    "#[\"Expert\", \"Head_Entropy_Mid\", \"Avg_HandEntropy_Mid\", \"Avg_SentenceLength_Mid\", \"Ratio1_Mid\", \n",
    "#                            \"Ratio2_Mid\", \"Duration\"]\n",
    "agent_features_nophase_a = [feat for feat in agent_features_a if not (feat.endswith('_Begin_agent') \n",
    "                            or feat.endswith('_End_agent') or feat.endswith('_Start_agent'))]\n",
    "print(agent_features)\n",
    "print(agent_features_nophase)\n",
    "all_features_nophase_a = subject_features_nophase_a + agent_features_nophase_a[:-1]\n",
    "\n",
    "subject_features_nophase_full =  [feat for feat in subject_features_full if not (feat.endswith('_Begin') \n",
    "                            or feat.endswith('_End') or feat.endswith('_Start'))]\n",
    "all_features_nophase_full = subject_features_nophase_full + agent_features_nophase_a[:-1]             \n",
    "\n",
    "# for modalities verbal, non-verbal etc, we do not consider phases\n",
    "# verbal features\n",
    "\n",
    "subject_features_verbal = ['Avg_AnswersDelay_Mid', \"Avg_SentenceLength_Mid\", \"Avg_IPUlen_Mid\", \"Ratio1_Mid\", \"Ratio2_Mid\"]\n",
    "agent_features_verbal = [feat+'_agent' for feat in subject_features_verbal]\n",
    "all_features_verbal = subject_features_verbal + agent_features_verbal\n",
    "\n",
    "# non-verbal features\n",
    "\n",
    "subject_features_nonverbal = [\"Head_Entropy_Mid\", \"Avg_HandEntropy_Mid\"]\n",
    "agent_features_nonverbal = [feat+'_agent' for feat in subject_features_nonverbal]\n",
    "all_features_nonverbal = subject_features_nonverbal + agent_features_nonverbal\n",
    "\n",
    "subject_features_nonverbal_a =  [f for f in mean_angular_features if '_Mid' in f and ('_Head' in f or 'Avg_' in f)] + [f for f in std_angular_features if '_Mid' in f and ('_Head' in f or 'Avg_' in f)]\n",
    "agent_features_nonverbal_a = [feat+'_agent' for feat in subject_features_nonverbal_a]\n",
    "all_features_nonverbal_a = subject_features_nonverbal_a + agent_features_nonverbal_a\n",
    "\n",
    "# multimodal features\n",
    "\n",
    "subject_features_multimodal = subject_features_verbal + subject_features_nonverbal\n",
    "agent_features_multimodal = [feat+'_agent' for feat in subject_features_multimodal]\n",
    "all_features_multimodal = subject_features_multimodal + agent_features_multimodal\n",
    "\n",
    "subject_features_multimodal_a = subject_features_verbal + subject_features_nonverbal_a\n",
    "agent_features_multimodal_a = [feat+'_agent' for feat in subject_features_multimodal_a]\n",
    "all_features_multimodal_a = subject_features_multimodal_a + agent_features_multimodal_a\n",
    "\n",
    "# multimodal + duration features\n",
    "\n",
    "#subject_features_multimodal_duration = subject_features_multimodal + ['Duration']\n",
    "#agent_features_multimodal_duration = agent_features_multimodal + ['Duration']\n",
    "#all_features_multimodal_duration = subject_features_multimodal_duration + agent_features_multimodal\n",
    "\n",
    "for feat in features_df.columns:\n",
    "    features_df.loc[('Doctor', 'No Phase'), feat] = True if feat in subject_features_nophase else False\n",
    "    features_df.loc[('Doctor', '157015'), feat] = True if feat in subject_features else False\n",
    "    features_df.loc[('Doctor', 'No Phase-A'), feat] = True if feat in subject_features_nophase_a else False\n",
    "    features_df.loc[('Doctor', '157015-A'), feat] = True if feat in subject_features_a else False    \n",
    "    features_df.loc[('Doctor', 'No Phase-FULL'), feat] = True if feat in subject_features_nophase_full else False\n",
    "    features_df.loc[('Doctor', '157015-FULL'), feat] = True if feat in subject_features_full else False    \n",
    "    features_df.loc[('Doctor', 'Verbal'), feat] = True if feat in subject_features_verbal else False\n",
    "    features_df.loc[('Doctor', 'Non-Verbal'), feat] = True if feat in subject_features_nonverbal else False\n",
    "    features_df.loc[('Doctor', 'Non-Verbal-A'), feat] = True if feat in subject_features_nonverbal_a else False    \n",
    "    features_df.loc[('Doctor', 'Multimodal'), feat] = True if feat in subject_features_multimodal else False\n",
    "    features_df.loc[('Doctor', 'Multimodal-A'), feat] = True if feat in subject_features_multimodal_a else False    \n",
    "    #features_df.loc[('Doctor', 'Multimodal+Duration'), feat] = True if feat in subject_features_multimodal_duration else False\n",
    "    features_df.loc[('Agent', 'No Phase'), feat] = True if feat in agent_features_nophase else False\n",
    "    features_df.loc[('Agent', '157015'), feat] = True if feat in agent_features else False\n",
    "    features_df.loc[('Agent', 'No Phase-A'), feat] = True if feat in agent_features_nophase_a else False\n",
    "    features_df.loc[('Agent', '157015-A'), feat] = True if feat in agent_features_a else False    \n",
    "    features_df.loc[('Agent', 'No Phase-FULL'), feat] = True if feat in agent_features_nophase_a else False\n",
    "    features_df.loc[('Agent', '157015-FULL'), feat] = True if feat in agent_features_a else False    \n",
    "    features_df.loc[('Agent', 'Verbal'), feat] = True if feat in agent_features_verbal else False\n",
    "    features_df.loc[('Agent', 'Non-Verbal'), feat] = True if feat in agent_features_nonverbal else False\n",
    "    features_df.loc[('Agent', 'Non-Verbal-A'), feat] = True if feat in agent_features_nonverbal_a else False    \n",
    "    features_df.loc[('Agent', 'Multimodal'), feat] = True if feat in agent_features_multimodal else False\n",
    "    features_df.loc[('Agent', 'Multimodal-A'), feat] = True if feat in agent_features_multimodal_a else False\n",
    "    #features_df.loc[('Agent', 'Multimodal+Duration'), feat] = True if feat in agent_features_multimodal_duration else False\n",
    "    features_df.loc[('Doctor+Agent', 'No Phase'), feat] = True if feat in all_features_nophase else False\n",
    "    features_df.loc[('Doctor+Agent', '157015'), feat] = True if feat in all_features else False\n",
    "    features_df.loc[('Doctor+Agent', 'No Phase-A'), feat] = True if feat in all_features_nophase_a else False\n",
    "    features_df.loc[('Doctor+Agent', '157015-A'), feat] = True if feat in all_features_a else False\n",
    "    features_df.loc[('Doctor+Agent', 'No Phase-FULL'), feat] = True if feat in all_features_nophase_full else False\n",
    "    features_df.loc[('Doctor+Agent', '157015-FULL'), feat] = True if feat in all_features_full else False\n",
    "    features_df.loc[('Doctor+Agent', 'Verbal'), feat] = True if feat in all_features_verbal else False\n",
    "    features_df.loc[('Doctor+Agent', 'Non-Verbal'), feat] = True if feat in all_features_nonverbal else False\n",
    "    features_df.loc[('Doctor+Agent', 'Non-Verbal-A'), feat] = True if feat in all_features_nonverbal_a else False    \n",
    "    features_df.loc[('Doctor+Agent', 'Multimodal'), feat] = True if feat in all_features_multimodal else False\n",
    "    features_df.loc[('Doctor+Agent', 'Multimodal-A'), feat] = True if feat in all_features_multimodal_a else False\n",
    "    #features_df.loc[('Doctor+Agent', 'Multimodal+Duration'), feat] = True if feat in all_features_multimodal_duration else False\n",
    "\n",
    "features_df.to_excel(os.path.join(config.OUT_PATH, 'features_sets.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import cmp_to_key\n",
    "\n",
    "def cmp_feats(a,b):\n",
    "    #print('%s - %s' % (a,b))\n",
    "    \n",
    "    if a == b:\n",
    "        return 0\n",
    "    \n",
    "    r = r\"(.*)(_Begin|_Start|_Mid|_End)(_agent)?\"\n",
    "    ar = re.findall(r, a)\n",
    "    if len(ar) > 0:\n",
    "        ar = [val for val in ar[0] if val is not '']\n",
    "    else:\n",
    "        ar = [a]\n",
    "    br = re.findall(r, b)\n",
    "    if len(br) > 0:\n",
    "        br = [val for val in br[0] if val is not '']\n",
    "    else:\n",
    "        br = [b]\n",
    "    #print(str(ar) + ' ' + str(len(ar)))\n",
    "    #print(str(br) + ' ' + str(len(br)))\n",
    "    \n",
    "    if len(ar) == 1:\n",
    "        if len(br) == 1:\n",
    "            if a > b:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1\n",
    "    if len(br) == 1:\n",
    "        return 1\n",
    "    \n",
    "    if len(ar) == 2:\n",
    "        if len(br) == 2:\n",
    "            if ar[0] != br[0]:\n",
    "                if a > b:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            if ar[1] == '_Begin' or ar[1] == '_Start':\n",
    "                return -1\n",
    "            if br[1] == '_Begin' or br[1] == '_Start':\n",
    "                return 1\n",
    "            if ar[1] == '_End':\n",
    "                return 1\n",
    "            if br[1] == '_End':\n",
    "                return -1\n",
    "            \n",
    "    if len(ar) == 3:\n",
    "        if len(br) == 2:\n",
    "            return 1\n",
    "        #print('AGENT')\n",
    "        return cmp_feats(ar[0]+ar[1], br[0]+br[1])\n",
    "        \n",
    "    if len(br) == 3:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "l = list(features_df.columns)\n",
    "#print(l)\n",
    "l = sorted(l, key=cmp_to_key(cmp_feats))\n",
    "features_df = features_df[l]\n",
    "#print(features_df.columns)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(features_df.fillna(0).astype('int'), linecolor='white', linewidth=1, annot=False, cbar=False)\n",
    "plt.savefig(os.path.join(config.OUT_PATH, 'features_sets.png'), bbox_inches = \"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export datasets as Excel sheets for other analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = False\n",
    "\n",
    "if export_df:\n",
    "    cols = list(get_features_set('Doctor+Agent', 'No Phase-FULL'))\n",
    "    cols.append('Candidate')\n",
    "    cols.append('Environment')\n",
    "    cols.append('Presence Score')\n",
    "    cols.append('Co-presence Score')\n",
    "    print(all_np_df.columns)\n",
    "    df = all_np_df[cols]\n",
    "    df = df.dropna(axis='columns', how='all')\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "    l = list(df.columns)\n",
    "    print(l)\n",
    "    l.sort(cmp_feats)\n",
    "    df = df[l]\n",
    "    display(HTML(df.to_html()))\n",
    "    df.to_excel('dataset_nophase.xlsx')\n",
    "    \n",
    "    cols = list(get_features_set('Doctor+Agent', '157015-FULL'))\n",
    "    cols.append('Candidate')\n",
    "    cols.append('Environment')\n",
    "    cols.append('Presence Score')\n",
    "    cols.append('Co-presence Score')\n",
    "    print(all_p_df.columns)\n",
    "    df = all_p_df[cols]\n",
    "    df = df.dropna(axis='columns', how='all')\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "    l = list(df.columns)\n",
    "    print(l)\n",
    "    l.sort(cmp_feats)\n",
    "    df = df[l]\n",
    "    display(HTML(df.to_html()))\n",
    "    df.to_excel('dataset_phases.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_from_estimator(estimator):\n",
    "    \"\"\"\n",
    "    Returns the classifier from a built estimator (that could be a classifier already, or a pipeline including a classifier).\n",
    "    If there is none returns None.\n",
    "    \"\"\"\n",
    "    if not isinstance(estimator, Pipeline):\n",
    "        return estimator\n",
    "    else:\n",
    "        if 'clf' in estimator.named_steps:\n",
    "            return estimator.named_steps['clf']\n",
    "    \n",
    "    return None\n",
    "           \n",
    "\n",
    "def discretize_class(samples, prediction_type='classification', prediction_task='presence', bins=None, strategy=None, test_set=0.1, verbose=0):\n",
    "    \"\"\"\n",
    "    Discretizes presence or co-presence score and put result in a new column (if not already existing).\n",
    "    \n",
    "    Returns: the name of the new column, or None if one of discretized groups only has 2 samples or less, or if discretization\n",
    "    failed for any reason.\n",
    "    \"\"\"\n",
    "    \n",
    "    if prediction_type == 'regression':\n",
    "        if prediction_task == 'presence':\n",
    "            return 'Presence Score'\n",
    "        else:\n",
    "            return 'Co-presence Score'\n",
    "    \n",
    "    cols = []\n",
    "    for col in samples.columns:\n",
    "        if 'core' in col:\n",
    "            cols.append(col)\n",
    "    if verbose > 1:\n",
    "        print('discretize_class columns %s' % str(cols))\n",
    "    if bins is None or strategy is None:\n",
    "        if prediction_task == 'presence':\n",
    "            class_column='PresenceClass'\n",
    "        else:\n",
    "            class_column='CopresenceClass'\n",
    "        return class_column\n",
    "    else:\n",
    "        if prediction_task == 'presence':\n",
    "            score_column = 'Presence Score'\n",
    "            class_column = 'PresenceClass_%d_%s' % (bins, strategy)\n",
    "        else:\n",
    "            score_column = 'Co-presence Score'\n",
    "            class_column = 'CopresenceClass_%d_%s' % (bins, strategy)      \n",
    "    \n",
    "    if class_column not in samples.columns:\n",
    "        samples[class_column], enc = discretize(samples[score_column], bins, strategy)\n",
    "        samples[class_column] = samples[class_column].astype(int)\n",
    "        #TODO: transform test data with discretizer fit on train data\n",
    "    \n",
    "    if any(samples[class_column] == -1) or any(samples[class_column].value_counts() < 3):\n",
    "        return None\n",
    "    else:\n",
    "        return class_column\n",
    "\n",
    "def prepare_train_data(samples, features, prediction_task, bins=None, strategy=None, upsample=None, \n",
    "                       test_set=0.1, n_splits=1, splits_mode='shuffle', verbose=0):\n",
    "    \"\"\"\n",
    "    Prepares a dataset for ML.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    samples: DataFrame\n",
    "    \n",
    "    features: list\n",
    "    \n",
    "    prediction_task: str\n",
    "        'presence' or 'copresence'\n",
    "        \n",
    "    bins: None or int, default None.\n",
    "        Number of classes to create when discretizing presence and co-presence.\n",
    "        If 0 there no discretization is applied and returned y is for a regression task.\n",
    "    \n",
    "    strategy: None, 'kmeans', 'uniform' or 'quantile', default None.\n",
    "        See sklearn, if None defaults to original strategy.\n",
    "        \n",
    "    upsample: None or str, default None\n",
    "        None: no oversampling\n",
    "        For description of other oversampling method see https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html\n",
    "        imb_random\n",
    "        imb_smote\n",
    "        imb_adasyn\n",
    "        imb_smotenc\n",
    "        imb_borderlinesmote1\n",
    "        imb_borderlinesmote2\n",
    "        imb_svmsmote\n",
    "        \n",
    "    test_set: None or float, default 0.1\n",
    "        If not None or 0, a test set will produced with specified float as test ratio\n",
    "        \n",
    "    n_splits: int, default 1\n",
    "        Will produce this number of train/test splits.\n",
    "        \n",
    "    splits_mode: str, default 'shuffle'\n",
    "        'shuffle' to produce n_splits stratified and shuffled splits of the data.\n",
    "        'kfold' to produce n_splits repetitions of K-folds splits, K being set to approximately respect test_set ratio.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    An array of tuples (X, y, X_test, y_test).\n",
    "    \n",
    "    \"\"\"\n",
    "    if verbose > 1:\n",
    "        print('prepare_train_data(samples nb=%d, features=%s, prediction_task=%s, bins=%s, strategy=%s, upsample=%s, test_set=%f, n_splits=%d)' \n",
    "              % (len(samples), str(features), prediction_task, str(bins), str(strategy), str(upsample), test_set, n_splits))\n",
    "        #print('prepare_train_data: unique %d' %  len(np.unique(samples.loc[:, features], return_counts=True, axis=0)[1]))\n",
    "    \n",
    "    return_tuples = []\n",
    "    \n",
    "    # TODO: should fit discretizer on train then transform train and test\n",
    "    if bins == 0:\n",
    "        class_column = 'Presence Score'\n",
    "    else:\n",
    "        class_column = discretize_class(samples, 'classification', prediction_task, bins, strategy, verbose)\n",
    "    if verbose > 1:\n",
    "        print('class_column: %s' % class_column)\n",
    "    if test_set is not None and test_set > 0:\n",
    "        if bins == 0:\n",
    "            # regression\n",
    "            sss = ShuffleSplit(n_splits=n_splits, test_size=test_set, random_state=RANDOM_STATE)\n",
    "            splits = [[split[0], split[1]] for split in sss.split(np.zeros(len(samples)), samples[class_column])]\n",
    "        else:\n",
    "            if splits_mode == 'shuffle':\n",
    "                sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_set, random_state=RANDOM_STATE)\n",
    "                splits = [[split[0], split[1]] for split in sss.split(np.zeros(len(samples)), samples[class_column])]\n",
    "            else:\n",
    "                nb_folds = int(1/test_set)\n",
    "                sss = RepeatedStratifiedKFold(n_splits=nb_folds, n_repeats=n_splits, random_state=RANDOM_STATE)\n",
    "                splits = [[split[0], split[1]] for split in sss.split(np.zeros(len(samples)), samples[class_column])]\n",
    "    else:\n",
    "        splits = [[None, None]]\n",
    "    if verbose > 1:\n",
    "        print(\"splits \" + str(splits))\n",
    "    for idx, split in enumerate(splits):\n",
    "        train_idx = split[0]\n",
    "        test_idx = split[1]\n",
    "        if verbose > 1:\n",
    "            print(\"train \" + str(train_idx) + \", test \" + str(test_idx))\n",
    "        if train_idx is None or test_idx is None:\n",
    "            X = samples.loc[:, features].fillna(0)\n",
    "            y = samples[class_column]\n",
    "            X_test = None\n",
    "            y_test = None\n",
    "        else:\n",
    "            X = samples.loc[train_idx, features].fillna(0)\n",
    "            X_test = samples.loc[test_idx, features].fillna(0)\n",
    "            y = samples.loc[train_idx, class_column]\n",
    "            y_test = samples.loc[test_idx, class_column]        \n",
    "        \n",
    "        ros = None\n",
    "        if upsample == 'imb_random':\n",
    "            ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "        elif upsample == 'imb_smote':\n",
    "            ros = SMOTE(random_state=RANDOM_STATE) \n",
    "        elif upsample == 'imb_adasyn':\n",
    "            ros = ADASYN(random_state=RANDOM_STATE)\n",
    "        elif upsample == 'imb_smotenc':\n",
    "            if 'Expert' in features:        \n",
    "                # SMOTENC is the only one dealing specifically with categorical data (ie 'Expert' feature)\n",
    "                categorical_vars_indices = [list(features).index('Expert')]\n",
    "                ros = SMOTENC(random_state=RANDOM_STATE, categorical_features=categorical_vars_indices)\n",
    "            else:\n",
    "                ros = SMOTE(random_state=RANDOM_STATE)\n",
    "\n",
    "        elif upsample == 'imb_borderlinesmote1':\n",
    "            ros = BorderlineSMOTE(kind='borderline-1', random_state=RANDOM_STATE)      \n",
    "        elif upsample == 'imb_borderlinesmote2':\n",
    "            ros = BorderlineSMOTE(kind='borderline-2', random_state=RANDOM_STATE)     \n",
    "        elif upsample == 'imb_svmsmote':\n",
    "            ros = SVMSMOTE(random_state=RANDOM_STATE)\n",
    "\n",
    "        if ros is not None:\n",
    "            # test set is never oversampled\n",
    "            if verbose > 1:\n",
    "                print(\"prepare_train_data: Applying %s oversampling to data %d / %s / unique %d\" \n",
    "                      % (ros.__class__.__name__, len(X), collections.Counter(y), \n",
    "                         len(X.drop_duplicates()) )) \n",
    "            X, y = ros.fit_resample(X, y)     \n",
    "            X = pd.DataFrame(X, columns=features)\n",
    "            if verbose > 1:\n",
    "                print(\"prepare_train_data: Resampled data %d / %s / unique %d\" \n",
    "                      % (len(X), collections.Counter(y), len(X.drop_duplicates())))\n",
    "\n",
    "        # ! not needed anymore because .arff needs to be generated on-the-fly (to take into consideriation\n",
    "        # a pipeline where dataset could be standard scaled, features selected, etc)\n",
    "        # special treatment for JNCC2, generate .arff files\n",
    "        \"\"\"if isinstance(clf, JNCC2Wrapper) and False:\n",
    "            if bins == 0:\n",
    "                print(\"ERROR: arff can only be generated for classification task\")\n",
    "                return None\n",
    "            else:\n",
    "                arff_path = os.path.join(clf.arff_root_path_, str(idx))\n",
    "                clf.generate_arff(os.path.join(arff_path, 'train'), features, prediction_task, X, y)\n",
    "                if test_set is not None and test_set > 0:\n",
    "                    clf.generate_arff(os.path.join(arff_path, 'test'), features, prediction_task, X_test, y_test)\n",
    "        \"\"\"\n",
    "        \n",
    "                \n",
    "        return_tuples.append( (X, y, X_test, y_test) )\n",
    "        \n",
    "    return return_tuples\n",
    "\n",
    "def build_scoring(clf, y):\n",
    "    labels = sorted(np.unique(y).astype(int))\n",
    "    \n",
    "    scoring = {}\n",
    "    scoring['f1_macro_weighted'] = 'f1_weighted'\n",
    "    for idx, label in enumerate(labels):\n",
    "        scoring['f1_macro_class_%d' % label] = make_scorer(f1_score, labels=[label], average='weighted')\n",
    "    scoring['precision_macro_weighted'] = 'precision_weighted'\n",
    "    for idx, label in enumerate(labels):\n",
    "        scoring['precision_macro_class_%d' % label] = make_scorer(precision_score, labels=[label], average='weighted')    \n",
    "    scoring['recall_macro_weighted'] = 'recall_weighted'\n",
    "    for idx, label in enumerate(labels):\n",
    "        scoring['recall_macro_class_%d' % label] = make_scorer(recall_score, labels=[label], average='weighted')     \n",
    "    #scoring['f1_micro'] = 'f1_micro'\n",
    "    \n",
    "    if type(clf) == RandomForestClassifier:\n",
    "        scoring ['oob'] = score_forest\n",
    "    else:\n",
    "        scoring['fi'] = svm_fi    \n",
    "    if isinstance(clf, JNCC2Wrapper) or (isinstance(clf, Pipeline) and 'clf' in clf.named_steps \n",
    "                                        and isinstance(clf.named_steps['clf'], JNCC2Wrapper)):\n",
    "        scoring['mrr'] = make_scorer(lambda yt,yp: feu.compute_mrr(yt, yp, labels))\n",
    "        \n",
    "        # Note: determinacy is useless for cross-validation scores, as in this case JNCC2 will return only deterministic\n",
    "        # predictions - this score would always be 1. It then makes sense only for test scores (where JNCC2 may return\n",
    "        #  non-deterministic predictions)\n",
    "        #scoring['determinacy'] = make_scorer(lambda yt, yp: np.sum(np.sum(yp != 6666, axis=1) == 1) / float(len(yp)))\n",
    "\n",
    "    return scoring\n",
    "\n",
    "def gridsearch(clf, X, y, prediction_task, param_grid, features, cv=10, n_jobs=-1, verbose=0):\n",
    "    if verbose > 1:\n",
    "        print(\"gridsearch(clf=%s, modelTarget=%s, param_grid=%s, features=%s)\" \n",
    "              % (type(clf), prediction_task, param_grid, features))\n",
    "\n",
    "    #n_estimators = np.concatenate((np.arange(1,10), np.arange(10,100,10)))\n",
    "    print(np.unique(y, return_counts=True))\n",
    "    grid = GridSearchCV(estimator=clf, param_grid=param_grid,\n",
    "                        scoring=['f1_weighted', 'precision_weighted', 'recall_weighted'],\n",
    "                        refit='f1_weighted',\n",
    "                        cv=cv,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=n_jobs if not isinstance(get_clf_from_estimator(clf), JNCC2Wrapper) else 1,\n",
    "                        verbose=max(0, verbose-1))\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    results = grid.cv_results_\n",
    "    # print(\"best params \", grid.best_params_)\n",
    "    # print(\"best score \", grid.best_score_)\n",
    "\n",
    "    return grid\n",
    "\n",
    "def gridsearch2(clf, X, y, prediction_task, param_grid, features, cv=10, n_jobs=-1, verbose=0):\n",
    "    if verbose > 1:\n",
    "        print(\"gridsearch(clf=%s/%s, prediction_task=%s, param_grid=%s, features=%s, cv=%d, n_jobs=%d)\" \n",
    "              % (type(clf), type(get_clf_from_estimator(clf)), prediction_task, param_grid, features, cv, n_jobs))\n",
    "\n",
    "    if verbose > 2:\n",
    "        print('gridsearch: support %s' % str(np.unique(y, return_counts=True)))\n",
    "    grid = GridSearchCV(estimator=clf, param_grid=param_grid,\n",
    "                        scoring=build_scoring(clf, y),\n",
    "                        refit='f1_macro_weighted',\n",
    "                        cv=cv,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=n_jobs,\n",
    "                        verbose=max(0, verbose-1))\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rfpimp\n",
    "\n",
    "def dropcol_importances_sklearn(model, X_train, y_train, X_valid=None, y_valid=None, metric=None, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Compute drop-column feature importances for scikit-learn.\n",
    "\n",
    "    Given a classifier or regression in model\n",
    "    and training X and y data, return a data frame with columns\n",
    "    Feature and Importance sorted in reverse order by importance.\n",
    "\n",
    "    A clone of model is trained once to get the baseline score and then\n",
    "    again, once per feature to compute the drop in either the model's .score() output\n",
    "    or a custom metric callable in the form of metric(model, X_valid, y_valid).\n",
    "    In case of a custom metric the X_valid and y_valid parameters should be set.\n",
    "    return: A data frame with Feature, Importance columns\n",
    "\n",
    "    SAMPLE CODE\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    X_train, y_train = ..., ...\n",
    "    rf.fit(X_train, y_train)\n",
    "    imp = dropcol_importances(rf, X_train, y_train)\n",
    "    \"\"\"\n",
    "\n",
    "    if X_valid is None: X_valid = X_train\n",
    "    if y_valid is None: y_valid = y_train\n",
    "    model_ = clone(model)\n",
    "    #model_.random_state = RANDOM_STATE\n",
    "    model_.fit(X_train, y_train)\n",
    "\n",
    "    if callable(metric):\n",
    "        baseline = metric(model_, X_valid, y_valid)\n",
    "    else:\n",
    "        baseline = model_.score(X_valid, y_valid)\n",
    "    print(\"baseline \" + str(baseline))\n",
    "    imp = []\n",
    "    for col in X_train.columns:\n",
    "        print(\"column \" + col)\n",
    "        model__ = clone(model)\n",
    "        #model__.random_state = RANDOM_STATE\n",
    "        X_tr = X_train.drop(col, axis=1)\n",
    "        X_va = X_valid.drop(col, axis=1)\n",
    "        model__.fit(X_tr, y_train)\n",
    "        if callable(metric):\n",
    "            s = metric(model__, X_va, y_valid)\n",
    "        else:\n",
    "            s = model__.score(X_va, y_valid)\n",
    "        print(\"new score \" + str(s))\n",
    "        drop_in_score = baseline - s\n",
    "        imp.append(drop_in_score)\n",
    "\n",
    "    imp = np.array(imp)\n",
    "\n",
    "    #I = pd.DataFrame(data={'Feature':X_train.columns, 'Importance':imp})\n",
    "    #I = I.set_index('Feature')\n",
    "    #I = I.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return imp #I\n",
    "\n",
    "def dropcol_importances(rf, X_train, y_train, random_state):\n",
    "    rf_ = clone(rf)\n",
    "    rf_.random_state = random_state\n",
    "    rf_.fit(X_train, y_train)\n",
    "    baseline = rf_.oob_score_\n",
    "    #print(\"baseline \" + str(baseline))\n",
    "    imp = []\n",
    "    for col in X_train.columns:\n",
    "        X = X_train.drop(col, axis=1)\n",
    "        rf_ = clone(rf)\n",
    "        rf_.random_state = random_state\n",
    "        rf_.fit(X, y_train)\n",
    "        o = rf_.oob_score_\n",
    "        imp.append(baseline - o)\n",
    "    imp = np.array(imp)\n",
    "    #I = pd.DataFrame(\n",
    "    #        data={'Feature':X_train.columns,\n",
    "    #              'Importance':imp})\n",
    "    #I = I.set_index('Feature')\n",
    "    #I = I.sort_values('Importance', ascending=True)\n",
    "    return imp #I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling of Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(name, verbose=0):\n",
    "    regex = r\"fs_(.*)_(.*)\"\n",
    "\n",
    "    matches = re.findall(regex, name)\n",
    "    print(matches)\n",
    "    \n",
    "    # feature selection\n",
    "    if len(matches) > 0:\n",
    "        fs_name = matches[0][0]\n",
    "        print('feature selection method %s' % fs_name)\n",
    "        clf_name = matches[0][1]\n",
    "        print('classifier %s' % clf_name)\n",
    "        steps = []\n",
    "        if not 'forest' in clf_name:\n",
    "            steps.append(('scaler', make_column_transformer((FunctionTransformer(), get_categorical_features), # default FunctionTransformer implements identity\n",
    "                                                 remainder=StandardScaler()))\n",
    "                        )\n",
    "        steps.append(('fs', configuration['feature_selection'][fs_name]['fs']))\n",
    "        steps.append(('clf', configuration['classifier'][clf_name]['clf']))\n",
    "    \n",
    "    # no feature selection\n",
    "    else:\n",
    "        clf_name = name\n",
    "        steps = []\n",
    "        if not 'forest' in clf_name:\n",
    "            steps.append(('scaler', make_column_transformer((FunctionTransformer(), get_categorical_features), # default FunctionTransformer implements identity\n",
    "                                                 remainder=StandardScaler()))\n",
    "                        )\n",
    "        steps.append(('clf', configuration['classifier'][clf_name]['clf']))\n",
    "    if verbose > 1:\n",
    "        print('steps %s' % str(steps))\n",
    "    \n",
    "    if len(steps) == 1:\n",
    "        return steps[0][1]\n",
    "    else:\n",
    "        return Pipeline(steps)\n",
    "\n",
    "def get_feature_names_from_pipeline(pipeline, original_feature_names, verbose=0):\n",
    "    \"\"\"get_feature_names not available on Pipeline, should handle what happens to feature names depending on \n",
    "    Pipeline steps (in order to be able to compute features importance)\n",
    "    Mainly scaler which is columnTransformer may change the order of columns.\n",
    "    If there is feature selection, its support_ param will refer to indices from features with order changed by scaler.\n",
    "    And normally the classifier step should not change anything to the features list or order, but if feat importance is \n",
    "    retrieved from clf, then it must also take into account proper order and list of features names (from scaler+fs if \n",
    "    any)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    feature_names: dict\n",
    "        Dictionnary with pipeline steps as keys (limited to 'scaler', 'fs' or 'clf'), and feature names at corresponding\n",
    "        steps as list values.\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_names = {}\n",
    "    current_feature_names = None\n",
    "    fs_feature_names = []\n",
    "    if isinstance(pipeline, Pipeline):\n",
    "        masked_features = None\n",
    "        if verbose > 1:\n",
    "            print('DEBUG features %s' % str(features))\n",
    "            print('DEBUG features from Pipeline %s' % str(parse_feature_names(best_clf.get_feature_names())))\n",
    "            print('DEBUG pipeline steps %s' % best_clf.named_steps)\n",
    "         \n",
    "        for step in ['scaler', 'fs', 'clf']:\n",
    "            if step in pipeline.named_steps:\n",
    "                if step == 'scaler':\n",
    "                    feature_names['scaler'] = parse_feature_names(pipeline.named_steps['scaler'].get_feature_names())\n",
    "                    current_feature_names = feature_names['scaler']\n",
    "                elif step == 'fs':\n",
    "                    fs = pipeline.named_steps['fs']\n",
    "                    if isinstance(fs, RFECV):\n",
    "                        support = fs.support_\n",
    "                    elif isinstance(fs, SelectFromModel) or isinstance(fs, SelectKBest):\n",
    "                        support = fs.get_support()\n",
    "                    if current_feature_names is not None:\n",
    "                        current_feature_names = current_feature_names[support]\n",
    "                    else:\n",
    "                        current_feature_names = original_feature_names[support]\n",
    "                    feature_names['fs'] = current_feature_names\n",
    "                elif step == 'clf':\n",
    "                    if current_feature_names is not None:\n",
    "                        feature_names['clf'] = current_feature_names\n",
    "                    else:\n",
    "                        feature_names['clf'] = original_feature_names\n",
    "    else:\n",
    "        feature_names['clf'] = original_feature_names\n",
    "        \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_df(grid_search, features):\n",
    "    \"\"\"\n",
    "    Return the feature names and coefficients from the final classifier of the\n",
    "    best pipeline found by GridSearchCV. See https://git.io/vPWLI.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    grid_search: GridSearchCV object\n",
    "        A post-fit GridSearchCV object where the estimator is a Pipeline.\n",
    "    features: list\n",
    "        initial feature names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Dataframe of feature name and coefficient values\n",
    "    \"\"\"\n",
    "    features = np.array(features)\n",
    "    pipeline = grid_search.best_estimator_\n",
    "    for name, transformer in pipeline.steps:\n",
    "        if name.startswith('select'):\n",
    "            X_index = np.arange(len(features)).reshape(1, -1)\n",
    "            indexes = transformer.transform(X_index).tolist()\n",
    "            features = features[indexes]\n",
    "    step_name, classifier = pipeline.steps[-1]\n",
    "    coefficients, = classifier.coef_\n",
    "    feature_df = pd.DataFrame.from_items([\n",
    "        ('feature', features),\n",
    "        ('coefficient', coefficients),\n",
    "    ])\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_forest(estimator, X_, y_):\n",
    "    \"\"\"\n",
    "    Custom scoring function for hyperparameter optimization. In this case, we want to print out the oob score.\n",
    "    estimator was fitted on train data, X and y here are validation data.\n",
    "    For OOB we don't need validation data - OOB is inherent to tree building.\n",
    "    Same for feature importances (with default gini impurity measure done by sklearn).\n",
    "    \"\"\"\n",
    "    #all_feature_importances.append(estimator.feature_importances_) # global variable - ugly trick\n",
    "    score = estimator.oob_score_\n",
    "    #print \"oob_score_:\", score\n",
    "    return score\n",
    "\n",
    "def svm_fi(estimator, X_, y_):\n",
    "    # for SVM dropcol feature importances computation, we use validation data (X_, y_) and train data (X, y) are global.\n",
    "    #X_train_df = pd.DataFrame(X, columns=feats)\n",
    "    #y_train_df = pd.DataFrame(y)\n",
    "    #X_valid_df = pd.DataFrame(X_, columns=feats)\n",
    "    #y_valid_df = pd.DataFrame(y_)\n",
    "    #dcs = dropcol_importances_sklearn(clf, X_train_df, y_train_df, X_valid_df, y_valid_df)\n",
    "    #all_dropcol_feature_importances.append(dcs)\n",
    "    #print(dcs)\n",
    "    return 0\n",
    "\n",
    "def run_cross_val_score(clf, X, y, idx=None, n_jobs=-1, verbose=0):\n",
    "    if verbose > 1:\n",
    "        print('run_cross_val_score(clf=%s) % str(clf)')\n",
    "    #print(\"gridsearch(clf=%s, modelTarget=%s, param_grid=%s, features=%s, upsample=%s)\" \n",
    "    #      % (type(clf), modelTarget, param_grid, features, upsample))\n",
    "\n",
    "    scoring = build_scoring(clf, y)\n",
    "    \n",
    "    #if not isinstance(clf, JNCC2Wrapper) and not (isinstance(clf, Pipeline) and isinstance(clf.named_steps['clf'], JNCC2Wrapper)):\n",
    "    cross_val_scores = cross_validate(clf, X, y, scoring=scoring, cv=10, return_train_score=False, \n",
    "                                      n_jobs=n_jobs if not isinstance(get_clf_from_estimator(clf), JNCC2Wrapper) else 1, \n",
    "                                      verbose=max(0, verbose-1))\n",
    "\n",
    "    if verbose > 1:\n",
    "        print('run_cross_val_score: return %s' % str(cross_val_scores))\n",
    "\n",
    "    return cross_val_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(clf, param_grid, data, features, prediction_task, cv=10, repeat_dc=300, test_set=0.1, n_splits=1, \n",
    "             splits_mode='shuffle', upsample=None, fi=False, bins=None, strategy=None, force_gs=True, n_jobs=-1, verbose=0):\n",
    "    if verbose > 1:\n",
    "        print(('run_test(clf=%s, param_grid=%s, features=%s, cv=%d, n_splits=%d, test_set=%f, splits_mode=%s, upsample=%s,'\n",
    "              +' bins=%d, strategy=%s, n_jobs=%d)') \n",
    "              % (clf.__class__.__name__, str(param_grid), str(features), cv, n_splits, test_set, splits_mode, upsample, \n",
    "                 bins, strategy, n_jobs))\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "    create_test_set = test_set is not None and test_set > 0\n",
    "    best_params = None\n",
    "    grid = None\n",
    "    is_jncc = isinstance(get_clf_from_estimator(clf), JNCC2Wrapper)\n",
    "    if verbose > 2:\n",
    "        print('run_test: is_jncc %s' % str(is_jncc))\n",
    "    \n",
    "    all_scores = []\n",
    "    all_test_scores = []    \n",
    "    all_supports = []\n",
    "    all_test_supports = []\n",
    "    fi_method = []\n",
    "    fi_fs = pd.DataFrame(np.zeros((1, len(features_df.columns))), columns=features_df.columns)\n",
    "    \n",
    "    splits = prepare_train_data(samples=data, features=features, prediction_task=prediction_task, test_set=test_set, \n",
    "                                n_splits=n_splits, splits_mode=splits_mode, upsample=upsample, bins=bins, \n",
    "                                strategy=strategy, verbose=verbose)\n",
    "\n",
    "    for idx, (X, y, X_test, y_test) in enumerate(splits):\n",
    "        if verbose > 1:\n",
    "            print(\"*** Train/Test Split #%d / %d\" % (idx, len(splits)-1))\n",
    "        y = y.astype(int)\n",
    "        support = np.unique(y, return_counts=True)\n",
    "        support_test = None\n",
    "        if y_test is not None:\n",
    "            y_test = y_test.astype(int)\n",
    "            support_test = np.unique(y_test, return_counts=True)\n",
    "        all_supports.append(str(support))\n",
    "        all_test_supports.append(str(support_test))\n",
    "        \n",
    "        # first evaluate best params through grid search for this particular dataset/featureset\n",
    "        # !!! important, see http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf\n",
    "        # use 'force_gs' so hyper-parameters search is performed independently on each train split before evaluation on test, which would be unbiased protocol\n",
    "\n",
    "        grid = gridsearch2(clf, \n",
    "                          X, y, \n",
    "                          prediction_task, \n",
    "                          param_grid=param_grid, \n",
    "                          features=features,\n",
    "                          cv=cv,\n",
    "                          n_jobs=n_jobs if not is_jncc else 1, # avoid multithreading with JNCCWrapper\n",
    "                          verbose=verbose)\n",
    "        best_params = grid.best_params_\n",
    "        single_best_score = grid.best_score_\n",
    "        if verbose > 1:\n",
    "            print('  Gridsearch evaluated best score %s' % (single_best_score))\n",
    "            print('  Gridsearch best params : %s' % str(best_params))\n",
    "\n",
    "\n",
    "\n",
    "        if create_test_set:\n",
    "\n",
    "            # compute cv scores (redundant with gridsearch ?)\n",
    "\n",
    "            best_clf = grid.best_estimator_ #sklearn.base.clone(grid.best_estimator_)\n",
    "            val_scores = pd.DataFrame(grid.cv_results_, columns=grid.cv_results_.keys())\n",
    "            all_scores.append(val_scores.loc[grid.best_index_])\n",
    "\n",
    "            # compute test scores\n",
    "            \n",
    "            labels = sorted(support[0])\n",
    "            if verbose > 2:\n",
    "                print(\"LABELS \" + str(labels))            \n",
    "\n",
    "            #best_clf.fit(X, y) # normally has been fitted by gridsearchcv\n",
    "            y_pred = best_clf.predict(X_test)\n",
    "            \n",
    "            if is_jncc:\n",
    "                # jncc returns a multi-label proposition\n",
    "                jncc_result = y_pred\n",
    "                if y_pred.values.ndim == 1:\n",
    "                    y_pred = jncc_result\n",
    "                else:\n",
    "                    y_pred = jncc_result.iloc(axis=1)[0]\n",
    "\n",
    "            # retrieve feature selection information, if any was performed\n",
    "            if type(best_clf) == Pipeline and 'fs' in best_clf.named_steps:\n",
    "                selected_features = get_feature_names_from_pipeline(best_clf, features)['fs']\n",
    "                if selected_features is not None:\n",
    "                    if verbose > 1:\n",
    "                        print('  Selected features %s' % str(selected_features))\n",
    "                    for ifeat in selected_features:\n",
    "                        fi_fs[ifeat] += 1                          \n",
    "            \n",
    "            \n",
    "            test_scores_row = {\n",
    "                'test_balanced_acc': balanced_accuracy_score(y_test, y_pred),\n",
    "                #'test_balanced_acc_adjusted': balanced_accuracy_score(y_test, y_pred, adjusted=True),\n",
    "                'test_f1_macro_weighted': f1_score(y_test, y_pred, average='weighted') }\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['test_f1_macro_class_%d' % label] = f1_score(y_test, y_pred, labels=[label], average='weighted')\n",
    "            test_scores_row['test_precision_macro_weighted'] =  precision_score(y_test, y_pred, average='weighted')\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['test_precision_macro_class_%d' % label] = precision_score(y_test, y_pred, labels=[label], average='weighted')            \n",
    "            test_scores_row['test_recall_macro_weighted'] =  recall_score(y_test, y_pred, average='weighted')\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['test_recall_macro_class_%d' % label] = recall_score(y_test, y_pred, labels=[label], average='weighted')     \n",
    "            #test_scores_row['test_f1_micro'] = f1_score(y_test, y_pred, average='micro')\n",
    "            if is_jncc:\n",
    "                test_scores_row['test_mrr'] = feu.compute_mrr(y_test, jncc_result, labels)\n",
    "                test_scores_row['test_determinacy'] = feu.compute_determinacy(jncc_result)\n",
    "                # TODO: add scores performed by JNCC2 ? (single accuracy etc)\n",
    "                \n",
    "            # Compute dummy classifier scores using different strategies, as a baseline\n",
    "            for dummy_strat in ['uniform', 'stratified', 'most_frequent']:\n",
    "                dummy = DummyClassifier(strategy=dummy_strat, random_state=RANDOM_STATE)\n",
    "                dummy.fit(X, y)\n",
    "                y_pred = dummy.predict(X_test)\n",
    "                test_scores_row ['rnd_%s_balanced_acc' % dummy_strat] = balanced_accuracy_score(y_test, y_pred)\n",
    "                test_scores_row['rnd_%s_f1_macro_weighted' % dummy_strat] = f1_score(y_test, y_pred, average='weighted')\n",
    "                for idx, label in enumerate(labels):\n",
    "                    test_scores_row['rnd_%s_f1_macro_class_%d' % (dummy_strat, label)] = f1_score(y_test, y_pred, labels=[label], average='weighted')\n",
    "                test_scores_row['rnd_%s_precision_macro_weighted' % dummy_strat] =  precision_score(y_test, y_pred, average='weighted')\n",
    "                for idx, label in enumerate(labels):\n",
    "                    test_scores_row['rnd_%s_precision_macro_class_%d' % (dummy_strat, label)] = precision_score(y_test, y_pred, labels=[label], average='weighted')            \n",
    "                test_scores_row['rnd_%s_recall_macro_weighted' % dummy_strat] =  recall_score(y_test, y_pred, average='weighted')\n",
    "                for idx, label in enumerate(labels):\n",
    "                    test_scores_row['rnd_%s_recall_macro_class_%d' % (dummy_strat, label)] = recall_score(y_test, y_pred, labels=[label], average='weighted')                  \n",
    "                test_scores_row['rnd_%s_f1_micro' % dummy_strat] = f1_score(y_test, y_pred, average='micro')\n",
    "                    \n",
    "            test_scores_df = pd.DataFrame([test_scores_row], index=['score'])\n",
    "            all_test_scores.append(test_scores_df)\n",
    "\n",
    "\n",
    "    if len(all_scores) == 1:\n",
    "        all_scores = all_scores[0]\n",
    "    elif len(all_scores) > 0:\n",
    "        all_scores = pd.concat(all_scores, axis=1).T\n",
    "    else:\n",
    "        all_scores = pd.DataFrame()\n",
    "    if len(all_test_scores) == 1:\n",
    "        all_test_scores = all_test_scores[0]\n",
    "    elif len(all_test_scores) > 0:\n",
    "        all_test_scores = pd.concat(all_test_scores)\n",
    "    else:\n",
    "        all_test_scores = pd.DataFrame()\n",
    "        \n",
    "    result = {\n",
    "        'all_scores': all_scores,\n",
    "        'all_test_scores': all_test_scores,\n",
    "        'dropcol_fi': all_dropcol_feature_importances,\n",
    "        'fi': all_feature_importances,\n",
    "        'fi_fs': fi_fs,\n",
    "        'support': support,\n",
    "        'support_test': support_test\n",
    "    }\n",
    "    \n",
    "    if verbose > 2:\n",
    "        print('run_gridsearch: all_scores =')\n",
    "        display(HTML(pd.DataFrame(all_scores).to_html()))\n",
    "        print('run_gridsearch: all_test_scores =')\n",
    "        display(HTML(pd.DataFrame(all_test_scores).to_html()))\n",
    "        \n",
    "    return grid, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test plan utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch hyper-parameters ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forests\n",
    "param_grid_rf =  {\n",
    "    'n_estimators' : [300],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_leaf': [1],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "# for quickly testing the loops\n",
    "param_grid_rf_tests = {\n",
    "    'n_estimators' : [3],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_leaf': [1],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "# SVM\n",
    "param_grid_svm = {\n",
    "    'clf__kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "    'clf__C': np.logspace(-2, 1, 4),\n",
    "    'clf__gamma': np.logspace(-3, 1, 5)\n",
    "}\n",
    "# Gaussian Naive Bayes\n",
    "param_grid_gnb = {\n",
    "    'clf__priors': [None], \n",
    "    'clf__var_smoothing': [1e-09]\n",
    "}\n",
    "\n",
    "param_grid_jncc2 = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTO\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_categorical_features(X):\n",
    "    if 'Expert' in X.columns:\n",
    "        return ['Expert']\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "ct = make_column_transformer(\n",
    "    (FunctionTransformer(), get_categorical_features), # default FunctionTransformer implements identity\n",
    "    remainder=StandardScaler()\n",
    ") #ERROR #TODO columns in output are in different order than input !!!!!!!!!!!!\n",
    "\n",
    "configuration = {\n",
    "    'subject': {\n",
    "        'doctor': {\n",
    "            'name': 'Doctor',\n",
    "            'modes': {\n",
    "                'verbal': {\n",
    "                    'dataset': subject_np_df,\n",
    "                    'name': 'Verbal'\n",
    "                },\n",
    "                'nonverbal': {\n",
    "                    'dataset': subject_np_df,\n",
    "                    'name': 'Non-Verbal'\n",
    "                },\n",
    "                'multimodal': {\n",
    "                    'dataset': subject_np_df,\n",
    "                    'name': 'Multimodal'\n",
    "                },\n",
    "                'nonverbal-a': {\n",
    "                    'dataset': subject_np_df,\n",
    "                    'name': 'Non-Verbal-A'\n",
    "                },\n",
    "                'multimodal-a': {\n",
    "                    'dataset': subject_np_df,\n",
    "                    'name': 'Multimodal-A'\n",
    "                },\n",
    "            },\n",
    "            'phases': {\n",
    "                'nophase': {\n",
    "                    'dataset': subject_np_df,\n",
    "                    'name': 'No Phase'\n",
    "                },\n",
    "                '157015': {\n",
    "                    'dataset': subject_p_df,\n",
    "                    'name': '157015'\n",
    "                },\n",
    "                'nophase-a': {\n",
    "                    'dataset': subject_np_df,\n",
    "                    'name': 'No Phase-A'\n",
    "                },\n",
    "                '157015-a': {\n",
    "                    'dataset': subject_p_df,\n",
    "                    'name': '157015-A'\n",
    "                },\n",
    "                'nophase-full': {\n",
    "                    'dataset': subject_np_df,\n",
    "                    'name': 'No Phase-FULL'\n",
    "                },\n",
    "                '157015-full': {\n",
    "                    'dataset': subject_p_df,\n",
    "                    'name': '157015-FULL'\n",
    "                }    \n",
    "            }\n",
    "        },\n",
    "        'agent': {\n",
    "            'name': 'Agent',\n",
    "            'modes': {\n",
    "                'verbal': {\n",
    "                    'dataset': agent_np_df,\n",
    "                    'name': 'Verbal'\n",
    "                },\n",
    "                'nonverbal': {\n",
    "                    'dataset': agent_np_df,\n",
    "                    'name': 'Non-Verbal'\n",
    "                },\n",
    "                'multimodal': {\n",
    "                    'dataset': agent_np_df,\n",
    "                    'name': 'Multimodal'\n",
    "                },\n",
    "                'nonverbal-a': {\n",
    "                    'dataset': agent_np_df,\n",
    "                    'name': 'Non-Verbal-A'\n",
    "                },\n",
    "                'multimodal-a': {\n",
    "                    'dataset': agent_np_df,\n",
    "                    'name': 'Multimodal-A'\n",
    "                },\n",
    "            },\n",
    "            'phases': {\n",
    "                'nophase': {\n",
    "                    'dataset': agent_np_df,\n",
    "                    'name': 'No Phase'\n",
    "                },\n",
    "                '157015': {\n",
    "                    'dataset': agent_p_df,\n",
    "                    'name': '157015'\n",
    "                },\n",
    "                'nophase-a': {\n",
    "                    'dataset': agent_np_df,\n",
    "                    'name': 'No Phase-A'\n",
    "                },\n",
    "                '157015-a': {\n",
    "                    'dataset': agent_p_df,\n",
    "                    'name': '157015-A'\n",
    "                },\n",
    "                'nophase-full': {\n",
    "                    'dataset': agent_np_df,\n",
    "                    'name': 'No Phase-FULL'\n",
    "                },\n",
    "                '157015-full': {\n",
    "                    'dataset': agent_p_df,\n",
    "                    'name': '157015-FULL'\n",
    "                }                    \n",
    "            }            \n",
    "        },\n",
    "        'doctor+agent': {\n",
    "            'name': 'Doctor+Agent',\n",
    "            'modes': {\n",
    "                'verbal': {\n",
    "                    'dataset': all_np_df,\n",
    "                    'name': 'Verbal'\n",
    "                },\n",
    "                'nonverbal': {\n",
    "                    'dataset': all_np_df,\n",
    "                    'name': 'Non-Verbal'\n",
    "                },\n",
    "                'multimodal': {\n",
    "                    'dataset': all_np_df,\n",
    "                    'name': 'Multimodal'\n",
    "                },\n",
    "                'nonverbal-a': {\n",
    "                    'dataset': all_np_df,\n",
    "                    'name': 'Non-Verbal-A'\n",
    "                },\n",
    "                'multimodal-a': {\n",
    "                    'dataset': all_np_df,\n",
    "                    'name': 'Multimodal-A'\n",
    "                },\n",
    "            },\n",
    "            'phases': {\n",
    "                'nophase': {\n",
    "                    'dataset': all_np_df,\n",
    "                    'name': 'No Phase'\n",
    "                },\n",
    "                '157015': {\n",
    "                    'dataset': all_p_df,\n",
    "                    'name': '157015'\n",
    "                },\n",
    "                'nophase-a': {\n",
    "                    'dataset': all_np_df,\n",
    "                    'name': 'No Phase-A'\n",
    "                },\n",
    "                '157015-a': {\n",
    "                    'dataset': all_p_df,\n",
    "                    'name': '157015-A'\n",
    "                },\n",
    "                'nophase-full': {\n",
    "                    'dataset': all_np_df,\n",
    "                    'name': 'No Phase-FULL'\n",
    "                },\n",
    "                '157015-full': {\n",
    "                    'dataset': all_p_df,\n",
    "                    'name': '157015-FULL'\n",
    "                }                    \n",
    "            }            \n",
    "        }        \n",
    "    },\n",
    "    'features': {\n",
    "        'verbal': 'Verbal',\n",
    "        'nonverbal': 'Non-Verbal',\n",
    "        'multimodal': 'Multimodal',\n",
    "        'multimodal+duration': 'Multimodal+Duration',\n",
    "        '157015': '157015',\n",
    "        'nophase': 'No Phase',\n",
    "        'nonverbal-a': 'Non-Verbal-A',    \n",
    "        'multimodal-a': 'Multimodal-A',\n",
    "        '157015-a': '157015-A',\n",
    "        'nophase-a': 'No Phase-A',\n",
    "        '157015-full': '157015-FULL',\n",
    "        'nophase-full': 'No Phase-FULL'\n",
    "    },\n",
    "    'classifier': {\n",
    "        'forest': {\n",
    "            'clf': RandomForestClassifier(random_state=RANDOM_STATE, oob_score=True),\n",
    "            'param_grid': param_grid_rf,\n",
    "            'name': 'RF'\n",
    "        },\n",
    "        'forest-test': {\n",
    "            'clf': RandomForestClassifier(random_state=RANDOM_STATE, oob_score=True),\n",
    "            'param_grid': param_grid_rf_tests,\n",
    "            'name': 'RF-test'\n",
    "        },        \n",
    "        'r-forest': {\n",
    "            'clf': RandomForestRegressor(random_state=RANDOM_STATE, oob_score=True),\n",
    "            'param_grid': param_grid_rf,\n",
    "            'name': 'RF-REGR'\n",
    "        },\n",
    "        'svm-back': {\n",
    "            'clf': make_pipeline(ct, SVC(random_state=RANDOM_STATE, probability=True)),\n",
    "            'param_grid': param_grid_svm,\n",
    "            'name': 'SVM'\n",
    "        },      \n",
    "        'gnb-back': {\n",
    "            'clf': make_pipeline(ct, GaussianNB()),\n",
    "            'param_grid': param_grid_gnb,\n",
    "            'name': 'NB-G'\n",
    "        },\n",
    "        \n",
    "        'svm': {\n",
    "            'clf': SVC(random_state=RANDOM_STATE, probability=True),\n",
    "            'param_grid': param_grid_svm,\n",
    "            'name': 'SVM'\n",
    "        },\n",
    "        \n",
    "        'gnb': {\n",
    "            'clf': GaussianNB(),\n",
    "            'param_grid': param_grid_gnb,\n",
    "            'name': 'NB-G'\n",
    "        },        \n",
    "        \n",
    "        'jncc2': {\n",
    "            'clf': JNCC2Wrapper(),\n",
    "            'param_grid': param_grid_jncc2,\n",
    "            'name': 'JNCC2'\n",
    "            \n",
    "        },\n",
    "        'fs_rfe-svm-l2_gnb': {\n",
    "            'clf': None, #make_pipeline(ct, RFECV(estimator=LinearSVC(penalty='l2'), step=1, cv=10, scoring='f1'), GaussianNB()),  \n",
    "            'param_grid': param_grid_gnb,\n",
    "            'name': 'FS_RFE-SVM-L2_NB-G'\n",
    "        },\n",
    "        'fs_rfe-svm-l2_svm': {\n",
    "            'clf': None, #make_pipeline(ct, RFECV(estimator=LinearSVC(penalty='l2'), step=1, cv=10, scoring='f1'), SVC(random_state=RANDOM_STATE, probability=True)),\n",
    "            'param_grid': param_grid_svm,\n",
    "            'name': 'FS_RFE-SVM-L2_SVM'\n",
    "        },        \n",
    "        'fs_svm-l1_gnb': {\n",
    "            'clf': None, #make_pipeline(ct, SelectFromModel(estimator=LinearSVC(dual=False, penalty='l1', C=0.1)), GaussianNB()),\n",
    "            'param_grid': param_grid_gnb,\n",
    "            'name': 'FS_SVM-L1_NB-G'\n",
    "        },\n",
    "        'fs_svm-l1_svm': {\n",
    "            'clf': None, #make_pipeline(ct, SelectFromModel(estimator=LinearSVC(dual=False, penalty='l1', C=0.1)), SVC(random_state=RANDOM_STATE, probability=True)),\n",
    "            'param_grid': param_grid_svm,\n",
    "            'name': 'FS_SVM-L1_NB-G'\n",
    "        },   \n",
    "        'fs_kb-chi2_gnb': {\n",
    "            'clf': None, #make_pipeline(ct, SelectKBest(chi2, k=5), GaussianNB()),\n",
    "            'param_grid': param_grid_gnb,\n",
    "            'name': 'FS_KB5-CHI2_NB-G'\n",
    "        },\n",
    "        'fs_kb-mi_gnb': {\n",
    "            'clf': None, #make_pipeline(ct, SelectKBest(mutual_info_classif, k=5), GaussianNB()),\n",
    "            'param_grid': param_grid_gnb,\n",
    "            'name': 'FS_KB5-MI_NB-G'\n",
    "        },           \n",
    "        'name' : 'Classifier'\n",
    "    },\n",
    "    'feature_selection': {\n",
    "        'rfe-svm-l2': {\n",
    "            'fs': RFECV(estimator=LinearSVC(penalty='l2'), step=1, cv=10, scoring='f1'),  \n",
    "            'name': 'RFE-SVM-L2'\n",
    "        },\n",
    "        'svm-l1': {\n",
    "            'fs': SelectFromModel(estimator=LinearSVC(dual=False, penalty='l1', C=0.1)),\n",
    "            'name': 'SVM-L1'\n",
    "        },\n",
    "        'kb-mi': {\n",
    "            'fs': SelectPercentile(mutual_info_classif, percentile=10),\n",
    "            'name': 'KB5-MI'\n",
    "        },           \n",
    "        'name' : 'Feature Selection'\n",
    "    },    \n",
    "    'prediction_task': {\n",
    "        'presence': 'Presence',\n",
    "        'copresence': 'Co-Presence'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_multiindex(test_plan_params, metric_level=True, verbose=0):\n",
    "    iindex = []\n",
    "    inames = ['Target', 'Subject']\n",
    "    \n",
    "    pred_level = []\n",
    "    if 'presence' in test_plan_params['prediction_task']:\n",
    "        pred_level.append('Presence')\n",
    "    if 'copresence' in test_plan_params['prediction_task']:\n",
    "        pred_level.append('Co-Presence')\n",
    "    iindex.append(pred_level)\n",
    "    \n",
    "    subj_level = []\n",
    "    if 'doctor' in test_plan_params['subject']:\n",
    "        subj_level.append('Doctor')\n",
    "    if 'agent' in test_plan_params['subject']:\n",
    "        subj_level.append('Agent')\n",
    "    if 'doctor+agent' in test_plan_params['subject']:\n",
    "        subj_level.append('Doctor+Agent')\n",
    "    iindex.append(subj_level)\n",
    "        \n",
    "    if 'modes' in test_plan_params.keys():\n",
    "        exps = test_plan_params['modes']\n",
    "        exps_name = 'Mode'\n",
    "    elif 'phases' in test_plan_params.keys():\n",
    "        exps = test_plan_params['phases']\n",
    "        exps_name = 'Phases'\n",
    "\n",
    "    exp_level = []\n",
    "    for exp in exps:\n",
    "        exp_level.append(configuration['features'][exp])\n",
    "    if len(exp_level) > 0:\n",
    "        iindex.append(exp_level)\n",
    "        inames.append(exps_name)\n",
    "\n",
    "    clf_level = []\n",
    "    for clf_id in test_plan_params['classifier']:\n",
    "        clf_level.append(configuration['classifier'][clf_id]['name'])\n",
    "    if len(clf_level) > 0:\n",
    "        iindex.append(clf_level)\n",
    "        inames.append('Classifier')\n",
    "\n",
    "    if 'upsampling' in test_plan_params.keys():\n",
    "        up_level = []\n",
    "        for up_id in test_plan_params['upsampling']:\n",
    "            up_level.append(str(up_id))\n",
    "        iindex.append(up_level)\n",
    "        inames.append('Oversampling')\n",
    "        \n",
    "    if 'nbclasses' in test_plan_params.keys():\n",
    "        cl_level = []\n",
    "        for cl_id in test_plan_params['nbclasses']:\n",
    "            cl_level.append(cl_id)\n",
    "        iindex.append(cl_level)\n",
    "        inames.append('Nb Classes')\n",
    "        \n",
    "    if 'clstrategy' in test_plan_params.keys():\n",
    "        cls_level = []\n",
    "        for cls_id in test_plan_params['clstrategy']:\n",
    "            cls_level.append(str(cls_id))\n",
    "        iindex.append(cls_level)\n",
    "        inames.append('Classes discretization strategy')\n",
    "\n",
    "    if metric_level:\n",
    "        iindex.append(['score', 'err'])\n",
    "        inames.append('Metric')\n",
    "\n",
    "    scores_df_index = iindex\n",
    "    if verbose > 1:\n",
    "        print(scores_df_index)\n",
    "    scores_df_multiindex = pd.MultiIndex.from_product(scores_df_index, \n",
    "                                                      names = inames)\n",
    "    return scores_df_multiindex\n",
    "\n",
    "def get_results_tuple(test, upsample_strategy_default):\n",
    "    subject = test['subject']\n",
    "    subj_conf = configuration['subject'][subject]\n",
    "    prediction_task = test['prediction_task']\n",
    "    classifier = test['classifier']\n",
    "    \n",
    "    if 'modes' in test.keys():\n",
    "        fset = test['modes']  \n",
    "    elif 'phases' in test.keys():\n",
    "        fset = test['phases']     \n",
    "\n",
    "    if 'upsampling' in test:\n",
    "        upsample = test['upsampling']\n",
    "    else:\n",
    "        upsample = upsample_strategy_default\n",
    "\n",
    "    result_tuple = ()\n",
    "    result_tuple = result_tuple + ('Presence',) if prediction_task == 'presence' else ('Co-Presence',)\n",
    "    result_tuple = result_tuple + (subj_conf['name'],)\n",
    "    result_tuple = result_tuple + (configuration['features'][fset],)\n",
    "    result_tuple = result_tuple + (configuration['classifier'][classifier]['name'],)\n",
    "    \n",
    "    if 'upsampling' in test:\n",
    "        result_tuple = result_tuple + (str(upsample),)\n",
    "        \n",
    "    if 'nbclasses' in test:\n",
    "        result_tuple = result_tuple + (str(test['nbclasses']),)\n",
    "    \n",
    "    if 'clstrategy' in test:\n",
    "        result_tuple = result_tuple + (str(test['clstrategy']),)\n",
    "        \n",
    "    return result_tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test plan runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bins for presence / co-presence discretization\n",
    "n_bins_default = 5\n",
    "# strategy for presence / co-presence discretization\n",
    "bin_strategy_default = 'kmeans'\n",
    "# default upsampling\n",
    "upsample_strategy_default = 'imb_smotenc'\n",
    "# whether to apply 'original' upsampling method\n",
    "#upsample = True\n",
    "verbose=1\n",
    "test_set = True\n",
    "fi = False\n",
    "\n",
    "all_feature_importances = []    \n",
    "all_dropcol_feature_importances = []\n",
    "\n",
    "def run_test_plan(test_plan_params, group, exp, iter, \n",
    "                  test_set=0.1, n_splits=1, splits_mode='shuffle', cv=10, repeat_cv=10, \n",
    "                  repeat_fi=300, upsample_strategy=upsample_strategy_default, \n",
    "                  n_bins=n_bins_default, bin_strategy=bin_strategy_default, \n",
    "                  n_jobs=-1, verbose=verbose):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tp_start = time.time()\n",
    "    \n",
    "    ### Create dataframe for results, context, feature importance (from feature selection)\n",
    "    tests_df_file = os.path.join(config.OUT_PATH, 'tests_executions.xlsx')\n",
    "    if os.path.isfile(tests_df_file):\n",
    "        tests_df = pd.read_excel(tests_df_file)\n",
    "    else:\n",
    "        tests_df = pd.DataFrame()\n",
    "    tests_df_cols = ['Test plan', 'Group', 'Experiment', 'Exp. Iteration',  'Test plan size', 'Test set ratio', \n",
    "                     'Internal CV', 'External CV', 'External CV mode', 'Upsampling', 'Classes', 'Classes binning', \n",
    "                     'Date Start', 'Date End', 'Duration', 'Nb executed tests']\n",
    "    tests_df_row = pd.Series({\n",
    "        'Test plan': str(test_plan_params),\n",
    "        'Group': group,\n",
    "        'Experiment': exp,\n",
    "        'Exp. Iteration': iter,\n",
    "        'Test plan size': len(ParameterGrid(test_plan_params)),\n",
    "        'Test set ratio': test_set,\n",
    "        'Internal CV':cv,\n",
    "        'External CV': n_splits if splits_mode=='shuffle' else n_splits*(int(1/test_set)),\n",
    "        'External CV mode' : splits_mode,\n",
    "        'Upsampling': upsample_strategy,\n",
    "        'Classes': n_bins,\n",
    "        'Classes binning': bin_strategy,\n",
    "        'Date Start': datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "        'Date End': None,\n",
    "        'Duration': None,\n",
    "        'Nb executed tests': 0\n",
    "    })\n",
    "    tests_df = tests_df.append(tests_df_row, ignore_index=True)\n",
    "    tests_df.to_excel(tests_df_file, index=False, columns=tests_df_cols)\n",
    "    \n",
    "    dh = DataHandler(group, exp, iter)\n",
    "    scores_df_multiindex = get_df_multiindex(test_plan_params)\n",
    "\n",
    "    scores_df = dh.load_obj('results', 'scores')\n",
    "    if scores_df is None:\n",
    "        scores_df = pd.DataFrame(index=scores_df_multiindex)\n",
    "        dh.save_obj(scores_df, 'results', 'scores')\n",
    "    context_df = dh.load_obj('.', 'context')\n",
    "    if context_df is None:\n",
    "        ctx_df_multiindex = get_df_multiindex(test_plan_params, metric_level=False)\n",
    "        if verbose > 2:\n",
    "            print('CTX multiindex %s' %  str(ctx_df_multiindex))\n",
    "        context_df = pd.DataFrame(index=ctx_df_multiindex)\n",
    "        #context_df['Best Params'] = ''\n",
    "        #context_df['Best Params'] = context_df['Best Params'].apply(str)\n",
    "        dh.save_obj(context_df, '.', 'context')\n",
    "    fi_fs_df = dh.load_obj('results', 'fi_fs')\n",
    "    if fi_fs_df is None:\n",
    "        fi_fs_df_multiindex = get_df_multiindex(test_plan_params, metric_level=False)\n",
    "        fi_fs_df = pd.DataFrame(index=fi_fs_df_multiindex, columns=features_df.columns)\n",
    "        dh.save_obj(fi_fs_df, '.', 'fi_fs')\n",
    "    \n",
    "    ### Execute test plan and fill results dataframes\n",
    "    \n",
    "    test_plan = ParameterGrid(test_plan_params)\n",
    "    if verbose > 0:\n",
    "        print(\"%d tests to be performed\" % len(test_plan))\n",
    "        display(HTML(pd.DataFrame(list(test_plan)).to_html()))\n",
    "        \n",
    "        \n",
    "    for idx, test in enumerate(test_plan):\n",
    "        if verbose > 0:\n",
    "            print(\"########## TEST %d / %d : %s\" % (idx, len(test_plan), test))\n",
    "\n",
    "        #try:\n",
    "        #print(\"before\")\n",
    "        # make it global because we use them to compute feature importance for forests\n",
    "        all_feature_importances = []    \n",
    "        all_dropcol_feature_importances = []\n",
    "\n",
    "        subject = test['subject']\n",
    "        prediction_task = test['prediction_task']\n",
    "        classifier = test['classifier']\n",
    "\n",
    "        if 'upsampling' in test:\n",
    "            upsample = test['upsampling']\n",
    "        else:\n",
    "            upsample = upsample_strategy\n",
    "            \n",
    "        if 'nbclasses' in test:\n",
    "            n_bins = test['nbclasses']\n",
    "        if 'clstrategy' in test:\n",
    "            bin_strategy = test['clstrategy']\n",
    "\n",
    "        #clf = configuration['classifier'][classifier]['clf']\n",
    "        clf = build_estimator(classifier)\n",
    "        pgrid = configuration['classifier'][classifier]['param_grid']\n",
    "        subj_conf = configuration['subject'][subject]\n",
    "        if 'modes' in test.keys():\n",
    "            fset = test['modes']\n",
    "            df = subj_conf['modes'][fset]['dataset']\n",
    "            feats = get_features_set(subj_conf['name'], subj_conf['modes'][fset]['name'])      \n",
    "        elif 'phases' in test.keys():\n",
    "            fset = test['phases']\n",
    "            df = subj_conf['phases'][fset]['dataset']\n",
    "            feats = get_features_set(subj_conf['name'], subj_conf['phases'][fset]['name'])\n",
    "\n",
    "        if upsample not in (None, 'imb_smotenc', 'imb_random') and 'Expert' in feats:\n",
    "            print ('ERROR: only SMOTE-NC and RANDOM strategies are supported with categorical features, skipping test')\n",
    "            continue\n",
    "            \n",
    "        # regression task\n",
    "        if classifier.startswith('r-'):\n",
    "            n_bins = 0\n",
    "            upsample = None\n",
    "        # JNCC2\n",
    "        if isinstance(clf, JNCC2Wrapper):\n",
    "            # pass the DataHandler\n",
    "            clf = JNCC2Wrapper(dh)\n",
    "        elif isinstance(clf, Pipeline):\n",
    "            if 'clf' in clf.named_steps and isinstance(clf.named_steps['clf'], JNCC2Wrapper):\n",
    "                # pass the DataHandler to the pipelined JNCC2\n",
    "                clf.set_params(clf__dataHandler=dh) \n",
    "                if verbose > 1:\n",
    "                    print('DEBUG dh root_path %s' % clf.named_steps['clf'].dataHandler.root_path)\n",
    "            if verbose > 1:\n",
    "                print('DEBUG pipeline named steps %s' % str(clf.named_steps))  \n",
    "            \n",
    "\n",
    "        #print(\"after\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        #X, y = prepare_train_data(samples=df, features=feats, target_model=target, upsample=upsample, test_set=True) \n",
    "\n",
    "        grid, result = \\\n",
    "            run_test(\n",
    "                    clf, \n",
    "                    pgrid, \n",
    "                    df, feats, \n",
    "                    prediction_task, \n",
    "                    cv=cv, \n",
    "                    upsample=upsample, \n",
    "                    test_set=test_set,\n",
    "                    n_splits=n_splits,\n",
    "                    splits_mode=splits_mode,\n",
    "                    fi=fi,\n",
    "                    bins=n_bins, strategy=bin_strategy,\n",
    "                    n_jobs=n_jobs, verbose=verbose)\n",
    "        scores, test_scores, dropcol_dis, fis = result['all_scores'], result['all_test_scores'], result['dropcol_fi'], result['fi']\n",
    "        support, support_test, fi_fs = result['support'], result['support_test'], result['fi_fs']\n",
    "        #print(scores.mean(axis='columns'))\n",
    "        #print('TEST SCORES (RAW):')\n",
    "        #print(test_scores)\n",
    "        if verbose > 1:\n",
    "            display(HTML(scores.to_html()))\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        if verbose > 0:\n",
    "            print(\"   Elapsed: %s seconds\" % str(elapsed))\n",
    "\n",
    "        isDoc = 'doctor' in subject\n",
    "        isAgent = 'agent' in subject\n",
    "\n",
    "        results_tuple = get_results_tuple(test, upsample_strategy_default=upsample_strategy)\n",
    "        \n",
    "\n",
    "\n",
    "        # compute err\n",
    "\n",
    "        cols_to_remove = [col for col in list(scores.columns) if 'time' in col or 'split' in col or 'rank' in col or 'param' in col]\n",
    "        if verbose > 2:\n",
    "            print('scores columns: %s' % str(scores.columns))\n",
    "            print('scores columns to remove    : %s' % str(cols_to_remove))\n",
    "        all_scores_ = scores.drop(cols_to_remove, axis=1).T\n",
    "        if verbose > 2:\n",
    "            print('scores new columns: %s' % str(all_scores_.columns))\n",
    "            display(HTML(all_scores_.to_html()))\n",
    "        #!!!TODO: pour gridsearch, calculer le mean (sur 10xCVs = OK) et la std \n",
    "        # (racine de var globale = var intra+var inter) = PAS OK\n",
    "        mean_cols = [col for col in list(all_scores_.index) if 'mean' in col]\n",
    "        std_cols = [col for col in list(all_scores_.index) if 'std' in col]\n",
    "        all_scores_means = all_scores_.loc[mean_cols].mean(axis=1).rename(index={name:name.replace('mean_', '') \n",
    "                                                                                 for name in mean_cols})\n",
    "        inter_var = all_scores_.loc[mean_cols].var(axis=1)\n",
    "        inter_var = inter_var.rename(index={name:name.replace('mean_', '') for name in mean_cols})\n",
    "        intra_var = np.square(all_scores_.loc[std_cols]).mean(axis=1)\n",
    "        intra_var  = intra_var.rename(index={name:name.replace('std_', '') for name in std_cols})\n",
    "        global_var = inter_var + intra_var\n",
    "        all_scores_cit = global_var / np.sqrt(3*10) * 1.96        \n",
    "        \n",
    "        final = pd.DataFrame(index=all_scores_cit.index)\n",
    "\n",
    "        if verbose > 2:\n",
    "            print('mean cols %s, std cols %s' % (mean_cols, std_cols))\n",
    "        final['score'] = all_scores_means\n",
    "        #final['std'] = \n",
    "        final['err'] = all_scores_cit     \n",
    "    \n",
    "        final.index = [row.replace('test_', 'val_') for row in final.index]\n",
    "        if verbose>2:\n",
    "            print('score and err for gs results:')\n",
    "            display(HTML(final.to_html()))        \n",
    "        \n",
    "        \n",
    "        if test_scores is not None and not test_scores.empty:\n",
    "            test_scores_df = pd.DataFrame()\n",
    "            #!!! pour les scores de TEST ce calcul est OK par contre\n",
    "            test_scores_df['score'] = test_scores.mean(axis=0)\n",
    "            test_scores_df['err'] = test_scores.std(axis=0) # pour tre cohrent il faudrait calculer l'intervalle de confiance !\n",
    "            #print(\"TEST scores:\")\n",
    "            #display(HTML(test_scores_df.to_html()))\n",
    "\n",
    "            final_ = pd.concat([final, test_scores_df], axis=0, join='outer').T\n",
    "        else:\n",
    "            final_ = final.T\n",
    "        #display(HTML(final_.to_html()))\n",
    "        final = pd.DataFrame(index=scores_df_multiindex, columns=final_.columns)\n",
    "        #display(HTML(final.to_html()))\n",
    "        final.loc[results_tuple + ('score',)] = final_.loc['score']\n",
    "        final.loc[results_tuple + ('err',)] = final_.loc['err']\n",
    "\n",
    "        final.dropna(axis='rows', how='all', inplace=True)\n",
    "        #display(HTML(final.to_html()))\n",
    "        if verbose > 1:\n",
    "            print(\"      updating scores for %s\" % str(results_tuple))\n",
    "\n",
    "        # insert missing columns in results df if needed\n",
    "        s = set(final.columns) - set(scores_df.columns)\n",
    "        #print(\"DEBUG inserting new columns %s\" % str(s))\n",
    "        new = list(sorted(s))\n",
    "        for newcol in new:\n",
    "            scores_df[newcol] = [np.nan]*len(scores_df)\n",
    "\n",
    "        #display(HTML(scores_df.dropna(axis='rows', how='all', inplace=False).to_html()))\n",
    "        scores_df.loc[results_tuple + ('score',)] = pd.Series(final.loc[results_tuple + ('score',)].to_dict())\n",
    "        scores_df.loc[results_tuple + ('err',)] = pd.Series(final.loc[results_tuple + ('err',)].to_dict())\n",
    "        #display(HTML(scores_df.dropna(axis='rows', how='all', inplace=False).to_html()))  \n",
    "        dh.save_obj(scores_df, 'results', 'scores')\n",
    "\n",
    "        dumpPath = os.path.join(dh.root_path, 'results', 'scores.xlsx')\n",
    "        scores_df.to_excel(dumpPath, index=True)\n",
    "        \n",
    "        if grid is not None:\n",
    "            best_params = grid.best_params_\n",
    "        else:\n",
    "            best_params = list(ParameterGrid(pgrid))\n",
    "            \n",
    "        \n",
    "        \n",
    "        new_row = pd.Series({\n",
    "            'Duration': elapsed,                \n",
    "            'Support': str(support), \n",
    "            'Support (test)': str(support_test), \n",
    "            'Oversampling': str(upsample),             \n",
    "            'Y nb classes': str(n_bins),\n",
    "            'Y strategy': str(bin_strategy),\n",
    "            'Test set ratio': str(test_set),\n",
    "            'Grid search params space': str(pgrid),\n",
    "            'Best Params': str(best_params),            \n",
    "            'Features': str(feats)\n",
    "        })\n",
    "        ordered_cols = ['Duration', 'Support', 'Support (test)', 'Oversampling', \n",
    "                        'Y nb classes', 'Y strategy', 'Test set ratio', 'Grid search params space', 'Best Params',\n",
    "                        'Features']\n",
    "        # insert missing columns in context df if needed\n",
    "        s = set(new_row.index) - set(context_df.columns)\n",
    "        new = list(s)\n",
    "        for newcol in new:\n",
    "            context_df[newcol] = [np.nan]*len(context_df)\n",
    "        context_df.loc[results_tuple] = new_row\n",
    "        dumpPath = os.path.join(dh.root_path, '.',  'context.csv')\n",
    "        context_df.to_csv(dumpPath, index=True)\n",
    "\n",
    "        if np.mean(fi_fs.values, axis=None) > 0:\n",
    "            fi_fs_df.loc[results_tuple] = fi_fs.values\n",
    "            dumpPath = os.path.join(dh.root_path, 'results', 'fi_fs.csv')\n",
    "            fi_fs_df[ordered_cols].to_csv(dumpPath, index=True)\n",
    "\n",
    "        #except Exception as e:\n",
    "        #    print(\"ERROR: exception occurred, skiping test\")\n",
    "        #    print(e)\n",
    "        tests_df.iloc[len(tests_df)-1, tests_df.columns.get_loc('Nb executed tests')] = idx+1\n",
    "        tests_df.iloc[len(tests_df)-1, tests_df.columns.get_loc('Date End')] = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        tests_df.to_excel(tests_df_file, index=False, columns=tests_df_cols)\n",
    "    \n",
    "    tp_end = time.time()\n",
    "    tests_df.iloc[len(tests_df)-1, tests_df.columns.get_loc('Duration')] = tp_end - tp_start\n",
    "    tests_df.to_excel(tests_df_file, index=False, columns=tests_df_cols)\n",
    "    \n",
    "    return dh, grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(title, importances, importances_err, feature_names, sort=True):\n",
    "    # Sort feature importances in descending order\n",
    "    if sort:\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "    else:\n",
    "        indices = np.arange(len(feature_names))\n",
    "\n",
    "    # Rearrange feature names so they match the sorted feature importances\n",
    "    names = [feature_names[i] for i in indices]\n",
    "    if importances_err is not None:\n",
    "        errs = [importances_err[i] for i in indices]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure()\n",
    "\n",
    "    # Create plot title\n",
    "    plt.title(\"Feature Importance - \" + title)\n",
    "\n",
    "    # Add bars\n",
    "    if importances_err is not None:\n",
    "        plt.bar(range(len(feature_names)), importances[indices], yerr=errs, xerr=0.1)\n",
    "    else:\n",
    "        plt.bar(range(len(feature_names)), importances[indices])\n",
    "\n",
    "    # Add feature names as x-axis labels\n",
    "    plt.xticks(range(len(feature_names)), names, rotation=90)\n",
    "\n",
    "    dh.save_fig(title)\n",
    "    #plt.savefig(os.path.join(dh.root_path, 'figures', title+'.png'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utilities for plots\n",
    "\n",
    "\n",
    "def add_line(ax, xpos, ypos):\n",
    "    line = plt.Line2D([xpos, xpos], [ypos + .1, ypos],\n",
    "                      transform=ax.transAxes, color='gray')\n",
    "    line.set_clip_on(False)\n",
    "    ax.add_line(line)\n",
    "\n",
    "def label_len(my_index,level):\n",
    "    labels = my_index.get_level_values(level)\n",
    "    return [(k, sum(1 for i in g)) for k,g in groupby(labels)]\n",
    "\n",
    "def label_group_bar_table_(ax, df, nlevels=-1):\n",
    "    ypos = -.1\n",
    "    scale = 1./df.index.size\n",
    "    if nlevels == -1:\n",
    "        nlevels = df.index.nlevels\n",
    "    \n",
    "    for level in range(nlevels)[::-1]:\n",
    "        pos = 0\n",
    "        for label, rpos in label_len(df.index,level):\n",
    "            lxpos = (pos + .5 * rpos)*scale\n",
    "            ax.text(lxpos, ypos, label, ha='center', transform=ax.transAxes)\n",
    "            add_line(ax, pos*scale, ypos)\n",
    "            pos += rpos\n",
    "        add_line(ax, pos*scale , ypos)\n",
    "        ypos -= .1\n",
    "        \n",
    "def label_group_bar_table(ax, df, nlevels=-1, fixed_levels = [], verbose=0):\n",
    "    ypos = -.1\n",
    "    scale = 1./df.index.size\n",
    "    if nlevels == -1:\n",
    "        nlevels = df.index.nlevels\n",
    "        \n",
    "    index_spec = df.index\n",
    "    levels_to_drop = []\n",
    "    for idx, level in enumerate(df.index.levels):\n",
    "        if verbose > 0:\n",
    "            print('debug cur level %d %s / cur unique value %s' % (idx, level, np.unique(df.index.get_level_values(idx).values)))\n",
    "        if len(np.unique(df.index.get_level_values(idx).values)) == 1 and idx not in fixed_levels:\n",
    "            if verbose > 0:\n",
    "                print('debug drop level ' + str(idx))\n",
    "            levels_to_drop.append(idx)\n",
    "            #index_spec = df.index.droplevel(idx)\n",
    "            nlevels -= 1\n",
    "    df.index = df.index.droplevel(levels_to_drop) \n",
    "    if verbose > 0:\n",
    "        print('debug df.index ' + str(df.index))\n",
    "    levels_lens = [ len(np.unique(df.index.get_level_values(idx).values)) for idx,_ in enumerate(df.index.levels)]\n",
    "    if verbose > 0:\n",
    "        print('debug levels lens %s ' % levels_lens)\n",
    "    sorted_levels = np.arange(len(df.index.levels))[np.argsort(levels_lens)]\n",
    "    if verbose > 0:\n",
    "        print('debug sorted levels by len %s' % sorted_levels)\n",
    "    df.index = df.index.reorder_levels(sorted_levels)\n",
    "    for level in range(nlevels)[::-1]:\n",
    "        if verbose > 0:\n",
    "            print('debug level '+str(level))\n",
    "        pos = 0\n",
    "        for label, rpos in label_len(df.index,level):\n",
    "            if verbose > 0:\n",
    "                print('debug label %s rpos %s' % (str(label), str(rpos)))\n",
    "            lxpos = (pos + .5 * rpos)*scale\n",
    "            ax.text(lxpos, ypos, label, ha='center', transform=ax.transAxes)\n",
    "            add_line(ax, pos*scale, ypos)\n",
    "            pos += rpos\n",
    "        add_line(ax, pos*scale , ypos)\n",
    "        ypos -= .1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_leveled_bars(df, df_err, levels, title):\n",
    "    ax = df \\\n",
    "        .plot(kind='bar', figsize=(10,5), yerr=df_err,\n",
    "              stacked=False,\n",
    "              title=title,\n",
    "              capsize=2)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.grid(axis='y')\n",
    "    label_group_bar_table(ax, df, levels)\n",
    "    return ax\n",
    "\n",
    "def get_selector_tuple(df, prediction_task_level, classifier_level, prediction_task, classifier):\n",
    "    selector = []\n",
    "    multi_levels_nb = 0\n",
    "    for idx, name in enumerate(df.index.names[0:-1]):\n",
    "        if idx == prediction_task_level:\n",
    "            selector.append(prediction_task)\n",
    "        elif idx == classifier_level:\n",
    "            selector.append(classifier)\n",
    "        else:\n",
    "            selector.append(slice(None))\n",
    "            multi_levels_nb += 1\n",
    "    return tuple(selector), multi_levels_nb\n",
    "\n",
    "def plot_results(dh, df, title_prefix, prediction_task_level, classifier_level, repeats=100, verbose=0):\n",
    "    \n",
    "    columns_spec = {\n",
    "        'train': [col for col in df.columns if 'train_' in col and 'class' not in col and 'fi' not in col],\n",
    "        'train (classes)': [col for col in df.columns if 'train_' in col and 'class' in col and 'fi' not in col], \n",
    "        'val': [col for col in df.columns if 'val_' in col and 'class' not in col and 'fi' not in col],\n",
    "        'val (classes)': [col for col in df.columns if 'val_' in col and 'class' in col and 'fi' not in col],\n",
    "        'test': [col for col in df.columns if 'test_' in col and 'class' not in col and 'fi' not in col],\n",
    "        'test (classes)': [col for col in df.columns if 'test_' in col and 'class' in col and 'fi' not in col]\n",
    "    }\n",
    "    \n",
    "    width = 0.095\n",
    "    w = 0\n",
    "    N = len(df)\n",
    "    ind = np.arange(N) + .15\n",
    "    cmap = plt.cm.tab10.colors\n",
    "    \n",
    "    \n",
    "    prediction_tasks = ['all']\n",
    "    classifiers = ['all']\n",
    "    if prediction_task_level is not None:\n",
    "        prediction_tasks = df.index.levels[prediction_task_level]\n",
    "    if classifier_level is not None:\n",
    "        classifiers = df.index.levels[classifier_level]\n",
    "        \n",
    "    \n",
    "    for cset, columns in columns_spec.items():\n",
    "        \n",
    "        for prediction_task in prediction_tasks:\n",
    "\n",
    "            for classifier in classifiers:\n",
    "                      \n",
    "                try:\n",
    "                    columns = sorted(columns)\n",
    "                    width = 0.7 / len(columns)\n",
    "                    w = 0\n",
    "\n",
    "                    s, multi_levels_nb = get_selector_tuple(df, prediction_task_level, classifier_level, prediction_task, classifier)\n",
    "                    if verbose > 0:\n",
    "                        print('debug s %s multi %s' % (str(s), str(multi_levels_nb)))\n",
    "                    multi_levels_nb=len(df.index.levels)\n",
    "\n",
    "                    if 'test' in cset:\n",
    "                        # choose random classifier with best f1 score for display\n",
    "                        f1_rnd_scores = []\n",
    "                        f1_rnd_names  = []\n",
    "                        for i in ['most_frequent', 'uniform', 'stratified']:\n",
    "                            if 'rnd_%s_f1_macro_weighted' % i in df.columns:\n",
    "                                f1_rnd_scores.append(df.loc(axis=0)[s+('score',)]['rnd_%s_f1_macro_weighted' % i].mean())\n",
    "                                f1_rnd_names.append(i)\n",
    "                        best_rnd_strat = f1_rnd_names[np.argmax(f1_rnd_scores)]\n",
    "                        #print('(best random classifier strategy: %s)' % best_rnd_strat)               \n",
    "\n",
    "                    #print('multi levels nb ' + str(multi_levels_nb))\n",
    "                    if 'classes' in cset:\n",
    "                        figsize=(30,15)\n",
    "                    else:\n",
    "                        figsize=(20,10)\n",
    "                    fig, ax = plt.subplots(figsize=figsize)\n",
    "                    score_df = df.loc(axis=0)[s + ('score',)]\n",
    "                    if verbose > 0:\n",
    "                        display(HTML(score_df.to_html()))\n",
    "                    err_df = df.loc(axis=0)[s + ('err',)]\n",
    "                    #display(HTML(err_df.to_html()))\n",
    "                    N = len(score_df)\n",
    "                    ind = np.arange(N) - width * len(columns) / 2.0 #+ .15\n",
    "                    ymin = 1\n",
    "                    first_rnd = True\n",
    "                    for i,n in enumerate(columns):\n",
    "                        rect1 = ax.bar(ind+w, score_df[n], float(width), color=cmap[i%len(cmap)], label=n, yerr=err_df[n] / np.sqrt(repeats), capsize=3)\n",
    "                        #print('STD %s ERR %s' % (str(err_df[n]), str(err_df[n] / np.sqrt(repeats))))\n",
    "                        if np.amin(score_df[n]) < ymin:\n",
    "                            ymin = np.amin(score_df[n])\n",
    "                        if 'test' in cset:\n",
    "                            rnd_col = n.replace('test_', 'rnd_%s_' % best_rnd_strat)\n",
    "                            if rnd_col in df.columns:\n",
    "                                if first_rnd:\n",
    "                                    label_rnd = 'random classifier (strategy: %s)' % best_rnd_strat\n",
    "                                    first_rnd = False\n",
    "                                else:\n",
    "                                    label_rnd = None\n",
    "                                rect2 = ax.bar(ind+w, score_df[rnd_col], float(width), color='lightgray', label=label_rnd,alpha=0.3, yerr=err_df[rnd_col] / np.sqrt(repeats), capsize=3, ecolor='darkgray')\n",
    "                                if np.amin(score_df[rnd_col]) < ymin:\n",
    "                                    ymin = np.amin(score_df[rnd_col])\n",
    "                        w += width\n",
    "                    ax.set_xticklabels('')\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.grid(axis='y')\n",
    "                    label_group_bar_table(ax, score_df, multi_levels_nb, [prediction_task_level, classifier_level])\n",
    "                    ax.autoscale_view()\n",
    "                    plt.xlim(-0.5, N-0.5)\n",
    "                    #print(ymin-0.1)\n",
    "                    plt.ylim(bottom=np.max([0, ymin-0.1]))\n",
    "                    plt.title('%s for %s prediction (%s) (%s)' % (title_prefix, prediction_task, classifier, cset))\n",
    "                    plt.legend()\n",
    "                    dh.save_fig(plt, '%s_%s_%s_%s.png' % (title_prefix.replace(' ', '-'), prediction_task, classifier, cset.replace(' ', '-')))\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print('<<no data for %s %s>>' % (prediction_task, classifier))\n",
    "                    print(e)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "# test      \n",
    "try:\n",
    "    dh = DataHandler('classifiers-comparison', 'bins-2-uniform_repeats-100', 1)\n",
    "    scores_df = dh.load_obj('results', 'scores')\n",
    "    scores_df = scores_df.dropna(axis='rows', how='all')\n",
    "    display(HTML(scores_df.to_html()))\n",
    "    plot_results(dh, scores_df, 'Classifiers', 0, 3)\n",
    "except Exception as e:\n",
    "    print('No data to display for %s' % dh.root_path)\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests - n_estimators evaluation\n",
    "First for Random Forests Classifier, we study when out of bag score stabilizes for a range of n_estimators values.\n",
    "We will use found n_estimator value with good balance between score and reliability, and performances.\n",
    "\n",
    "Note: we hypothetize that we can find this value on a specific feature set (subject doctor, no phase) and use it for other sets of features, given overall domain and task remain similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_score(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Custom scoring function for hyperparameter optimization. In this case, we want to print out the oob score.\n",
    "    estimator was fitted on train data, X and y here are validation data.\n",
    "    For OOB we don't need validation data - OOB is inherent to tree building.\n",
    "    Same for feature importances (with default gini impurity measure done by sklearn).\n",
    "    We add to this the \n",
    "    \"\"\"\n",
    "    score = estimator.oob_score_\n",
    "    return score\n",
    "\n",
    "\n",
    "splits = prepare_train_data(all_p_df, get_features_set('Doctor', '157015'), prediction_task='presence', \n",
    "                                          upsample='imb_smotenc', bins=2, strategy='uniform', test_set=0.1, n_splits=1)\n",
    "(X, y, X_test, y_test) = splits[0]\n",
    "#n_estimators_range = np.arange(1,100, 10)#\n",
    "n_estimators_range = np.unique(np.logspace(0, 3, 100, dtype=int)) #TODO launch with exp+5 and more values\n",
    "scores = []\n",
    "errs = []\n",
    "test_acc = []\n",
    "test_acc_adjusted = []\n",
    "test_f1 = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "scoring = {\n",
    "    'oob' : oob_score,\n",
    "    'f1_macro' : 'f1_macro',\n",
    "    'precision_macro' : 'precision_macro',\n",
    "    'recall_macro' : 'recall_macro'         \n",
    "}\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "pbar = tqdm_notebook(n_estimators_range, desc='n_estimators')\n",
    "for i in pbar:\n",
    "    pbar.set_description('n_estimators = %d / %s' % (i, n_estimators_range[-1]))\n",
    "    clf = RandomForestClassifier(n_estimators=int(i), oob_score=True)\n",
    "    result = cross_validate(clf, X, y, scoring=scoring, return_train_score=False, cv=5, verbose=0)\n",
    "    scores.append(pd.DataFrame(result).mean().values)\n",
    "    errs.append(pd.DataFrame(result).std().values)\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_acc.append(balanced_accuracy_score(y_test, y_pred))\n",
    "    test_acc_adjusted.append(balanced_accuracy_score(y_test, y_pred, adjusted=True))\n",
    "    test_f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    test_precision.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "    test_recall.append(precision_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scores, columns=pd.DataFrame(result).columns)\n",
    "dferr = pd.DataFrame(errs, columns=pd.DataFrame(result).columns)\n",
    "df = df.drop(['fit_time', 'score_time'], axis=1)\n",
    "dferr = dferr.drop(['fit_time', 'score_time'], axis=1)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hlines(y=df['test_oob'][400:].mean(), xmin=0.0, xmax=1000, color='black')\n",
    "for key in df.columns:\n",
    "    plt.plot(n_estimators_range, df[key], label=key.replace('test_', 'val_'))\n",
    "    plt.fill_between(n_estimators_range, \n",
    "                     df[key]+(dferr[key]/np.sqrt(len(dferr))), \n",
    "                     df[key]-(dferr[key]/np.sqrt(len(dferr))),\n",
    "                     alpha=0.1)\n",
    "plt.plot(n_estimators_range, test_acc, '--', label='Test accuracy')\n",
    "plt.plot(n_estimators_range, test_acc_adjusted, '--', label='Test accuracy (adjusted)')\n",
    "plt.plot(n_estimators_range, test_f1, '--', label='Test f1 (weighted)')\n",
    "plt.plot(n_estimators_range, test_precision, '--', label='Test precision (weighted)')\n",
    "plt.plot(n_estimators_range, test_recall, '--', label='Test recall (weighted)')\n",
    "plt.xticks(n_estimators_range[::4], n_estimators_range[::4])\n",
    "plt.legend()\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('Random Forests results depending on number of trees')\n",
    "dh = DataHandler('common', 'nb_trees_2_uniform')\n",
    "dh.save_fig(plt, 'RF_nb_trees_2bins-uniform-smotenc_detailed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes discretization experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['gnb'],\n",
    "    'subject': ['doctor'],\n",
    "    'prediction_task': ['presence', 'copresence'],\n",
    "    'phases': ['nophase-full'],\n",
    "    'upsampling': [None],\n",
    "    'clstrategy': [None, 'uniform', 'kmeans', 'quantile'],    \n",
    "    'nbclasses': [2, 3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "dh = run_test_plan(test_plan_params, '_discretization', 'NB-G_pres-copres_doctor_noup', 1, \n",
    "                   n_bins=None, bin_strategy=None, n_splits=100, test_set=0.1, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandler('_discretization', 'NB-G_pres-copres_doctor_noup', 1)\n",
    "df = dh.load_obj('results', 'scores')\n",
    "df = df.dropna(axis='rows', how='all')\n",
    "\n",
    "plot_results(dh, df, 'Discretization', df.index.names.index('Target'), \n",
    "             df.index.names.index('Classifier'), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of classifiers\n",
    "\n",
    "We will compare 4 classifiers:\n",
    "\n",
    "* Random (varying strategies)\n",
    "* Nave Bayes\n",
    "* Nave Credal\n",
    "* SVM\n",
    "* Random Forests\n",
    "\n",
    "Random classifier will define a bottom-line. We compute random classifiers scores for different strategies (most-frequent, uniform, stratified) and keep the one with best f1 score for comparison.\n",
    "Nave Bayes will define our baseline, because it is a very simple and effective classifier that is also very fast.\n",
    "So we will effectively compare and keep the best among Nave Credal, SVM and Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['gnb', 'jncc2', 'svm', 'forest'],#, 'forest'],\n",
    "    'subject': ['doctor'],\n",
    "    'prediction_task': ['presence', 'copresence'],\n",
    "    'phases': ['nophase-full'],\n",
    "    'upsampling': [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "dh, grid = run_test_plan(test_plan_params, 'classifiers-comparison', 'bins-2-uniform_repeats-100', 5, \n",
    "                   test_set=0.1, n_bins=2, bin_strategy='uniform', n_splits=10, splits_mode='kfold', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = grid.cv_results_\n",
    "res = pd.DataFrame(res)\n",
    "cols_to_remove = [col for col in res.columns if 'time' in col or 'split' in col or 'rank' in col or 'param' in col]\n",
    "res = res.drop(cols_to_remove, axis=1).T\n",
    "display(HTML(res.to_html()))\n",
    "\n",
    "mean_cols = [col for col in list(res.index) if 'mean' in col]\n",
    "std_cols = [col for col in list(res.index) if 'std' in col]\n",
    "print('0 means')\n",
    "print(res.loc[mean_cols].mean(axis=1).rename(index={name:name.replace('mean_', '') for name in mean_cols}))\n",
    "print()\n",
    "print('1 inter var')\n",
    "inter_var = res.loc[mean_cols].var(axis=1)\n",
    "inter_var = inter_var.rename(index={name:name.replace('mean_', '') for name in mean_cols})\n",
    "print(np.square(res.loc[mean_cols].std(axis=1)))\n",
    "print(inter_var)\n",
    "print()\n",
    "print('2 intra var')\n",
    "intra_var = np.square(res.loc[std_cols]).mean(axis=1)\n",
    "intra_var  = intra_var.rename(index={name:name.replace('std_', '') for name in std_cols})\n",
    "print(intra_var)\n",
    "print()\n",
    "print('3 global var')\n",
    "global_var = inter_var + intra_var\n",
    "print(global_var)\n",
    "\n",
    "print()\n",
    "print('4 confidence interval 95%')\n",
    "ci_95 = global_var / np.sqrt(3*10) * 1.96\n",
    "print(ci_95)\n",
    "#print(np.sqrt((res.loc[mean_cols].var(axis=1))\n",
    "#                        + np.square(res.loc[std_cols]).mean(axis=1)\n",
    "#               )) / np.sqrt(3*10) * 1.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dh = DataHandler('classifiers-comparison', 'bins-2-uniform_repeats-100', 5)\n",
    "df = dh.load_obj('results', 'scores')\n",
    "df = df.dropna(axis='rows', how='all')\n",
    " \n",
    "plot_results(dh, df, 'Classifiers comparison', 0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JNCC2 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['jncc2', 'gnb'],\n",
    "    'subject': ['doctor'],\n",
    "    'prediction_task': ['presence', 'copresence'],\n",
    "    'phases': ['nophase-full', '157015-full'],\n",
    "    'upsampling': [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "\n",
    "\"\"\"logging.basicConfig(filename=config.LOGFILE, level=logging.DEBUG,\n",
    "                        format='%(levelname)s : %(asctime)s : %(name)s : %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "console_handler = logging.StreamHandler()\n",
    "logger.addHandler(console_handler)\"\"\"\n",
    "\n",
    "dh = run_test_plan(test_plan_params, 'jncc2', 'pres_bins-2-uniform_repeats-100', 2, \n",
    "                   n_bins=2, bin_strategy='uniform', n_splits=100, n_jobs=1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randint(2, size=(10,2))\n",
    "df = pd.DataFrame(arr, columns=['a', 'b'])\n",
    "print(df['a'].to_numpy()[:, np.newaxis].shape)\n",
    "jncc_result = np.random.randint(2, size=(10))\n",
    "print(jncc_result)\n",
    "determ = np.sum(np.sum(jncc_result != 6666, axis=1) == 1) / float(len(jncc_result))\n",
    "print(determ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandler('jncc2', 'pres_bins-2-uniform_repeats-100', 2)\n",
    "df = dh.load_obj('results', 'scores')\n",
    "df = df.dropna(axis='rows', how='all')\n",
    " \n",
    "plot_results(dh, df, 'Naive Bayes and Naive Credal', 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: entropy versus angular speed as non-verbal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['gnb'],\n",
    "    'subject': ['doctor'],\n",
    "    'prediction_task': ['presence', 'co-presence'],\n",
    "    'modes': ['nonverbal', 'nonverbal-a'],\n",
    "    'upsampling': [None]\n",
    "}\n",
    "test_plan_params = {\n",
    "    'classifier': ['gnb'],\n",
    "    'subject': ['doctor'],\n",
    "    'prediction_task': ['presence'],\n",
    "    'modes': ['nonverbal-a'],\n",
    "    'upsampling': [None]\n",
    "}\n",
    "dh = run_test_plan(test_plan_params, '__tests', 'metrics_no_gs', 0, \n",
    "                   n_bins=2, bin_strategy='uniform', n_splits=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandler('__tests', 'metrics_no_gs', 0)\n",
    "df = dh.load_obj('results', 'scores')\n",
    "df = df.dropna(axis='rows', how='all')\n",
    " \n",
    "plot_results(dh, df, 'Test - metrics', 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "dh = run_test_plan(test_plan_params, 'entropy_vs_angularSpeed', 'pres_bins-2-uniform_repeats-100', 1, n_bins=2, \n",
    "                   bin_strategy='uniform', n_splits=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandler('entropy_vs_angularSpeed', 'pres_bins-2-uniform_repeats-100', 1)\n",
    "df = dh.load_obj('results', 'scores')\n",
    "df = df.dropna(axis='rows', how='all')\n",
    " \n",
    "plot_results(dh, df, 'Entropy vs Angular speed features', 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: upsampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['gnb'],\n",
    "    'subject': ['doctor', 'agent', 'doctor+agent'],\n",
    "    'prediction_task': ['presence', 'copresence'],\n",
    "    'phases': ['nophase-a', '157015-a'],\n",
    "    'upsampling': [None, 'imb_random', 'imb_smotenc']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "dh = run_test_plan(test_plan_params, 'oversampling_method-a', 'pres_bins-2-uniform_repeats-100', 2, \n",
    "                   n_bins=2, bin_strategy='uniform', n_splits=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandler('oversampling_method-a', 'pres_bins-2-uniform_repeats-100', 2)\n",
    "df = dh.load_obj('results', 'scores')\n",
    "df = df.dropna(axis='rows', how='all')\n",
    "\n",
    "#scores_df = pd.read_excel(os.path.join(dh.root_path, 'results', 'scores.xlsx'), index_col=[0,1,2,3,4,5])\n",
    "#display(HTML(scores_df.to_html()))\n",
    "# .loc(axis=0)[(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(None,None,None),'None',slice(None,None,None))]\n",
    "plot_results(dh, df, 'Oversampling method', 0, 3)#\n",
    "#plot_results(dh, df, 'Oversampling method', 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "dh_upsampling = run_test_plan(test_plan_params, 'upsampling_strategy', 'bins_3_kmeans_50repeats', 0, n_bins=3, bin_strategy='kmeans', n_splits=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['gnb', 'fs-svml1+gnb', 'fs-svm+gnb'],\n",
    "    'subject': ['doctor+agent'],\n",
    "    'prediction_task': ['presence', 'copresence'],\n",
    "    'phases': ['nophase-a', '157015-a'],\n",
    "    'upsampling': [None, 'imb_random', 'imb_smotenc']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "dh = run_test_plan(test_plan_params, '__tests', 'feature-selection_gnb', 0, n_bins=2, bin_strategy='uniform', n_splits=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandler('__tests', 'feature-selection_gnb', 0)\n",
    "scores_df = dh.load_obj('results', 'scores')\n",
    "scores_df = scores_df.dropna(axis='rows', how='all')\n",
    "\n",
    "\n",
    "plot_results(dh, scores_df, 'Feature Selection + Gaussian Naive Bayes', 0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params_upsampling = {\n",
    "    'classifier': ['r-forest'],\n",
    "    'subject': ['doctor+agent'],\n",
    "    'prediction_task': ['presence'],\n",
    "    'modes': ['multimodal'],\n",
    "    'upsampling': [None]\n",
    "}\n",
    "\n",
    "RANDOM_STATE=42\n",
    "dh_upsampling = run_test_plan(test_plan_params_upsampling, 'regression', 'upsampling_strategy', 0, n_bins=0, bin_strategy='kmeans', n_splits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "data = np.arange(1,100)\n",
    "data_y = np.random.choice(np.arange(1, 4), size=len(data), p=[0.1, 0.4, 0.5])\n",
    "print(data_y)\n",
    "print(np.unique(data_y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where(data_y == 1)[0]\n",
    "b = np.where(data_y == 2)[0]\n",
    "c = np.where(data_y == 3)[0]\n",
    "print('a ' + str(a))\n",
    "print('b ' + str(b))\n",
    "print('c ' + str(c))\n",
    "\n",
    "test_ratio = 0.2\n",
    "test_size = int(len(data) * test_ratio)\n",
    "train_size = int(len(data) * (1 - test_ratio))\n",
    "print\n",
    "print('train %d train %d' % (train_size, test_size))\n",
    "def get_exhaustive_splits(arr, test_ratio):\n",
    "    test_arr_size = int(len(arr) * test_ratio)\n",
    "    print(test_arr_size)\n",
    "    test_a = list(combinations(arr, test_arr_size))\n",
    "    test_a = [list(item) for item in test_a]\n",
    "    train_a = []\n",
    "    for comb in test_a:\n",
    "        train_a.append(list(set(arr) - set(comb)))\n",
    "    splits_arr = zip(train_a, test_a)\n",
    "    print(splits_arr)\n",
    "    print(len(splits_arr))\n",
    "    return splits_arr\n",
    "\n",
    "get_exhaustive_splits(a, test_ratio)\n",
    "get_exhaustive_splits(b, test_ratio)\n",
    "get_exhaustive_splits(c, test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Subject and Agent features, phases versus no phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['forest', 'jncc2', 'gnb', 'svm', 'forest'],\n",
    "    'subject': ['doctor+agent', 'doctor', 'agent'],\n",
    "    'prediction_task': ['presence', 'copresence'],\n",
    "    'phases': ['nophase-a', '157015-a'],\n",
    "    'upsampling': [None, 'imb_random', 'imb_smotenc']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_phases = run_test_plan(test_plan_params, '_new_dataset-a', 'phase-nophase_2-classes-uniform', 3, \n",
    "                          n_bins=2, bin_strategy='uniform', n_splits=100, test_set=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dh = DataHandler('_new_dataset-a', 'phase-nophase_2-classes-uniform', 0)\n",
    "df = dh.load_obj('results', 'scores')\n",
    "df = df.dropna(axis='rows', how='all')\n",
    " \n",
    "plot_results(dh, df, 'Phases vs no phase', 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbal, non-verbal, multi-modal and multi-modal + duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test plan definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params_modes = {\n",
    "    'classifier': [ 'gnb'],\n",
    "    'subject': ['doctor', 'agent', 'doctor+agent'],\n",
    "    'prediction_task': ['presence', 'copresence'],\n",
    "    'modes': ['verbal', 'nonverbal', 'multimodal']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_modes = run_test_plan(test_plan_params_modes, '_new_dataset', 'modes_2-classes-uniform_answersDelay', 0, n_bins=2, bin_strategy='uniform', n_splits=100, test_set=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_modes = DataHandler('_new_dataset', 'modes_2-classes-uniform_answersDelay', 0)\n",
    "scores_df = dh_modes.load_obj('results', 'scores')\n",
    "scores_df = scores_df.dropna(axis='rows', how='all')\n",
    " \n",
    "plot_results(dh_modes, scores_df, 'Verbal, non-verbal and multimodal features', 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['gnb', 'fs_rfe-svm-l2_gnb', 'fs_svm-l1_gnb', 'fs_kb5-chi2_gnb', 'fs_kb5-mi_gnb'],\n",
    "    'subject': ['doctor+agent'],\n",
    "    'prediction_task': ['presence', 'copresence'],\n",
    "    'phases': ['157015-a', 'nophase-a']\n",
    "}\n",
    "# using NB-G as reference for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "dh = run_test_plan(test_plan_params, 'feature_selection', 'bins_2_uniform_100repeats', 1, n_bins=2, bin_strategy='uniform', test_set=0.1, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandler('feature_selection', 'bins_2_uniform_100repeats', 0)\n",
    "df = dh_modes.load_obj('results', 'scores')\n",
    "df = df.dropna(axis='rows', how='all')\n",
    " \n",
    "plot_results(dh, df, 'Automatic Feature Selection', 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "test_plan_params = {\n",
    "    'classifier': ['forest'],\n",
    "    'subject': ['doctor+agent', 'doctor', 'agent'],\n",
    "    'target': ['presence', 'copresence'],\n",
    "    'modes': ['verbal', 'nonverbal', 'multimodal']\n",
    "}\n",
    "test_plan = list(ParameterGrid(test_plan_params))\n",
    "\n",
    "forest = RandomForestClassifier(random_state=RANDOM_STATE, oob_score=True)\n",
    "svm = make_pipeline(StandardScaler(), SVC(random_state=RANDOM_STATE, probability=True))\n",
    "\n",
    "# whether to apply 'original' upsampling method\n",
    "upsample = True\n",
    "\n",
    "cols= 6\n",
    "fig, ax = plt.subplots(int(len(test_plan)/cols), cols, sharex=True, sharey=True, figsize=(30,15))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for idx, test in enumerate(test_plan):\n",
    "    print(\"Test %d : %s\" % (idx, test))\n",
    "    \n",
    "\n",
    "    subject = test['subject']\n",
    "    target = test['target']\n",
    "    modes = test['modes']\n",
    "    classifier = test['classifier']\n",
    "    clf = forest if classifier == 'forest' else svm\n",
    "\n",
    "    if subject == 'doctor':\n",
    "        df = subject_np_df\n",
    "        if modes == 'verbal':\n",
    "            feats = subject_features_verbal\n",
    "        elif modes == 'nonverbal':\n",
    "            feats = subject_features_nonverbal\n",
    "        elif modes == 'multimodal':\n",
    "            feats = subject_features_multimodal\n",
    "        elif modes == 'multimodal+duration':\n",
    "            feats = subject_features_multimodal_duration\n",
    "    elif subject == 'agent':\n",
    "        df = agent_np_df\n",
    "        if modes == 'verbal':\n",
    "            feats = agent_features_verbal\n",
    "        elif modes == 'nonverbal':\n",
    "            feats = agent_features_nonverbal\n",
    "        elif modes == 'multimodal':\n",
    "            feats = agent_features_multimodal\n",
    "        elif modes == 'multimodal+duration':\n",
    "            feats = agent_features_multimodal_duration        \n",
    "    else:\n",
    "        df = all_np_df\n",
    "        if modes == 'verbal':\n",
    "            feats = all_features_verbal\n",
    "        elif modes == 'nonverbal':\n",
    "            feats = all_features_nonverbal\n",
    "        elif modes == 'multimodal':\n",
    "            feats = all_features_multimodal\n",
    "        elif modes == 'multimodal+duration':\n",
    "            feats = all_features_multimodal_duration        \n",
    "    \n",
    "    X, y = prepare_train_data(samples=df, features=feats, target_model=target, upsample=upsample) \n",
    "    \n",
    "    isDoc = 'doctor' in subject\n",
    "    isAgent = 'agent' in subject\n",
    "    presidx = 'Presence' if target == 'presence' else 'Co-Presence'\n",
    "    if subject == 'doctor':\n",
    "        subjidx = 'Doctor'\n",
    "    elif subject == 'agent':\n",
    "        subjidx = 'Agent'\n",
    "    else:\n",
    "        subjidx = 'Doctor+Agent'\n",
    "    if modes == 'verbal':\n",
    "        phidx = 'Verbal'\n",
    "    elif modes == 'nonverbal':\n",
    "        phidx = 'Non-Verbal'\n",
    "    elif modes == 'multimodal':\n",
    "        phidx = 'Multimodal'\n",
    "    elif modes == 'multimodal+duration':\n",
    "        phidx = 'Multimodal+Duration'\n",
    "    clfidx = 'Random Forests' if classifier == 'forest' else 'SVM'\n",
    "\n",
    "    test_set_name = dh.get_grid_name(target=='presence', isDoc, isAgent, modes, classifier)\n",
    "    \n",
    "    # create classifier from best params found previously\n",
    "    best_params = context_df.loc[presidx, subjidx, phidx, clfidx, 'score']['Best Params']\n",
    "    clf.set_params(**eval(best_params))\n",
    "    print(\"  Computing learning curve for best params : %s\" % str(best_params))\n",
    "\n",
    "    train_sizes = np.arange(35, len(df))\n",
    "    print(train_sizes)\n",
    "    print(np.unique(y, return_counts=True))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, \n",
    "                                                            X, y, \n",
    "                                                            train_sizes=train_sizes, \n",
    "                                                            cv=10, \n",
    "                                                            scoring='neg_log_loss',\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    results[test_set_name] = {}\n",
    "    results[test_set_name]['train'] = train_scores\n",
    "    results[test_set_name]['test'] = test_scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    cur_ax = ax[int(idx / cols), (idx) % cols]\n",
    "    cur_ax.plot(train_sizes, train_scores_mean, label='train')\n",
    "    cur_ax.fill_between(train_sizes, train_scores_mean + train_scores_std,\n",
    "                    train_scores_mean - train_scores_std, alpha=0.2)\n",
    "\n",
    "    cur_ax.plot(train_sizes, test_scores_mean, label='valid')\n",
    "    cur_ax.fill_between(train_sizes, test_scores_mean + test_scores_std,\n",
    "                    test_scores_mean - test_scores_std, alpha=0.2)\n",
    "    cur_ax.set_title(test_set_name)\n",
    "    #cur_ax.xlabel(\"Count of samples\")\n",
    "    #cur_ax.ylabel(\"Negative log loss\")\n",
    "    if idx == 0: cur_ax.legend()\n",
    "fig.text(0.5, 0.1, 'count of data samples', ha='center', va='center')\n",
    "fig.text(0.1, 0.5, 'negative log loss', ha='center', va='center', rotation='vertical')\n",
    "dh.save_fig('learning_curves_verbal-nonverbal_forest.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.save_obj(results, 'learning_curves', 'forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-verbal case we only have 2 features, we have then a chance to vizualize the classification results for some classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "df = subject_p_df\n",
    "feats = all_features\n",
    "target = 'presence'\n",
    "\n",
    "n_classes = 3\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "\n",
    "#best_params = context_df.loc['Presence', 'Doctor', 'Non-Verbal', 'Random Forests', 'score']['Best Params']\n",
    "#forest.set_params(**eval(best_params))\n",
    "forest = RandomForestClassifier(n_estimators=1)\n",
    "X, y = prepare_train_data(samples=df, features=feats, target_model=target, upsample='imb_smote', test_set=False)\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "forest.fit(X_reduced,y)\n",
    "\n",
    "model = forest\n",
    "x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n",
    "y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "# alpha blend the\n",
    "# decision surfaces of the ensemble of classifiers\n",
    "\n",
    "# Choose alpha blend level with respect to the number\n",
    "# of estimators\n",
    "# that are in use (noting that AdaBoost can use fewer estimators\n",
    "# than its maximum if it achieves a good enough fit early on)\n",
    "estimator_alpha = 1.0 / len(model.estimators_)\n",
    "for idx, tree in enumerate(model.estimators_):\n",
    "    print idx\n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "\n",
    "# Build a coarser grid to plot a set of ensemble classifications\n",
    "# to show how these are different to what we see in the decision\n",
    "# surfaces. These points are regularly space and do not have a\n",
    "# black outline\n",
    "xx_coarser, yy_coarser = np.meshgrid(\n",
    "    np.arange(x_min, x_max, plot_step_coarser),\n",
    "    np.arange(y_min, y_max, plot_step_coarser))\n",
    "Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n",
    "                                 yy_coarser.ravel()]\n",
    "                                 ).reshape(xx_coarser.shape)\n",
    "cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
    "                        c=Z_points_coarser, cmap=cmap,\n",
    "                        edgecolors=\"none\")\n",
    "\n",
    "# Plot the training points, these are clustered together and have a\n",
    "# black outline\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,\n",
    "            cmap=ListedColormap(['r', 'y', 'b']),\n",
    "            edgecolor='k', s=20)\n",
    "plt.xlabel(feats[0])\n",
    "plt.ylabel(feats[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, **params)\n",
    "    \n",
    "df = all_p_df\n",
    "feats = get_features_set('Doctor', '157015')\n",
    "target = 'presence'\n",
    "\n",
    "n_classes = 2\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "\n",
    "#best_params = context_df.loc['Presence', 'Doctor', 'Non-Verbal', 'SVM', 'score']['Best Params']\n",
    "#svm.set_params(**eval(best_params))\n",
    "\n",
    "splits = prepare_train_data(samples=df, features=feats, prediction_task=target, upsample='imb_random', test_set=0.1, n_splits=1, bins=n_classes, strategy='uniform', verbose=1) \n",
    "(X, y, X_test, y_test ) = splits[0]\n",
    "\n",
    "svm = make_pipeline(StandardScaler(), SVC(random_state=RANDOM_STATE))\n",
    "param_grid_svm = {\n",
    "    'svc__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'svc__C': np.logspace(-2, 1, 4),\n",
    "    'svc__gamma': np.logspace(-3, 1, 5)\n",
    "}\n",
    "grid = gridsearch(svm, \n",
    "                  X, y, \n",
    "                  'presence', \n",
    "                  param_grid=param_grid_svm, \n",
    "                  features=feats, \n",
    "                  verbose=verbose)\n",
    "best_params = grid.best_params_\n",
    "single_best_score = grid.best_score_\n",
    "\n",
    "print('  Gridsearch evaluated best score %s' % (single_best_score))\n",
    "print('    Best params %s ...' % (best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "Cs = [0.0001, 0.001, 0.01, 1, 10, 100]\n",
    "gammas = [0.001, 0.01, 0.1, 0.1, 1, 10]\n",
    "for idx, (C, gamma) in enumerate(zip(Cs, gammas)):\n",
    "\n",
    "    plt.subplot(2,3,idx+1)\n",
    "    \n",
    "    svm = SVC(C=C, gamma=gamma, kernel='rbf', random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_ = scaler.transform(X)\n",
    "    pca = PCA(n_components=2).fit(X_)\n",
    "    X_reduced = pca.transform(X_)\n",
    "    svm.fit(X_reduced,y)\n",
    "    X_test_ = scaler.transform(X_test)\n",
    "    X_test_reduced = pca.transform(X_test_)\n",
    "    y_pred = svm.predict(X_test_reduced)\n",
    "    train_score = balanced_accuracy_score(y, svm.predict(X_reduced))\n",
    "    test_score = balanced_accuracy_score(y_test, svm.predict(X_test_reduced))\n",
    "    scores = 'train %.02f test %.02f' % (train_score, test_score)\n",
    "\n",
    "    #print('TRAIN ' + str(svm.score(X_reduced, y)))\n",
    "\n",
    "    #fix, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "    # Set-up 2x2 grid for plotting.\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    X0, X1 = X_reduced[:, 0], X_reduced[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "    plot_contours(svm, xx, yy,cmap=plt.cm.coolwarm, alpha=0.6)\n",
    "    sct = plt.scatter(X0[y==0], X1[y==0], c='r', cmap=plt.cm.coolwarm, s=40, edgecolors='k', label='train class 1')\n",
    "    sct = plt.scatter(X0[y==1], X1[y==1], c='b', cmap=plt.cm.coolwarm, s=40, edgecolors='k', label='train class 2')\n",
    "\n",
    "    y_pred = svm.predict(X_test_reduced)\n",
    "    X2, X3 = X_test_reduced[:, 0], X_test_reduced[:, 1]\n",
    "\n",
    "    sct2 = plt.scatter(X2[y_test==0], X3[y_test==0], c='r', cmap=plt.cm.coolwarm, s=80, marker='x', edgecolors='k', label='test class 1')\n",
    "    sct2 = plt.scatter(X2[y_test==1], X3[y_test==1], c='b', cmap=plt.cm.coolwarm, s=80, marker='x', edgecolors='k', label='test class 2')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title('C=%f - %s' %(C, scores))\n",
    "    if idx==0:\n",
    "        plt.xlabel('PCA[0]')\n",
    "        plt.ylabel('PCA[1]')        \n",
    "        plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phase / no phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs for SVM phases vs no phase, Presence and Co-Presence\n",
    "\n",
    "dh = DataHandler('1-subject-agent_phases-nophase', 2)\n",
    "scores_df_index = [['Presence', 'Co-Presence'], \n",
    "             ['Doctor', 'Agent', 'Doctor+Agent'], \n",
    "             ['No Phase', '157015'], \n",
    "             ['Random Forests', 'SVM'],\n",
    "             ['score', 'err']\n",
    "            ]\n",
    "scores_df_columns = ['precision', 'f1', 'recall', 'params']\n",
    "scores_df_multiindex = pd.MultiIndex.from_product(scores_df_index, \n",
    "                                                  names=['Target', 'Subject', 'Phases', 'Classifier', 'Metric'])\n",
    "scores_df = dh.load_obj('results', 'scores')\n",
    "if scores_df is None:\n",
    "    print(\"No data to load, nothing to draw\")\n",
    "\n",
    "context_df = pd.read_csv(os.path.join(dh.root_path, 'context.csv'))\n",
    "if context_df is None:\n",
    "    print(\"missing contextual info context.pkl\")\n",
    "    \n",
    "context_df.index = scores_df_multiindex\n",
    "context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_params = {\n",
    "    'classifier': ['forest'],\n",
    "    'subject': ['doctor+agent', 'doctor', 'agent'],\n",
    "    'target': ['presence', 'copresence'],\n",
    "    'phases': [(0,1,0), (0.15,0.70,0.15)]\n",
    "}\n",
    "test_plan = list(ParameterGrid(test_plan_params))\n",
    "\n",
    "forest = RandomForestClassifier(random_state=RANDOM_STATE, oob_score=True)\n",
    "svm = make_pipeline(StandardScaler(), SVC(random_state=RANDOM_STATE, probability=True))\n",
    "\n",
    "# whether to apply 'original' upsampling method\n",
    "upsample = True\n",
    "\n",
    "cols= 6\n",
    "fig, ax = plt.subplots(int(len(test_plan)/cols), cols, sharex=True, sharey=True, figsize=(30,10))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for idx, test in enumerate(test_plan):\n",
    "    print(\"Test %d : %s\" % (idx, test))\n",
    "    \n",
    "\n",
    "    subject = test['subject']\n",
    "    target = test['target']\n",
    "    phases = test['phases']\n",
    "    classifier = test['classifier']\n",
    "    clf = forest if classifier == 'forest' else svm\n",
    "    pgrid = param_grid_rf if classifier == 'forest' else param_grid_svm\n",
    "\n",
    "    if phases is not None and not phases == (0,1,0):\n",
    "        print(\"phases \" + str(phases))\n",
    "        if subject == 'doctor':\n",
    "            df = subject_p_df\n",
    "            feats = subject_features\n",
    "        elif subject == 'agent':\n",
    "            df = agent_p_df\n",
    "            feats = agent_features\n",
    "        elif subject == 'doctor+agent':\n",
    "            df = all_p_df\n",
    "            feats=all_features\n",
    "    else:\n",
    "        print(\"No phase\")\n",
    "        if subject == 'doctor':\n",
    "            df = subject_np_df\n",
    "            feats = subject_features_nophase\n",
    "        elif subject == 'agent':\n",
    "            df = agent_np_df\n",
    "            feats = agent_features_nophase\n",
    "        elif subject == 'doctor+agent':\n",
    "            df = all_np_df\n",
    "            feats=all_features_nophase        \n",
    "    \n",
    "    X, y = prepare_train_data(samples=df, features=feats, target_model=target, upsample=upsample) \n",
    "    \n",
    "    isDoc = 'doctor' in subject\n",
    "    isAgent = 'agent' in subject\n",
    "    presidx = 'Presence' if target == 'presence' else 'Co-Presence'\n",
    "    if subject == 'doctor':\n",
    "        subjidx = 'Doctor'\n",
    "    elif subject == 'agent':\n",
    "        subjidx = 'Agent'\n",
    "    else:\n",
    "        subjidx = 'Doctor+Agent'\n",
    "    phidx = 'No Phase' if phases in [None, (0,1,0)] else '%02d%02d%02d' % ( phases[0]*100, phases[1]*100, phases[2]*100 )\n",
    "    clfidx = 'Random Forests' if classifier == 'forest' else 'SVM'\n",
    "\n",
    "    test_set_name = dh.get_grid_name(target=='presence', isDoc, isAgent, phases, classifier)\n",
    "    \n",
    "    # create classifier from best params found previously\n",
    "    best_params = context_df.loc[presidx, subjidx, phidx, clfidx, 'score']['Best Params']\n",
    "    clf.set_params(**eval(best_params))\n",
    "    print(\"  Computing learning curve for best params : %s\" % str(best_params))\n",
    "\n",
    "    train_sizes = np.arange(35, len(df))\n",
    "    print(train_sizes)\n",
    "    print(np.unique(y, return_counts=True))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, \n",
    "                                                            X, y, \n",
    "                                                            train_sizes=train_sizes, \n",
    "                                                            cv=10, \n",
    "                                                            scoring='neg_log_loss',\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    results[test_set_name] = {}\n",
    "    results[test_set_name]['train'] = train_scores\n",
    "    results[test_set_name]['test'] = test_scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    cur_ax = ax[int(idx / cols), (idx) % cols]\n",
    "    cur_ax.plot(train_sizes, train_scores_mean, label='train')\n",
    "    cur_ax.fill_between(train_sizes, train_scores_mean + train_scores_std,\n",
    "                    train_scores_mean - train_scores_std, alpha=0.2)\n",
    "\n",
    "    cur_ax.plot(train_sizes, test_scores_mean, label='valid')\n",
    "    cur_ax.fill_between(train_sizes, test_scores_mean + test_scores_std,\n",
    "                    test_scores_mean - test_scores_std, alpha=0.2)\n",
    "    cur_ax.set_title(test_set_name)\n",
    "    #cur_ax.xlabel(\"Count of samples\")\n",
    "    #cur_ax.ylabel(\"Negative log loss\")\n",
    "    if idx == 0: cur_ax.legend()\n",
    "fig.text(0.5, 0.09, 'count of data samples', ha='center', va='center')\n",
    "fig.text(0.1, 0.5, 'negative log loss', ha='center', va='center', rotation='vertical')\n",
    "dh.save_fig('learning_curves_phase-nophase_forest.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.save_obj(results, 'learning_curves', 'forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.plot(train_sizes, train_scores_mean, label='train')\n",
    "plt.fill_between(train_sizes, train_scores_mean + train_scores_std,\n",
    "                train_scores_mean - train_scores_std, alpha=0.2)\n",
    "\n",
    "plt.plot(train_sizes, test_scores_mean, label='valid')\n",
    "plt.fill_between(train_sizes, test_scores_mean + test_scores_std,\n",
    "                test_scores_mean - test_scores_std, alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train_scores, valid_scores = validation_curve(clf, X, y, \"n_estimators\",\n",
    "                                               np.arange(1, 300, 10),\n",
    "                                               cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = valid_scores\n",
    "x_range = np.arange(1,300,10)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.plot(x_range, train_scores_mean, label='train')\n",
    "plt.fill_between(x_range, train_scores_mean + train_scores_std,\n",
    "                train_scores_mean - train_scores_std, alpha=0.2)\n",
    "\n",
    "plt.plot(x_range, test_scores_mean, label='valid')\n",
    "plt.fill_between(x_range, test_scores_mean + test_scores_std,\n",
    "                test_scores_mean - test_scores_std, alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "plt.xlabel(\"x\", size=5)\n",
    "plt.ylabel(\"y\", size=5)\n",
    "plt.title('SVM Decision Region Boundary', size=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features importance experiments\n",
    "\n",
    "We would like to check variance and quality of this estimator.\n",
    "For this we first experiment cross validations with searching also n_estimators, or fixing it with a high value, and check if and how best parameters vary.\n",
    "Then we will check for a specific set of params, how results and feature importance vary upon repeated runs.\n",
    "Depending on results, we will compute feature importance for all test plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if 'PresenceClass_5_kmeans' in all_p_df.columns:\n",
    "    all_p_df = all_p_df.drop(['PresenceClass_5_kmeans'], axis=1)\n",
    "    \n",
    "dataset = all_p_df\n",
    "featureset = get_features_set('Doctor', '157015')\n",
    "print(featureset)\n",
    "ds_splits = prepare_train_data(dataset, featureset, \n",
    "                                          'presence', bins=5, strategy='kmeans', upsample='imb_smotenc', test_set=0.1, n_splits=1, arff='test1')\n",
    "print('Generated %d splits' % len(ds_splits))\n",
    "for (X, y, X_test, y_test) in ds_splits:\n",
    "    print(np.unique(y, return_counts=True))\n",
    "    print(np.unique(y_test, return_counts=True))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3).fit(X)\n",
    "    k_test_labels = kmeans.predict(X_test)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    X_ = pca.fit_transform(X)\n",
    "    X_test_ = pca.transform(X_test)\n",
    "    print(X_.shape)\n",
    "    plt.scatter(X_[:, 0], X_[:, 1], c=y, cmap='viridis', marker='o')\n",
    "    plt.scatter(X_test_[:, 0], X_test_[:, 1], c=y_test, cmap='viridis', marker='s')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection by feature clustering and removal of correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "markers = ['s', 'o', '8', 'p', 'P', '1']\n",
    "dataset = all_p_df\n",
    "featureset = get_features_set('Doctor+Agent', 'No Phase-FULL')\n",
    "print(featureset)\n",
    "print(featureset.values.shape)\n",
    "ds_splits = prepare_train_data(dataset, featureset, \n",
    "                               'presence', bins=2, strategy='kmeans', upsample='imb_smotenc', test_set=0.1, n_splits=1)\n",
    "\n",
    "def get_categorical_features(X):\n",
    "    if 'Expert' in X.columns:\n",
    "        return ['Expert']\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('Generated %d splits' % len(ds_splits))\n",
    "for (X, y, X_test, y_test) in ds_splits:\n",
    "    print(X.shape)\n",
    "    # revert to cluster features instead of samples\n",
    "    ct = make_column_transformer(\n",
    "        (PassthroughTransformer(), get_categorical_features), # default FunctionTransformer implements identity\n",
    "        remainder=CustomStandardScaler()\n",
    "    )\n",
    "    display(HTML(X.to_html()))\n",
    "    X_scaled = ct.fit_transform(X)\n",
    "    print(ct)\n",
    "    feature_names = parse_feature_names(ct.get_feature_names())\n",
    "    print(feature_names)\n",
    "    display(HTML(pd.DataFrame(X_scaled, columns=feature_names).to_html()))\n",
    "    X = X_scaled.T\n",
    "    #X['Expert'] = get_features_set('Doctor+Agent', 'No Phase-FULL').loc['Expert'] \n",
    "    print(X.shape)\n",
    "    display(HTML(pd.DataFrame(X, index=feature_names).to_html()))\n",
    "    print(np.unique(y, return_counts=True))\n",
    "    print(np.unique(y_test, return_counts=True))\n",
    "\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    X_ = pca.fit_transform(X)\n",
    "    #X_test_ = pca.transform(X_test)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=3).fit(X_)\n",
    "    y_ = kmeans.predict(X_)\n",
    "    #y_test_ = kmeans.predict(X_test_)\n",
    "    \n",
    "    plt.scatter(X_[:, 0], X_[:, 1], c=y_, marker='o')\n",
    "    for i in np.unique(kmeans.labels_):\n",
    "        group_feats = list(featureset[y_ == i])\n",
    "        group_feats_corr_score = pd.DataFrame(np.zeros((len(group_feats))), index=group_feats).T\n",
    "        print('group #%d : %s' % (i, group_feats))\n",
    "        for feat1 in group_feats:\n",
    "            for feat2 in group_feats:\n",
    "                if feat1 is not feat2:\n",
    "                    idx_feat1 = list(featureset).index(feat1)\n",
    "                    idx_feat2 = list(featureset).index(feat2)\n",
    "                    print('%d %d' % (idx_feat1, idx_feat2))\n",
    "                    print(X[idx_feat1])\n",
    "                    print(X[idx_feat2])\n",
    "                    corr_score = pearsonr(X[idx_feat1], X[idx_feat2])[0]\n",
    "                    print('Pearson %s vs %s : %f' % (feat1, feat2, corr_score))\n",
    "                    group_feats_corr_score[feat1] += np.abs(corr_score)\n",
    "                    group_feats_corr_score[feat2] += np.abs(corr_score)\n",
    "        display(HTML(group_feats_corr_score.to_html()))\n",
    "        print('feature to keep: %s' % group_feats[np.argmin(group_feats_corr_score.values)])\n",
    "        \n",
    "    print(zip(list(featureset), y_))\n",
    "    \n",
    "    \"\"\"print(X_.shape)\n",
    "    plt.scatter(X_[:, 0], X_[:, 1], c=y, cmap='viridis', marker='o')\n",
    "    plt.scatter(X_test_[:, 0], X_test_[:, 1], c=y_test, cmap='viridis', marker='s')\n",
    "    plt.show()\n",
    "    \n",
    "    for idx, label in enumerate(np.unique(k_test_labels)):\n",
    "        plt.scatter()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_feature_names(feature_names):\n",
    "    reg = r'.*__(.*)'\n",
    "    return [re.findall(reg, feat)[0] for feat in feature_names]\n",
    "\n",
    "print(ct.get_feature_names())\n",
    "parse_feature_names(ct.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

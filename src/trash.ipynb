{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Avg_SentenceLength_Mid_agent\n",
    "def plot_variables(samples, idx_1, idx_2, target_column):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(samples.iloc[:, idx_1], samples.iloc[:, idx_2], c=samples[target_column], s=300)\n",
    "    for idx in np.arange(len(samples)):\n",
    "        plt.annotate(str(idx), (samples.iloc[idx, idx_1], samples.iloc[idx, idx_2]))\n",
    "    plt.xlabel(samples.columns[idx_1])\n",
    "    plt.ylabel(samples.columns[idx_2])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "col1 = 'Avg_AnswersDelay_Mid'\n",
    "col2 = 'Avg_AnswersDelay_Mid_agent'    \n",
    "\n",
    "plot_variables(all_np_df, list(all_np_df.columns).index(col1), list(all_np_df.columns).index(col2), 'Presence Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "if 'PresenceClass_5_kmeans' in all_p_df.columns:\n",
    "    all_np_df = all_np_df.drop(['PresenceClass_5_kmeans'], axis=1)\n",
    "    \n",
    "dataset = all_p_df\n",
    "featureset = get_features_set('Doctor+Agent', '157015-A')\n",
    "print(featureset)\n",
    "nb = 100\n",
    "ds_splits = prepare_train_data(dataset, featureset,\n",
    "                                          'presence', bins=2, strategy='uniform', upsample=None, test_set=0.1, n_splits=nb,  verbose=0)\n",
    "#print('Generated %d splits' % len(ds_splits))\n",
    "scores = np.zeros((nb, 7))\n",
    "\n",
    "for idx, (X, y, X_test, y_test) in enumerate(ds_splits):\n",
    "    #print(np.unique(y, return_counts=True))\n",
    "    print('X ' + str(X.shape))\n",
    "    sc = StandardScaler()\n",
    "    X = sc.fit_transform(X)\n",
    "    X_test = sc.transform(X_test)\n",
    "    print(\"y : %s\" % str(np.unique(y, return_counts=True)))\n",
    "    \n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    lsvc = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    print('LSVC feature selection from %s to %s' % (str(X.shape), str(X_new.shape)))\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.feature_selection import RFECV\n",
    "    \n",
    "    lsvc = LinearSVC(C=0.1, penalty=\"l1\", dual=False)\n",
    "    rfecv = RFECV(estimator=lsvc, step=1, cv=StratifiedKFold(10), scoring='f1')\n",
    "    rfecv.fit(X, y)\n",
    "\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto', n_components=5)\n",
    "    lda.fit(X, y)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    scores[idx, 0] = balanced_accuracy_score(y_test, y_pred, adjusted=False)\n",
    "    scores[idx, 1] = balanced_accuracy_score(y_test, y_pred, adjusted=True)\n",
    "    print('X ' + str(X.shape) + ', coef_ ' + str(lda.coef_))\n",
    "    \n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(X, y)\n",
    "    y_pred = qda.predict(X_test)\n",
    "    scores[idx, 2] = balanced_accuracy_score(y_test, y_pred, adjusted=False)\n",
    "    scores[idx, 3] = balanced_accuracy_score(y_test, y_pred, adjusted=True)\n",
    "    print(str(scores[idx]))\n",
    "    #trans = lda.transform(X)\n",
    "    #trans_test = lda.transform(X_test)\n",
    "    #plt.scatter(trans[:, 0], trans[:, 1], c=y, marker='o')\n",
    "    #plt.scatter(trans_test[:, 0], trans_test[:, 1], c=y_test, marker='x')\n",
    "    #plt.show()\n",
    "    \n",
    "    dummy = DummyClassifier(strategy='uniform', random_state=RANDOM_STATE)\n",
    "    dummy.fit(X,y)\n",
    "    y_pred = dummy.predict(X_test)\n",
    "    scores[idx, 4] = balanced_accuracy_score(y_test, y_pred, adjusted=False)\n",
    "    \n",
    "    dummy = DummyClassifier(strategy='stratified', random_state=RANDOM_STATE)\n",
    "    dummy.fit(X,y)\n",
    "    y_pred = dummy.predict(X_test)\n",
    "    scores[idx, 5] = balanced_accuracy_score(y_test, y_pred, adjusted=False)\n",
    "    \n",
    "    dummy = DummyClassifier(strategy='most_frequent', random_state=RANDOM_STATE)\n",
    "    dummy.fit(X,y)\n",
    "    y_pred = dummy.predict(X_test)\n",
    "    scores[idx, 6] = balanced_accuracy_score(y_test, y_pred, adjusted=False)    \n",
    "means = np.mean(scores, axis=0)\n",
    "print('mean ' + str(means))\n",
    "best_rnd = np.max(means[4:-1])\n",
    "print('best random' + str(best_rnd))\n",
    "print('adjusted with best rnd ' + str(means[0] - best_rnd))\n",
    "\n",
    "print('std ' + str(np.std(scores, axis=0) / nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del sys.modules['feutils']\n",
    "    del sys.modules['config']\n",
    "except:\n",
    "    pass\n",
    "from feutils import DataHandler\n",
    "from feutils import JNCC2Wrapper\n",
    "import config\n",
    "\n",
    "if 'PresenceClass_5_kmeans' in all_p_df.columns:\n",
    "    all_np_df = all_np_df.drop(['PresenceClass_5_kmeans'], axis=1)\n",
    "    \n",
    "dataset = all_np_df\n",
    "featureset = get_features_set('Doctor', 'Multimodal-A')\n",
    "dh = DataHandler('__tests', 'jncc2', 0)\n",
    "print(featureset)\n",
    "nb = 100\n",
    "jncc = JNCC2Wrapper(dh)\n",
    "ds_splits = prepare_train_data(dataset, featureset,\n",
    "                                          'presence', bins=5, strategy='kmeans', upsample='imb_smotenc', test_set=0.1, n_splits=nb, clf=jncc, verbose=0)\n",
    "#print('Generated %d splits' % len(ds_splits))\n",
    "scores = np.zeros((nb, 2))\n",
    "\n",
    "for idx, (X, y, X_test, y_test) in enumerate(ds_splits):\n",
    "    #print(np.unique(y, return_counts=True))\n",
    "    #print('X ' + str(X.shape))\n",
    "    #sc = StandardScaler()\n",
    "    #X = sc.fit_transform(X)\n",
    "    #X_test = sc.transform(X_test)\n",
    "    \n",
    "    #print(dataset['Presence Score'])\n",
    "    #print(dataset['Presence Score'].iloc[y.index])\n",
    "\n",
    "    jncc_cv, y_trues_cv = jncc.cv('train.arff', 0)\n",
    "    \n",
    "    jncc_result, y_true_predict = jncc.predict('train.arff', 'test.arff', 0)\n",
    "    \n",
    "    # make it regression\n",
    "    y_train, enc = discretize(dataset['Presence Score'], 3, 'kmeans')\n",
    "    idx_train = y.index\n",
    "    idx_test = y_test.index\n",
    "    y = y_train[idx_train]\n",
    "    y_test = y_train[idx_test]\n",
    "    y_reg = dataset['Presence Score'].iloc[idx_train]\n",
    "    y_reg_test = dataset['Presence Score'].iloc[idx_test]\n",
    "    \n",
    "    from sklearn import linear_model\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.feature_selection import RFE\n",
    "    \n",
    "    #reg = linear_model.Lasso(alpha=0.1)\n",
    "    reg = RandomForestRegressor(n_estimators=500)\n",
    "    rfe = RFE(estimator=reg, n_features_to_select=2, step=1)\n",
    "    \n",
    "    #reg = LogisticRegression()\n",
    "    rfe.fit(X, y_reg)\n",
    "    print(zip(featureset.to_numpy(), rfe.support_))\n",
    "    print(\"Regression score %f\" % reg.score(X_test, y_reg_test))\n",
    "    y_reg_pred = reg.predict(X_test)\n",
    "    print(zip(featureset.to_numpy(), reg.feature_importances_))\n",
    "    \n",
    "    #print(enc.inverse_transform(y_train))\n",
    "    #print(\"shape \" + str(y_train.shape))\n",
    "    #print(\"shape pred \" + str(np.array(y_reg_pred).reshape(-1, 1).shape))\n",
    "    y_pred = enc.transform(np.array(y_reg_pred).reshape(-1, 1))\n",
    "    \n",
    "    scores[idx, 0] = balanced_accuracy_score(y_test, y_pred, adjusted=False)\n",
    "    scores[idx, 1] = balanced_accuracy_score(y_test, y_pred, adjusted=True)\n",
    "    \n",
    "print('mean ' + str(np.mean(scores, axis=0)))\n",
    "print('std ' + str(np.std(scores, axis=0) / nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def compute_lrap(y_true, y_pred, labels):\n",
    "    if type(y_true) is np.ndarray:\n",
    "        y_true = pd.DataFrame(y_true)\n",
    "    if type(y_pred) is np.ndarray:\n",
    "        y_pred = pd.DataFrame(y_pred)\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(labels)\n",
    "    y_score = np.zeros(y_pred.values.shape)\n",
    "    for i in np.arange(y_score.shape[1]):\n",
    "        col = y_pred.iloc[:, i]\n",
    "        binarized = lb.transform(col)\n",
    "        y_score = y_score + (binarized.astype(float) / (i+1))\n",
    "    y_true_bin = lb.transform(y_true.values.reshape(-1,1))\n",
    "    return label_ranking_average_precision_score(y_true_bin, y_score)\n",
    "\n",
    "print(compute_lrap(y_test, jncc_result, [0,1,2,3,4]))\n",
    "cv_lraps = [compute_lrap(y_trues_cv[i], jncc_cv[i], [0,1,2,3,4]) for i in np.arange(len(jncc_cv))]\n",
    "cv_lraps = np.array(cv_lraps)\n",
    "print(cv_lraps)\n",
    "print(cv_lraps.mean())\n",
    "print(cv_lraps.std())\n",
    "print(balanced_accuracy_score(y_true_predict, jncc_result.iloc(axis=1)[0], adjusted=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo \"Avg_IPUlen_Begin\", \"Avg_IPUlen_Middle\", \"Avg_IPUlen_End\" removed because of wrong values\n",
    "#__features = [\"Expert\", \"Head_Entropy_Start\", \"Head_Entropy_Mid\", \"Head_Entropy_End\", \"Avg_HandEntropy_Begin\",\n",
    "#             \"Avg_HandEntropy_Mid\", \"Avg_HandEntropy_End\", \"Avg_SentenceLength_Begin\", \"Avg_SentenceLength_Mid\",\n",
    "#             \"Avg_SentenceLength_End\",  \"Ratio1_Begin\", \"Ratio1_Mid\", \"Ratio1_End\", \"Ratio2_Begin\", \"Ratio2_Mid\", \n",
    "#                    \"Ratio2_End\", \"Duration\"]\n",
    "\"\"\"mean_angular_features = [\n",
    "    \"MeanYawAngularSpeed_Head_Start\", \"MeanYawAngularSpeed_Head_Mid\", \"MeanYawAngularSpeed_Head_End\",\n",
    "    \"MeanPitchAngularSpeed_Head_Start\", \"MeanPitchAngularSpeed_Head_Mid\", \"MeanPitchAngularSpeed_Head_End\",\n",
    "    \"MeanRollAngularSpeed_Head_Start\", \"MeanRollAngularSpeed_Head_Mid\", \"MeanRollAngularSpeed_Head_End\",\n",
    "    \"Avg_MeanYawAngularSpeed_Hand_Start\", \"Avg_MeanYawAngularSpeed_Hand_Mid\", \"Avg_MeanYawAngularSpeed_Hand_End\",\n",
    "    \"Avg_MeanPitchAngularSpeed_Hand_Start\", \"Avg_MeanPitchAngularSpeed_Hand_Mid\", \"Avg_MeanPitchAngularSpeed_Hand_End\",\n",
    "    \"Avg_MeanRollAngularSpeed_Hand_Start\", \"Avg_MeanRollAngularSpeed_Hand_Mid\", \"Avg_MeanRollAngularSpeed_Hand_End\"\n",
    "]\n",
    "std_angular_features = [\n",
    "    \"StdYawAngularSpeed_Head_Start\", \"StdYawAngularSpeed_Head_Mid\", \"StdYawAngularSpeed_Head_End\",\n",
    "    \"StdPitchAngularSpeed_Head_Start\", \"StdPitchAngularSpeed_Head_Mid\", \"StdPitchAngularSpeed_Head_End\",\n",
    "    \"StdRollAngularSpeed_Head_Start\", \"StdRollAngularSpeed_Head_Mid\", \"StdRollAngularSpeed_Head_End\",                    \n",
    "    \"Avg_StdYawAngularSpeed_Hand_Start\", \"Avg_StdYawAngularSpeed_Hand_Mid\", \"Avg_StdYawAngularSpeed_Hand_End\",\n",
    "    \"Avg_StdPitchAngularSpeed_Hand_Start\", \"Avg_StdPitchAngularSpeed_Hand_Mid\", \"Avg_StdPitchAngularSpeed_Hand_End\",\n",
    "    \"Avg_StdRollAngularSpeed_Hand_Start\", \"Avg_StdRollAngularSpeed_Hand_Mid\", \"Avg_StdRollAngularSpeed_Hand_End\"\n",
    "]\"\"\"\n",
    "\n",
    "\"\"\"subject_features = get_features_set('Doctor', '157015')\n",
    "agent_features = get_features_set('Agent', '157015')\n",
    "all_features = get_features_set('Doctor+Agent', '157015')\n",
    "subject_features_nophase = get_features_set('Doctor', 'No Phase')\n",
    "agent_features_nophase = get_features_set('Agent', 'No Phase')\n",
    "all_features_nophase = get_features_set('Doctor+Agent', 'No Phase')\n",
    "subject_features_verbal = get_features_set('Doctor', 'Verbal')\n",
    "agent_features_verbal = get_features_set('Agent', 'Verbal')\n",
    "all_features_verbal = get_features_set('Doctor+Agent', 'Verbal')\n",
    "subject_features_nonverbal = get_features_set('Doctor', 'Non-Verbal')\n",
    "agent_features_nonverbal = get_features_set('Agent', 'Non-Verbal')\n",
    "all_features_nonverbal = get_features_set('Doctor+Agent', 'Non-Verbal')\n",
    "subject_features_multimodal = get_features_set('Doctor', 'Multimodal')\n",
    "agent_features_multimodal = get_features_set('Agent', 'Multimodal')\n",
    "all_features_multimodal = get_features_set('Doctor+Agent', 'Multimodal')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#out_path = os.path.join(config.OUT_PATH, 'gridsearchcv')\n",
    "#if not os.path.exists(out_path): os.makedirs(out_path)\n",
    "    \n",
    "#OUT_PATH=os.path.join(config.OUT_PATH, 'cleaned-20190422')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \"\"\"from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "            y_predict_proba = clf.predict_proba(X_test)\n",
    "\n",
    "            # Compute ROC curve and ROC AUC for each class\n",
    "            n_classes = 3\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "            all_y_test_i = np.array([])\n",
    "            all_y_predict_proba = np.array([])\n",
    "            for i in range(n_classes):\n",
    "                y_test_i = map(lambda x: 1 if x == i else 0, y_test)\n",
    "                all_y_test_i = np.concatenate([all_y_test_i, y_test_i])\n",
    "                all_y_predict_proba = np.concatenate([all_y_predict_proba, y_predict_proba[:, i]])\n",
    "                fpr[i], tpr[i], _ = roc_curve(y_test_i, y_predict_proba[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            # Compute micro-average ROC curve and ROC area\n",
    "            fpr[\"average\"], tpr[\"average\"], _ = roc_curve(all_y_test_i, all_y_predict_proba)\n",
    "            roc_auc[\"average\"] = auc(fpr[\"average\"], tpr[\"average\"])\n",
    "\n",
    "\n",
    "            # Plot average ROC Curve\n",
    "            plt.figure()\n",
    "            plt.plot(fpr[\"average\"], tpr[\"average\"],\n",
    "                     label='Average ROC curve (area = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"average\"]),\n",
    "                     color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "            # Plot each individual ROC curve\n",
    "            for i in range(n_classes):\n",
    "                plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                         label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                         ''.format(i, roc_auc[i]))\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC curve %s / %s / test set ? %s / oversampling %s' % (clf.__class__.__name__, prediction_task, \n",
    "                                                                               test_set, upsample))\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"\n",
    "        # features importances\n",
    "        if dropcol_fis is not None:\n",
    "            final_fi = dropcol_fis.T\n",
    "            final_fi['means'] = final_fi.mean(axis=1)\n",
    "            final_fi['std'] = final_fi.std(axis=1)\n",
    "            final_fi['sem'] = final_fi.sem(axis=1)\n",
    "            final_fi['confidence'] = final_fi.apply(lambda row : 2*(np.mean(row) - st.t.interval(0.95, len(row)-1, loc=np.mean(row), scale=st.sem(row))[0]) , axis = 1)\n",
    "            plot_importance('Columns Drop - ' + test_set_name, \n",
    "                            final_fi['means'],\n",
    "                            final_fi['confidence'],\n",
    "                            feats, \n",
    "                            sort=False)\n",
    "            dumpPath = os.path.join(dh.root_path, 'results', 'features_importances_dropcol_%s.xlsx' % test_set_name)\n",
    "            final_fi.to_excel(dumpPath, index=True)\n",
    "\n",
    "\n",
    "        if fis is not None:\n",
    "            final_fi_ = fis.T\n",
    "            final_fi_['means'] = final_fi_.mean(axis=1)\n",
    "            final_fi_['std'] = final_fi_.std(axis=1)\n",
    "            final_fi_['sem'] = final_fi_.sem(axis=1)\n",
    "            final_fi_['confidence'] = final_fi_.apply(lambda row : 2*(np.mean(row) - st.t.interval(0.95, len(row)-1, loc=np.mean(row), scale=st.sem(row))[0]) , axis = 1)\n",
    "            plot_importance('Gini - ' + test_set_name, \n",
    "                            final_fi_['means'],\n",
    "                            final_fi_['confidence'],\n",
    "                            feats, \n",
    "                            sort=False)\n",
    "            dumpPath = os.path.join(dh.root_path, 'results', 'features_importances_%s.xlsx' % test_set_name)\n",
    "            final_fi.to_excel(dumpPath, index=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from run_gridsearch (if test_set: ...)\n",
    "\n",
    "        else: # no test set - useless ?\n",
    "            if verbose > 1:\n",
    "                print('    Running %d 10-folds cv iterations on best classifier %s ...' % (repeat, best_params))\n",
    "\n",
    "            global all_feature_importances\n",
    "            global all_dropcol_feature_importances\n",
    "            \n",
    "            if grid is not None:\n",
    "                best_clf = sklearn.base.clone(grid.best_estimator_) \n",
    "            else:\n",
    "                best_clf = clf\n",
    "            \n",
    "            for i in np.arange(repeat):\n",
    "                #forest = RandomForestClassifier(oob_score=True)\n",
    "                # set best params to classifier\n",
    "\n",
    "                scores = run_cross_val_score(best_clf, \n",
    "                                  X, y,  \n",
    "                                  n_jobs=n_jobs,\n",
    "                                  verbose=verbose)\n",
    "                if verbose > 2:\n",
    "                    print(\"iter # %d\" % i)\n",
    "                #    print(scores)\n",
    "                scores_df = pd.DataFrame(scores)\n",
    "                #print(scores_df)\n",
    "                #print(scores_df.columns)\n",
    "                all_scores.append(scores_df)\n",
    "\n",
    "\n",
    "\n",
    "            # compute feature importance for random forest : we don't need validation data as it relies on oob score\n",
    "            if type(clf) == RandomForestClassifier:\n",
    "                all_feature_importances = pd.DataFrame(all_feature_importances, columns=features)        \n",
    "                if fi:\n",
    "                    X_ = pd.DataFrame(X, columns=features)\n",
    "                    y_ = pd.DataFrame(y)        \n",
    "                    for i in np.arange(repeat_dc):\n",
    "                        # alternative computation for feature importance\n",
    "                        all_dropcol_feature_importances.append(dropcol_importances(best_clf, X_, y_, RANDOM_STATE+i))\n",
    "            else:\n",
    "                all_feature_importances = None\n",
    "                all_dropcol_feature_importances = None\n",
    "                '''print(X, y)\n",
    "                skf = StratifiedKFold(n_splits=repeat, random_state=RANDOM_STATE, shuffle=True)\n",
    "                for i in np.arange(repeat_dc):\n",
    "                    for train_index, test_index in skf.split(X, y):\n",
    "                        X_train, X_test = X[train_index], X[test_index]\n",
    "                        y_train, y_test = y[train_index], y[test_index]\n",
    "                        X_train = pd.DataFrame(X_train, columns=features)\n",
    "                        X_test = pd.DataFrame(X_test, columns=features)\n",
    "                        y_train = pd.DataFrame(y_train)\n",
    "                        y_test = pd.DataFrame(y_test)\n",
    "                        # alternative computation for feature importance\n",
    "                        dcs = dropcol_importances_sklearn(clf, X_train, y_train, X_test, y_test, RANDOM_STATE+i)\n",
    "                        print(\"X_train %s X_test %s y_train %s y_test %s\" % (len(X_train), len(X_test), len(y_train), len(y_test)))\n",
    "                        print(dcs)\n",
    "                        all_dropcol_feature_importances.append(dcs)'''\n",
    "\n",
    "            #all_scores = pd.concat(all_scores)\n",
    "            if all_dropcol_feature_importances is not None:\n",
    "                all_dropcol_feature_importances = pd.DataFrame(all_dropcol_feature_importances, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare results\n",
    "def update_results_from_gridsearch(rdf, cv_results_, p1, p2, p3, p4, p5):\n",
    "    df = pd.DataFrame(cv_results_)\n",
    "    best_precision = df.mean_test_precision_macro.max()\n",
    "    df_best = df.loc[df['mean_test_precision_macro'] == best_precision]\n",
    "    if p5 == 'score':\n",
    "        cols = [col for col in df_best.columns if ('mean_test_' in col)]\n",
    "    elif p5 == 'err':\n",
    "        cols = [col for col in df_best.columns if ('mean_std_' in col)]\n",
    "    print('update_results_from_gridsearch: cols %s' % cols)\n",
    "    update_results(rdf, df_best[cols], p1, p2, p3, p4, p5)\n",
    "    return rdf\n",
    "\n",
    "def update_results(rdf, df, p1, p2, p3, p4, p5):\n",
    "    for i, col in enumerate(df.columns):\n",
    "        rdf.loc[(p1, p2, p3, p4, p5), col] = df.iloc[0].loc[col]\n",
    "    return rdf\n",
    "\n",
    "\n",
    "def update_results_single(rdf, df, p1, p2, p3, p4, p5):\n",
    "    for i, col in enumerate(df.columns):\n",
    "        rdf.loc[(p1, p2, p3, p4, p5), col] = df.loc[col]\n",
    "    return rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from run_cross_val_score()\n",
    "\n",
    "#if not isinstance(clf, JNCC2Wrapper) and not (isinstance(clf, Pipeline) and isinstance(clf.named_steps['clf'], JNCC2Wrapper)):\n",
    "        \n",
    "    \"\"\"else:\n",
    "        labels = sorted(np.unique(y).astype(int))        \n",
    "        cross_val_scores = {}\n",
    "        cross_val_scores['val_f1_macro_weighted'] = []\n",
    "        for idx, label in enumerate(labels):\n",
    "            cross_val_scores['val_f1_macro_weighted_class_%d' % label] = []\n",
    "        cross_val_scores['val_precision_macro_weighted'] = []\n",
    "        for idx, label in enumerate(labels):\n",
    "            cross_val_scores['val_precision_macro_weighted_class_%d' % label] = []   \n",
    "        cross_val_scores['val_recall_macro_weighted'] = []\n",
    "        for idx, label in enumerate(labels):\n",
    "            cross_val_scores['val_recall_macro_weighted_class_%d' % label] = []     \n",
    "        cross_val_score['val_f1_micro'] = []\n",
    "        cross_val_scores['val_mrr'] = []\n",
    "        \n",
    "            \n",
    "        jncc_cv, y_trues_cv = clf.cv('train.arff', idx)\n",
    "        \n",
    "        cross_val_scores['score_time'] = [0]*len(jncc_cv)\n",
    "        \n",
    "        for i in np.arange(len(jncc_cv)):\n",
    "            y_true = y_trues_cv[i]\n",
    "            y_pred = jncc_cv[i][:, 0]\n",
    "            cross_val_scores['val_f1_macro_weighted'].append(f1_score(y_true, y_pred, average='weighted'))\n",
    "            for idx, label in enumerate(labels):\n",
    "                cross_val_scores['val_f1_macro_weighted_class_%d' % label].append(f1_score(y_true, y_pred, labels=[label], average='weighted'))\n",
    "            cross_val_scores['val_precision_macro_weighted'].append(precision_score(y_true, y_pred, average='weighted'))\n",
    "            for idx, label in enumerate(labels):\n",
    "                cross_val_scores['val_precision_macro_weighted_class_%d' % label].append(precision_score(y_true, y_pred, labels=[label], average='weighted'))    \n",
    "            cross_val_scores['val_recall_macro_weighted'].append(recall_score(y_true, y_pred, average='weighted'))\n",
    "            for idx, label in enumerate(labels):\n",
    "                cross_val_scores['val_recall_macro_weighted_class_%d' % label].append(recall_score(y_true, y_pred, labels=[label], average='weighted'))    \n",
    "            #print('y_true %s y_pred %s' % (y_true.shape, jncc_cv[i].shape))\n",
    "            cross_val_scores['val_f1_micro'].append(f1_score(y_true, y_pred, average='micro'))\n",
    "            cross_val_scores['val_mrr'].append(feu.compute_mrr(y_true, jncc_cv[i], labels))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def run_gridsearch(clf, param_grid, data, features, prediction_task, repeat=10, repeat_dc=300, test_set=0.1, n_splits=1, \n",
    "                   upsample=None, fi=False, bins=None, strategy=None, force_gs=True, n_jobs=-1, verbose=0):\n",
    "    if verbose > 1:\n",
    "        print('run_gridsearch(clf=%s, param_grid=%s, bins=%d, strategy=%s)' % (clf.__class__.__name__, str(param_grid), \n",
    "                                                                               bins, strategy))\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "    create_test_set = test_set is not None and test_set > 0\n",
    "    best_params = None\n",
    "    grid = None\n",
    "    is_jncc = isinstance(get_clf_from_estimator(clf), JNCC2Wrapper)\n",
    "    \n",
    "    all_scores = []\n",
    "    all_test_scores = []    \n",
    "    all_supports = []\n",
    "    all_test_supports = []\n",
    "    fi_method = []\n",
    "    fi_fs = pd.DataFrame(np.zeros((1, len(features_df.columns))), columns=features_df.columns)\n",
    "    \n",
    "    splits = prepare_train_data(samples=data, features=features, prediction_task=prediction_task, test_set=test_set, \n",
    "                                n_splits=n_splits, upsample=upsample, bins=bins, strategy=strategy, verbose=verbose)\n",
    "\n",
    "    for idx, (X, y, X_test, y_test) in enumerate(splits):\n",
    "        if verbose > 1:\n",
    "            print(\"*** Train/Test Split #%d\" % idx)\n",
    "        y = y.astype(int)\n",
    "        support = np.unique(y, return_counts=True)\n",
    "        support_test = None\n",
    "        if y_test is not None:\n",
    "            y_test = y_test.astype(int)\n",
    "            support_test = np.unique(y_test, return_counts=True)\n",
    "        all_supports.append(str(support))\n",
    "        all_test_supports.append(str(support_test))\n",
    "        \n",
    "        # first evaluate best params through grid search for this particular dataset/featureset\n",
    "        # !!! important, see http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf\n",
    "        # use 'force_gs' so hyper-parameters search is performed independently on each train split before evaluation on test, which would be unbiased protocol\n",
    "        if best_params is None or force_gs :\n",
    "            if len(ParameterGrid(param_grid)) > 1:\n",
    "                grid = gridsearch2(clf, \n",
    "                                  X, y, \n",
    "                                  prediction_task, \n",
    "                                  param_grid=param_grid, \n",
    "                                  features=features,\n",
    "                                  n_jobs=n_jobs if not is_jncc else 1, # avoid multithreading with JNCCWrapper\n",
    "                                  verbose=verbose)\n",
    "                best_params = grid.best_params_\n",
    "                single_best_score = grid.best_score_\n",
    "                if verbose > 1:\n",
    "                    print('  Gridsearch evaluated best score %s' % (single_best_score))\n",
    "                    print('  Gridsearch best params : %s' % str(best_params))\n",
    "\n",
    "            else:\n",
    "                if verbose > 1:\n",
    "                    print('run_gridsearch: only 1 set of parameters, gridsearch not required')\n",
    "                best_params = ParameterGrid(param_grid)[0]\n",
    "\n",
    "        if create_test_set:\n",
    "\n",
    "            # compute cv scores (redundant with gridsearch ?)\n",
    "            \n",
    "            #if not is_jncc:\n",
    "\n",
    "            if grid is not None:\n",
    "                best_clf = sklearn.base.clone(grid.best_estimator_)\n",
    "            else:\n",
    "                best_clf = clf\n",
    "                if verbose > 2:\n",
    "                    print('CLF possible params %s' % str(clf.get_params().keys()))\n",
    "                best_clf.set_params(**best_params)\n",
    "            \"\"if isinstance(best_clf, JNCC2Wrapper):\n",
    "                # pass the DataHandler\n",
    "                best_clf = JNCC2Wrapper(dh)\n",
    "            elif isinstance(best_clf, Pipeline):\n",
    "                if 'clf' in best_clf.named_steps and isinstance(best_clf.named_steps['clf'], JNCC2Wrapper):\n",
    "                    # pass the DataHandler to the pipelined JNCC2\n",
    "                    best_clf.set_params(clf__dataHandler=dh)   \n",
    "                print('DEBUG pipeline named steps %s' % str(best_clf.named_steps))  \"\"                  \n",
    "            val_scores = run_cross_val_score(best_clf, X, y, n_jobs=n_jobs, verbose=verbose)\n",
    "                            \n",
    "            #else:\n",
    "                \n",
    "            #    val_scores = run_cross_val_score(clf, X, y, idx=idx, verbose=verbose)\n",
    "                \n",
    "                \n",
    "    \n",
    "            #jncc_result, y_true_predict = jncc.predict('train.arff', 'test.arff', 0)\n",
    "                \n",
    "            val_scores = pd.DataFrame(val_scores)\n",
    "            all_scores.append(val_scores)\n",
    "\n",
    "            # compute test scores\n",
    "            \n",
    "            labels = sorted(support[0])\n",
    "            if verbose > 2:\n",
    "                print(\"LABELS \" + str(labels))            \n",
    "\n",
    "            #if not is_jncc:\n",
    "            \n",
    "            if grid is not None:\n",
    "                best_clf = sklearn.base.clone(grid.best_estimator_)\n",
    "            else:\n",
    "                best_clf = clf\n",
    "\n",
    "            best_clf.fit(X, y)\n",
    "            y_pred = best_clf.predict(X_test)\n",
    "            \n",
    "            if is_jncc:\n",
    "                # jncc returns a multi-label proposition\n",
    "                jncc_result = y_pred\n",
    "                if y_pred.values.ndim == 1:\n",
    "                    y_pred = jncc_result\n",
    "                else:\n",
    "                    y_pred = jncc_result.iloc(axis=1)[0]\n",
    "\n",
    "            # retrieve feature selection information, if any was performed\n",
    "            if type(best_clf) == Pipeline and 'fs' in best_clf.named_steps:\n",
    "                selected_features = get_feature_names_from_pipeline(best_clf, features)['fs']\n",
    "                if selected_features is not None:\n",
    "                    if verbose > 1:\n",
    "                        print('  Selected features %s' % str(selected_features))\n",
    "                    for ifeat in selected_features:\n",
    "                        fi_fs[ifeat] += 1                          \n",
    "                \n",
    "            #else:\n",
    "            #    # for JNCC (multilabel result), usual scores are computed thanks to first prediction, and MRR is computed based on ranked predictions\n",
    "            #    jncc_result, y_true = clf.predict('train.arff', 'test.arff', idx)\n",
    "            #    y_pred = jncc_result.iloc(axis=1)[0]\n",
    "            \n",
    "            \n",
    "            test_scores_row = {\n",
    "                'test_balanced_acc': balanced_accuracy_score(y_test, y_pred),\n",
    "                #'test_balanced_acc_adjusted': balanced_accuracy_score(y_test, y_pred, adjusted=True),\n",
    "                'test_f1_macro_weighted': f1_score(y_test, y_pred, average='weighted') }\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['test_f1_macro_class_%d' % label] = f1_score(y_test, y_pred, labels=[label], average='weighted')\n",
    "            test_scores_row['test_precision_macro_weighted'] =  precision_score(y_test, y_pred, average='weighted')\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['test_precision_macro_class_%d' % label] = precision_score(y_test, y_pred, labels=[label], average='weighted')            \n",
    "            test_scores_row['test_recall_macro_weighted'] =  recall_score(y_test, y_pred, average='weighted')\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['test_recall_macro_class_%d' % label] = recall_score(y_test, y_pred, labels=[label], average='weighted')     \n",
    "            #test_scores_row['test_f1_micro'] = f1_score(y_test, y_pred, average='micro')\n",
    "            if is_jncc:\n",
    "                test_scores_row['test_mrr'] = feu.compute_mrr(y_test, jncc_result, labels)\n",
    "                test_scores_row['test_determinacy'] = feu.compute_determinacy(jncc_result)\n",
    "                # TODO: add scores performed by JNCC2 ? (single accuracy etc)\n",
    "                \n",
    "            # Compute dummy classifier scores using different strategies, as a baseline\n",
    "            for dummy_strat in ['uniform', 'stratified', 'most_frequent']:\n",
    "                dummy = DummyClassifier(strategy=dummy_strat, random_state=RANDOM_STATE)\n",
    "                dummy.fit(X, y)\n",
    "                y_pred = dummy.predict(X_test)\n",
    "                test_scores_row ['rnd_%s_balanced_acc' % dummy_strat] = balanced_accuracy_score(y_test, y_pred)\n",
    "                test_scores_row['rnd_%s_f1_macro_weighted' % dummy_strat] = f1_score(y_test, y_pred, average='weighted')\n",
    "                for idx, label in enumerate(labels):\n",
    "                    test_scores_row['rnd_%s_f1_macro_class_%d' % (dummy_strat, label)] = f1_score(y_test, y_pred, labels=[label], average='weighted')\n",
    "                test_scores_row['rnd_%s_precision_macro_weighted' % dummy_strat] =  precision_score(y_test, y_pred, average='weighted')\n",
    "                for idx, label in enumerate(labels):\n",
    "                    test_scores_row['rnd_%s_precision_macro_class_%d' % (dummy_strat, label)] = precision_score(y_test, y_pred, labels=[label], average='weighted')            \n",
    "                test_scores_row['rnd_%s_recall_macro_weighted' % dummy_strat] =  recall_score(y_test, y_pred, average='weighted')\n",
    "                for idx, label in enumerate(labels):\n",
    "                    test_scores_row['rnd_%s_recall_macro_class_%d' % (dummy_strat, label)] = recall_score(y_test, y_pred, labels=[label], average='weighted')                  \n",
    "                test_scores_row['rnd_%s_f1_micro' % dummy_strat] = f1_score(y_test, y_pred, average='micro')\n",
    "                    \n",
    "            test_scores_df = pd.DataFrame([test_scores_row], index=['score'])\n",
    "            all_test_scores.append(test_scores_df)\n",
    "\n",
    "\n",
    "        if not create_test_set:\n",
    "            if verbose > 1:\n",
    "                print('    Running %d 10-folds cv iterations on best classifier %s ...' % (repeat, best_params))\n",
    "\n",
    "            global all_feature_importances\n",
    "            global all_dropcol_feature_importances\n",
    "            \n",
    "            if grid is not None:\n",
    "                best_clf = sklearn.base.clone(grid.best_estimator_) \n",
    "            else:\n",
    "                best_clf = clf\n",
    "            \n",
    "            for i in np.arange(repeat):\n",
    "                #forest = RandomForestClassifier(oob_score=True)\n",
    "                # set best params to classifier\n",
    "\n",
    "                scores = run_cross_val_score(best_clf, \n",
    "                                  X, y,  \n",
    "                                  n_jobs=n_jobs,\n",
    "                                  verbose=verbose)\n",
    "                if verbose > 2:\n",
    "                    print(\"iter # %d\" % i)\n",
    "                #    print(scores)\n",
    "                scores_df = pd.DataFrame(scores)\n",
    "                #print(scores_df)\n",
    "                #print(scores_df.columns)\n",
    "                all_scores.append(scores_df)\n",
    "\n",
    "\n",
    "\n",
    "            # compute feature importance for random forest : we don't need validation data as it relies on oob score\n",
    "            if type(clf) == RandomForestClassifier:\n",
    "                all_feature_importances = pd.DataFrame(all_feature_importances, columns=features)        \n",
    "                if fi:\n",
    "                    X_ = pd.DataFrame(X, columns=features)\n",
    "                    y_ = pd.DataFrame(y)        \n",
    "                    for i in np.arange(repeat_dc):\n",
    "                        # alternative computation for feature importance\n",
    "                        all_dropcol_feature_importances.append(dropcol_importances(best_clf, X_, y_, RANDOM_STATE+i))\n",
    "            else:\n",
    "                all_feature_importances = None\n",
    "                all_dropcol_feature_importances = None\n",
    "                '''print(X, y)\n",
    "                skf = StratifiedKFold(n_splits=repeat, random_state=RANDOM_STATE, shuffle=True)\n",
    "                for i in np.arange(repeat_dc):\n",
    "                    for train_index, test_index in skf.split(X, y):\n",
    "                        X_train, X_test = X[train_index], X[test_index]\n",
    "                        y_train, y_test = y[train_index], y[test_index]\n",
    "                        X_train = pd.DataFrame(X_train, columns=features)\n",
    "                        X_test = pd.DataFrame(X_test, columns=features)\n",
    "                        y_train = pd.DataFrame(y_train)\n",
    "                        y_test = pd.DataFrame(y_test)\n",
    "                        # alternative computation for feature importance\n",
    "                        dcs = dropcol_importances_sklearn(clf, X_train, y_train, X_test, y_test, RANDOM_STATE+i)\n",
    "                        print(\"X_train %s X_test %s y_train %s y_test %s\" % (len(X_train), len(X_test), len(y_train), len(y_test)))\n",
    "                        print(dcs)\n",
    "                        all_dropcol_feature_importances.append(dcs)'''\n",
    "\n",
    "            #all_scores = pd.concat(all_scores)\n",
    "            if all_dropcol_feature_importances is not None:\n",
    "                all_dropcol_feature_importances = pd.DataFrame(all_dropcol_feature_importances, columns=features)\n",
    "\n",
    "    if len(all_scores) == 1:\n",
    "        all_scores = all_scores[0]\n",
    "    elif len(all_scores) > 0:\n",
    "        all_scores = pd.concat(all_scores)\n",
    "    else:\n",
    "        all_scores = pd.DataFrame()\n",
    "    if len(all_test_scores) == 1:\n",
    "        all_test_scores = all_test_scores[0]\n",
    "    elif len(all_test_scores) > 0:\n",
    "        all_test_scores = pd.concat(all_test_scores)\n",
    "    else:\n",
    "        all_test_scores = pd.DataFrame()\n",
    "        \n",
    "    result = {\n",
    "        'all_scores': all_scores,\n",
    "        'all_test_scores': all_test_scores,\n",
    "        'dropcol_fi': all_dropcol_feature_importances,\n",
    "        'fi': all_feature_importances,\n",
    "        'fi_fs': fi_fs,\n",
    "        'support': support,\n",
    "        'support_test': support_test\n",
    "    }\n",
    "        \n",
    "    return grid, result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            y_pred = y_pred_rnd[:, best]\n",
    "            test_scores_row ['rnd_%s_balanced_acc' % dummy_strat] = balanced_accuracy_score(y_test, y_pred)\n",
    "            test_scores_row['rnd_%s_f1_macro_weighted' % dummy_strat] = f1_score(y_test, y_pred, average='weighted')\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['rnd_%s_f1_macro_class_%d' % (dummy_strat, label)] = f1_score(y_test, y_pred, labels=[label], average='weighted')\n",
    "            test_scores_row['rnd_%s_precision_macro_weighted' % dummy_strat] =  precision_score(y_test, y_pred, average='weighted')\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['rnd_%s_precision_macro_class_%d' % (dummy_strat, label)] = precision_score(y_test, y_pred, labels=[label], average='weighted')            \n",
    "            test_scores_row['rnd_%s_recall_macro_weighted' % dummy_strat] =  recall_score(y_test, y_pred, average='weighted')\n",
    "            for idx, label in enumerate(labels):\n",
    "                test_scores_row['rnd_%s_recall_macro_class_%d' % (dummy_strat, label)] = recall_score(y_test, y_pred, labels=[label], average='weighted')                  \n",
    "            test_scores_row['rnd_%s_f1_micro' % dummy_strat] = f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
